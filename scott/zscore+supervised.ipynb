{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import math\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '/Users/scottlichtenstein/Desktop/IE/Term 3/Capstone/capstone.airbus/Notebooks/generated_data/scaled_dataset_with_fuel_leak_test.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    247436\n",
       "1     61934\n",
       "2     61610\n",
       "3     61290\n",
       "Name: leakage, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.leakage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FUEL_USED_1</th>\n",
       "      <th>FUEL_USED_2</th>\n",
       "      <th>FUEL_USED_3</th>\n",
       "      <th>FUEL_USED_4</th>\n",
       "      <th>FW_GEO_ALTITUDE</th>\n",
       "      <th>VALUE_FOB</th>\n",
       "      <th>VALUE_FUEL_QTY_CT</th>\n",
       "      <th>VALUE_FUEL_QTY_FT1</th>\n",
       "      <th>VALUE_FUEL_QTY_FT2</th>\n",
       "      <th>VALUE_FUEL_QTY_FT3</th>\n",
       "      <th>...</th>\n",
       "      <th>VALUE_FUEL_QTY_LXT</th>\n",
       "      <th>VALUE_FUEL_QTY_RXT</th>\n",
       "      <th>FLIGHT</th>\n",
       "      <th>TOTAL_FUEL_USED</th>\n",
       "      <th>VALUE_FOB_DIFF</th>\n",
       "      <th>TOTAL_FOB_BY_QTY</th>\n",
       "      <th>DELTA_VFOB_VS_VFOBQTY</th>\n",
       "      <th>ALTITUDE_DIFF</th>\n",
       "      <th>VALUE_FOB_MISSING</th>\n",
       "      <th>leakage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-9.041711e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-4.784783e-07</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>3.908452e-01</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-1.063828e-03</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.001947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>-3.908470e-01</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.837354</td>\n",
       "      <td>3.481469e-03</td>\n",
       "      <td>0.042372</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>-0.000431</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.068767</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004485</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>3.908452e-01</td>\n",
       "      <td>-0.001947</td>\n",
       "      <td>0.837358</td>\n",
       "      <td>9.572180e-02</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.005867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.083058</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>-1.172539e+00</td>\n",
       "      <td>-0.001947</td>\n",
       "      <td>-1.674709</td>\n",
       "      <td>-9.807880e-02</td>\n",
       "      <td>0.127123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432265</th>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.083058</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004485</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>7.816912e-01</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.837358</td>\n",
       "      <td>-4.784783e-07</td>\n",
       "      <td>-0.042378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432266</th>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.006664</td>\n",
       "      <td>-0.002445</td>\n",
       "      <td>-0.005867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.083058</td>\n",
       "      <td>-0.068767</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004485</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.006915</td>\n",
       "      <td>-7.816930e-01</td>\n",
       "      <td>-0.001947</td>\n",
       "      <td>-1.674709</td>\n",
       "      <td>-2.085003e-01</td>\n",
       "      <td>0.127123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432267</th>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.001947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.068767</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.004488</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>-9.041711e-07</td>\n",
       "      <td>-0.001947</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-2.085003e-01</td>\n",
       "      <td>0.042372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432268</th>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>-0.003907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.066595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004485</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>-3.908470e-01</td>\n",
       "      <td>-0.003907</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.169992e-01</td>\n",
       "      <td>0.084748</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432269</th>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.083058</td>\n",
       "      <td>0.068904</td>\n",
       "      <td>-0.066595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004485</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>7.816912e-01</td>\n",
       "      <td>-0.001947</td>\n",
       "      <td>0.837358</td>\n",
       "      <td>-2.085003e-01</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432270 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FUEL_USED_1  FUEL_USED_2  FUEL_USED_3  FUEL_USED_4  FW_GEO_ALTITUDE  \\\n",
       "0         -0.000015    -0.000015    -0.000015    -0.000015        -0.000048   \n",
       "1          0.001045     0.001094     0.001179     0.001043        -0.000046   \n",
       "2          0.001041     0.001086     0.001081     0.001064        -0.000032   \n",
       "3          0.001044     0.001088     0.001077     0.001070        -0.000431   \n",
       "4          0.001039     0.001093     0.001077     0.001072        -0.000055   \n",
       "...             ...          ...          ...          ...              ...   \n",
       "432265     0.003116     0.003242     0.002834     0.003140         0.001550   \n",
       "432266     0.006247     0.006840     0.007266     0.006664        -0.002445   \n",
       "432267     0.003505     0.003212     0.003217     0.003141        -0.000048   \n",
       "432268     0.003116     0.003195     0.003629     0.003523         0.001550   \n",
       "432269     0.000381     0.000388     0.000395     0.000381         0.000751   \n",
       "\n",
       "        VALUE_FOB  VALUE_FUEL_QTY_CT  VALUE_FUEL_QTY_FT1  VALUE_FUEL_QTY_FT2  \\\n",
       "0        0.000013                0.0            0.000067            0.000069   \n",
       "1        0.000013                0.0            0.000067            0.000069   \n",
       "2       -0.001947                0.0            0.000067            0.000069   \n",
       "3        0.000013                0.0            0.000067           -0.068767   \n",
       "4       -0.005867                0.0           -0.083058            0.000069   \n",
       "...           ...                ...                 ...                 ...   \n",
       "432265   0.001973                0.0           -0.083058            0.000069   \n",
       "432266  -0.005867                0.0           -0.083058           -0.068767   \n",
       "432267  -0.001947                0.0            0.000067           -0.068767   \n",
       "432268  -0.003907                0.0            0.000067            0.000069   \n",
       "432269   0.000013                0.0           -0.083058            0.068904   \n",
       "\n",
       "        VALUE_FUEL_QTY_FT3  ...  VALUE_FUEL_QTY_LXT  VALUE_FUEL_QTY_RXT  \\\n",
       "0                 0.000070  ...            0.000007            0.000006   \n",
       "1                 0.000070  ...            0.000007            0.000006   \n",
       "2                 0.000070  ...            0.000007            0.000006   \n",
       "3                 0.000070  ...           -0.004485            0.004499   \n",
       "4                 0.000070  ...            0.000007            0.000006   \n",
       "...                    ...  ...                 ...                 ...   \n",
       "432265            0.000070  ...           -0.004485            0.004499   \n",
       "432266            0.000070  ...           -0.004485            0.004499   \n",
       "432267            0.000070  ...            0.000007           -0.004488   \n",
       "432268           -0.066595  ...           -0.004485            0.000006   \n",
       "432269           -0.066595  ...           -0.004485            0.004499   \n",
       "\n",
       "          FLIGHT  TOTAL_FUEL_USED  VALUE_FOB_DIFF  TOTAL_FOB_BY_QTY  \\\n",
       "0      -0.000726        -0.000015   -9.041711e-07          0.000013   \n",
       "1      -0.000726         0.001116    3.908452e-01          0.000013   \n",
       "2      -0.000726         0.001094   -3.908470e-01          0.000013   \n",
       "3      -0.000726         0.001096    3.908452e-01         -0.001947   \n",
       "4      -0.000726         0.001096   -1.172539e+00         -0.001947   \n",
       "...          ...              ...             ...               ...   \n",
       "432265 -0.000726         0.003158    7.816912e-01          0.000013   \n",
       "432266 -0.000726         0.006915   -7.816930e-01         -0.001947   \n",
       "432267 -0.000726         0.003349   -9.041711e-07         -0.001947   \n",
       "432268 -0.000726         0.003448   -3.908470e-01         -0.003907   \n",
       "432269 -0.000726         0.000395    7.816912e-01         -0.001947   \n",
       "\n",
       "        DELTA_VFOB_VS_VFOBQTY  ALTITUDE_DIFF  VALUE_FOB_MISSING  leakage  \n",
       "0                    0.000002  -4.784783e-07          -0.000003        0  \n",
       "1                    0.000002  -1.063828e-03          -0.000003        0  \n",
       "2                   -0.837354   3.481469e-03           0.042372        0  \n",
       "3                    0.837358   9.572180e-02          -0.000003        0  \n",
       "4                   -1.674709  -9.807880e-02           0.127123        0  \n",
       "...                       ...            ...                ...      ...  \n",
       "432265               0.837358  -4.784783e-07          -0.042378        1  \n",
       "432266              -1.674709  -2.085003e-01           0.127123        1  \n",
       "432267               0.000002  -2.085003e-01           0.042372        1  \n",
       "432268               0.000002   4.169992e-01           0.084748        1  \n",
       "432269               0.837358  -2.085003e-01          -0.000003        1  \n",
       "\n",
       "[432270 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new column Label with only 0 and 1\n",
    "df[\"Label\"] = np.where(df['leakage'] == 0, 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecting the data for the histogram\n",
    "data_1 = df[df['Label']==1]['VALUE_FOB_DIFF']\n",
    "data_0 = df[df['Label']==0]['VALUE_FOB_DIFF']\n",
    "\n",
    "# Plotting the histograms\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(data_1, bins=30, alpha=0.5, color='red', label='Label 1')\n",
    "plt.hist(data_0, bins=30, alpha=0.5, color='green', label='Label 0')\n",
    "plt.title('Histogram of VALUE_FOB_DIFF')\n",
    "plt.xlabel('VALUE_FOB_DIFF')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecting the data for the boxplot\n",
    "data_1 = df[df['leakage'] == 1]['VALUE_FOB_DIFF']\n",
    "data_0 = df[df['leakage'] == 0]['VALUE_FOB_DIFF']\n",
    "\n",
    "# Creating a figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Creating a boxplot with colored data points\n",
    "boxprops = dict(facecolor='white', color='black')\n",
    "whiskerprops = dict(color='black')\n",
    "flierprops = dict(marker='o', markerfacecolor='none', markersize=6)\n",
    "medianprops = dict(color='black')\n",
    "capprops = dict(color='black')\n",
    "\n",
    "ax.boxplot([data_1, data_0],\n",
    "           labels=['Label 1', 'Label 0'],\n",
    "           patch_artist=True,\n",
    "           boxprops=boxprops,\n",
    "           whiskerprops=whiskerprops,\n",
    "           flierprops=flierprops,\n",
    "           medianprops=medianprops,\n",
    "           capprops=capprops)\n",
    "\n",
    "# Setting the box colors\n",
    "box_colors = ['red', 'green']\n",
    "for patch, color in zip(ax.artists, box_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Setting the title and labels\n",
    "ax.set_title('Boxplot of VALUE_FOB_DIFF with Labels')\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('VALUE_FOB_DIFF')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z-Statistic algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALUE_FOB_DIFF</th>\n",
       "      <th>leakage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151164</th>\n",
       "      <td>-7.816930e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264691</th>\n",
       "      <td>-3.908470e-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154284</th>\n",
       "      <td>-9.041711e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62919</th>\n",
       "      <td>-1.172539e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212408</th>\n",
       "      <td>-3.908470e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77217</th>\n",
       "      <td>-1.563385e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41470</th>\n",
       "      <td>7.816912e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40559</th>\n",
       "      <td>-3.908470e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304553</th>\n",
       "      <td>3.908452e-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119359</th>\n",
       "      <td>7.816912e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        VALUE_FOB_DIFF  leakage\n",
       "151164   -7.816930e-01        0\n",
       "264691   -3.908470e-01        3\n",
       "154284   -9.041711e-07        0\n",
       "62919    -1.172539e+00        0\n",
       "212408   -3.908470e-01        0\n",
       "77217    -1.563385e+00        0\n",
       "41470     7.816912e-01        0\n",
       "40559    -3.908470e-01        0\n",
       "304553    3.908452e-01        3\n",
       "119359    7.816912e-01        0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new dataframe with only required columns\n",
    "dfx = df[[\"VALUE_FOB_DIFF\", \"leakage\"]]\n",
    "dfx.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check for normality \n",
    "dfx['VALUE_FOB_DIFF'].plot(kind='hist', edgecolor='black')\n",
    "plt.title('Histogram of VALUE_FOB_DIFF')\n",
    "plt.xlabel('VALUE_FOB_DIFF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4n/swp60yyd37qg_dmlq1v1_6jw0000gn/T/ipykernel_69231/1684253527.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfx[\"Label\"] = np.where(dfx['leakage'] == 0, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "#new column Label with only 0 and 1\n",
    "dfx[\"Label\"] = np.where(dfx['leakage'] == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    247436\n",
       "1    184834\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking balance\n",
    "dfx.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALUE_FOB_DIFF</th>\n",
       "      <th>leakage</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260404</th>\n",
       "      <td>3.908452e-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358956</th>\n",
       "      <td>-1.172539e+00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381001</th>\n",
       "      <td>-3.908470e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100272</th>\n",
       "      <td>3.908452e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14287</th>\n",
       "      <td>-9.041711e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394907</th>\n",
       "      <td>-9.041711e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407044</th>\n",
       "      <td>-7.816930e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37799</th>\n",
       "      <td>-7.816930e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363949</th>\n",
       "      <td>-7.816930e-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322716</th>\n",
       "      <td>-9.041711e-07</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        VALUE_FOB_DIFF  leakage  Label\n",
       "260404    3.908452e-01        3      1\n",
       "358956   -1.172539e+00        2      1\n",
       "381001   -3.908470e-01        1      1\n",
       "100272    3.908452e-01        0      0\n",
       "14287    -9.041711e-07        0      0\n",
       "394907   -9.041711e-07        1      1\n",
       "407044   -7.816930e-01        1      1\n",
       "37799    -7.816930e-01        0      0\n",
       "363949   -7.816930e-01        2      1\n",
       "322716   -9.041711e-07        2      1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the training set:\n",
      "Accuracy: 0.5278038031785689\n",
      "Precision: 0.41832838353839624\n",
      "Recall: 0.2666792884044546\n",
      "F1 Score: 0.3257175655419886\n",
      "\n",
      "Performance on the test set:\n",
      "Accuracy: 0.5279454970273209\n",
      "Precision: 0.41710932145305\n",
      "Recall: 0.26358246934300644\n",
      "F1 Score: 0.32303226341544333\n"
     ]
    }
   ],
   "source": [
    "#Z statistic\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfx['VALUE_FOB_DIFF'],\n",
    "                                                    dfx['Label'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "#  Calculate the z-scores for the training set\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "X_train['z_score'] = np.abs((X_train - mean) / std)\n",
    "\n",
    "# Determine the threshold for the z-value\n",
    "threshold = 0.8 # Set the threshold value for the z-score\n",
    "\n",
    "# Classify anomalies for the training set based on the threshold\n",
    "X_train['anomaly'] = np.where(X_train['z_score'] > threshold, 1, 0)\n",
    "\n",
    "# Evaluate the performance of the algorithm on the training set\n",
    "y_train_pred = X_train['anomaly']\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"Performance on the training set:\")\n",
    "print(\"Accuracy:\", accuracy_train)\n",
    "print(\"Precision:\", precision_train)\n",
    "print(\"Recall:\", recall_train)\n",
    "print(\"F1 Score:\", f1_train)\n",
    "\n",
    "# Apply the algorithm to the test set\n",
    "X_test['z_score'] = np.abs((X_test- mean) / std)\n",
    "X_test['anomaly'] = np.where(X_test['z_score'] > threshold, 1, 0)\n",
    "\n",
    "#  Evaluate the performance on the test set\n",
    "y_test_pred = X_test['anomaly']\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nPerformance on the test set:\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision:\", precision_test)\n",
    "print(\"Recall:\", recall_test)\n",
    "print(\"F1 Score:\", f1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1    345816\n",
      "Name: anomaly, dtype: int64\n",
      "0.5991100020457395\n",
      "\n",
      "0.050505050505050504\n",
      "1    281903\n",
      "0     63913\n",
      "Name: anomaly, dtype: int64\n",
      "0.5594607674338523\n",
      "\n",
      "0.10101010101010101\n",
      "1    281903\n",
      "0     63913\n",
      "Name: anomaly, dtype: int64\n",
      "0.5594607674338523\n",
      "\n",
      "0.15151515151515152\n",
      "1    281903\n",
      "0     63913\n",
      "Name: anomaly, dtype: int64\n",
      "0.5594607674338523\n",
      "\n",
      "0.20202020202020202\n",
      "1    281903\n",
      "0     63913\n",
      "Name: anomaly, dtype: int64\n",
      "0.5594607674338523\n",
      "\n",
      "0.25252525252525254\n",
      "1    281903\n",
      "0     63913\n",
      "Name: anomaly, dtype: int64\n",
      "0.5594607674338523\n",
      "\n",
      "0.30303030303030304\n",
      "1    281903\n",
      "0     63913\n",
      "Name: anomaly, dtype: int64\n",
      "0.5594607674338523\n",
      "\n",
      "0.35353535353535354\n",
      "1    281903\n",
      "0     63913\n",
      "Name: anomaly, dtype: int64\n",
      "0.5594607674338523\n",
      "\n",
      "0.40404040404040403\n",
      "0    174334\n",
      "1    171482\n",
      "Name: anomaly, dtype: int64\n",
      "0.45341682974559694\n",
      "\n",
      "0.45454545454545453\n",
      "0    174334\n",
      "1    171482\n",
      "Name: anomaly, dtype: int64\n",
      "0.45341682974559694\n",
      "\n",
      "0.5050505050505051\n",
      "0    174334\n",
      "1    171482\n",
      "Name: anomaly, dtype: int64\n",
      "0.45341682974559694\n",
      "\n",
      "0.5555555555555556\n",
      "0    174334\n",
      "1    171482\n",
      "Name: anomaly, dtype: int64\n",
      "0.45341682974559694\n",
      "\n",
      "0.6060606060606061\n",
      "0    174334\n",
      "1    171482\n",
      "Name: anomaly, dtype: int64\n",
      "0.45341682974559694\n",
      "\n",
      "0.6565656565656566\n",
      "0    174334\n",
      "1    171482\n",
      "Name: anomaly, dtype: int64\n",
      "0.45341682974559694\n",
      "\n",
      "0.7070707070707071\n",
      "0    174334\n",
      "1    171482\n",
      "Name: anomaly, dtype: int64\n",
      "0.45341682974559694\n",
      "\n",
      "0.7575757575757576\n",
      "0    174334\n",
      "1    171482\n",
      "Name: anomaly, dtype: int64\n",
      "0.45341682974559694\n",
      "\n",
      "0.8080808080808081\n",
      "0    251536\n",
      "1     94280\n",
      "Name: anomaly, dtype: int64\n",
      "0.3257175655419886\n",
      "\n",
      "0.8585858585858586\n",
      "0    251536\n",
      "1     94280\n",
      "Name: anomaly, dtype: int64\n",
      "0.3257175655419886\n",
      "\n",
      "0.9090909090909091\n",
      "0    251536\n",
      "1     94280\n",
      "Name: anomaly, dtype: int64\n",
      "0.3257175655419886\n",
      "\n",
      "0.9595959595959596\n",
      "0    251536\n",
      "1     94280\n",
      "Name: anomaly, dtype: int64\n",
      "0.3257175655419886\n",
      "\n",
      "1.0101010101010102\n",
      "0    251536\n",
      "1     94280\n",
      "Name: anomaly, dtype: int64\n",
      "0.3257175655419886\n",
      "\n",
      "1.0606060606060606\n",
      "0    251536\n",
      "1     94280\n",
      "Name: anomaly, dtype: int64\n",
      "0.3257175655419886\n",
      "\n",
      "1.1111111111111112\n",
      "0    251536\n",
      "1     94280\n",
      "Name: anomaly, dtype: int64\n",
      "0.3257175655419886\n",
      "\n",
      "1.1616161616161615\n",
      "0    251536\n",
      "1     94280\n",
      "Name: anomaly, dtype: int64\n",
      "0.3257175655419886\n",
      "\n",
      "1.2121212121212122\n",
      "0    297238\n",
      "1     48578\n",
      "Name: anomaly, dtype: int64\n",
      "0.2037959800683052\n",
      "\n",
      "1.2626262626262625\n",
      "0    297238\n",
      "1     48578\n",
      "Name: anomaly, dtype: int64\n",
      "0.2037959800683052\n",
      "\n",
      "1.3131313131313131\n",
      "0    297238\n",
      "1     48578\n",
      "Name: anomaly, dtype: int64\n",
      "0.2037959800683052\n",
      "\n",
      "1.3636363636363635\n",
      "0    297238\n",
      "1     48578\n",
      "Name: anomaly, dtype: int64\n",
      "0.2037959800683052\n",
      "\n",
      "1.4141414141414141\n",
      "0    297238\n",
      "1     48578\n",
      "Name: anomaly, dtype: int64\n",
      "0.2037959800683052\n",
      "\n",
      "1.4646464646464645\n",
      "0    297238\n",
      "1     48578\n",
      "Name: anomaly, dtype: int64\n",
      "0.2037959800683052\n",
      "\n",
      "1.5151515151515151\n",
      "0    297238\n",
      "1     48578\n",
      "Name: anomaly, dtype: int64\n",
      "0.2037959800683052\n",
      "\n",
      "1.5656565656565655\n",
      "0    320783\n",
      "1     25033\n",
      "Name: anomaly, dtype: int64\n",
      "0.11884852480251668\n",
      "\n",
      "1.6161616161616161\n",
      "0    320783\n",
      "1     25033\n",
      "Name: anomaly, dtype: int64\n",
      "0.11884852480251668\n",
      "\n",
      "1.6666666666666667\n",
      "0    320783\n",
      "1     25033\n",
      "Name: anomaly, dtype: int64\n",
      "0.11884852480251668\n",
      "\n",
      "1.7171717171717171\n",
      "0    320783\n",
      "1     25033\n",
      "Name: anomaly, dtype: int64\n",
      "0.11884852480251668\n",
      "\n",
      "1.7676767676767677\n",
      "0    320783\n",
      "1     25033\n",
      "Name: anomaly, dtype: int64\n",
      "0.11884852480251668\n",
      "\n",
      "1.8181818181818181\n",
      "0    320783\n",
      "1     25033\n",
      "Name: anomaly, dtype: int64\n",
      "0.11884852480251668\n",
      "\n",
      "1.8686868686868687\n",
      "0    320783\n",
      "1     25033\n",
      "Name: anomaly, dtype: int64\n",
      "0.11884852480251668\n",
      "\n",
      "1.9191919191919191\n",
      "0    320783\n",
      "1     25033\n",
      "Name: anomaly, dtype: int64\n",
      "0.11884852480251668\n",
      "\n",
      "1.9696969696969697\n",
      "0    332280\n",
      "1     13536\n",
      "Name: anomaly, dtype: int64\n",
      "0.07137503174770334\n",
      "\n",
      "2.0202020202020203\n",
      "0    332280\n",
      "1     13536\n",
      "Name: anomaly, dtype: int64\n",
      "0.07137503174770334\n",
      "\n",
      "2.0707070707070705\n",
      "0    332280\n",
      "1     13536\n",
      "Name: anomaly, dtype: int64\n",
      "0.07137503174770334\n",
      "\n",
      "2.121212121212121\n",
      "0    332280\n",
      "1     13536\n",
      "Name: anomaly, dtype: int64\n",
      "0.07137503174770334\n",
      "\n",
      "2.1717171717171717\n",
      "0    332280\n",
      "1     13536\n",
      "Name: anomaly, dtype: int64\n",
      "0.07137503174770334\n",
      "\n",
      "2.2222222222222223\n",
      "0    332280\n",
      "1     13536\n",
      "Name: anomaly, dtype: int64\n",
      "0.07137503174770334\n",
      "\n",
      "2.2727272727272725\n",
      "0    332280\n",
      "1     13536\n",
      "Name: anomaly, dtype: int64\n",
      "0.07137503174770334\n",
      "\n",
      "2.323232323232323\n",
      "0    332280\n",
      "1     13536\n",
      "Name: anomaly, dtype: int64\n",
      "0.07137503174770334\n",
      "\n",
      "2.3737373737373737\n",
      "0    338053\n",
      "1      7763\n",
      "Name: anomaly, dtype: int64\n",
      "0.04435421699131418\n",
      "\n",
      "2.4242424242424243\n",
      "0    338053\n",
      "1      7763\n",
      "Name: anomaly, dtype: int64\n",
      "0.04435421699131418\n",
      "\n",
      "2.474747474747475\n",
      "0    338053\n",
      "1      7763\n",
      "Name: anomaly, dtype: int64\n",
      "0.04435421699131418\n",
      "\n",
      "2.525252525252525\n",
      "0    338053\n",
      "1      7763\n",
      "Name: anomaly, dtype: int64\n",
      "0.04435421699131418\n",
      "\n",
      "2.5757575757575757\n",
      "0    338053\n",
      "1      7763\n",
      "Name: anomaly, dtype: int64\n",
      "0.04435421699131418\n",
      "\n",
      "2.6262626262626263\n",
      "0    338053\n",
      "1      7763\n",
      "Name: anomaly, dtype: int64\n",
      "0.04435421699131418\n",
      "\n",
      "2.676767676767677\n",
      "0    338053\n",
      "1      7763\n",
      "Name: anomaly, dtype: int64\n",
      "0.04435421699131418\n",
      "\n",
      "2.727272727272727\n",
      "0    338053\n",
      "1      7763\n",
      "Name: anomaly, dtype: int64\n",
      "0.04435421699131418\n",
      "\n",
      "2.7777777777777777\n",
      "0    341092\n",
      "1      4724\n",
      "Name: anomaly, dtype: int64\n",
      "0.029184166901459207\n",
      "\n",
      "2.8282828282828283\n",
      "0    341092\n",
      "1      4724\n",
      "Name: anomaly, dtype: int64\n",
      "0.029184166901459207\n",
      "\n",
      "2.878787878787879\n",
      "0    341092\n",
      "1      4724\n",
      "Name: anomaly, dtype: int64\n",
      "0.029184166901459207\n",
      "\n",
      "2.929292929292929\n",
      "0    341092\n",
      "1      4724\n",
      "Name: anomaly, dtype: int64\n",
      "0.029184166901459207\n",
      "\n",
      "2.9797979797979797\n",
      "0    341092\n",
      "1      4724\n",
      "Name: anomaly, dtype: int64\n",
      "0.029184166901459207\n",
      "\n",
      "3.0303030303030303\n",
      "0    341092\n",
      "1      4724\n",
      "Name: anomaly, dtype: int64\n",
      "0.029184166901459207\n",
      "\n",
      "3.080808080808081\n",
      "0    341092\n",
      "1      4724\n",
      "Name: anomaly, dtype: int64\n",
      "0.029184166901459207\n",
      "\n",
      "3.131313131313131\n",
      "0    343029\n",
      "1      2787\n",
      "Name: anomaly, dtype: int64\n",
      "0.018064773028935492\n",
      "\n",
      "3.1818181818181817\n",
      "0    343029\n",
      "1      2787\n",
      "Name: anomaly, dtype: int64\n",
      "0.018064773028935492\n",
      "\n",
      "3.2323232323232323\n",
      "0    343029\n",
      "1      2787\n",
      "Name: anomaly, dtype: int64\n",
      "0.018064773028935492\n",
      "\n",
      "3.282828282828283\n",
      "0    343029\n",
      "1      2787\n",
      "Name: anomaly, dtype: int64\n",
      "0.018064773028935492\n",
      "\n",
      "3.3333333333333335\n",
      "0    343029\n",
      "1      2787\n",
      "Name: anomaly, dtype: int64\n",
      "0.018064773028935492\n",
      "\n",
      "3.3838383838383836\n",
      "0    343029\n",
      "1      2787\n",
      "Name: anomaly, dtype: int64\n",
      "0.018064773028935492\n",
      "\n",
      "3.4343434343434343\n",
      "0    343029\n",
      "1      2787\n",
      "Name: anomaly, dtype: int64\n",
      "0.018064773028935492\n",
      "\n",
      "3.484848484848485\n",
      "0    343029\n",
      "1      2787\n",
      "Name: anomaly, dtype: int64\n",
      "0.018064773028935492\n",
      "\n",
      "3.5353535353535355\n",
      "0    344207\n",
      "1      1609\n",
      "Name: anomaly, dtype: int64\n",
      "0.010675442468997069\n",
      "\n",
      "3.5858585858585856\n",
      "0    344207\n",
      "1      1609\n",
      "Name: anomaly, dtype: int64\n",
      "0.010675442468997069\n",
      "\n",
      "3.6363636363636362\n",
      "0    344207\n",
      "1      1609\n",
      "Name: anomaly, dtype: int64\n",
      "0.010675442468997069\n",
      "\n",
      "3.686868686868687\n",
      "0    344207\n",
      "1      1609\n",
      "Name: anomaly, dtype: int64\n",
      "0.010675442468997069\n",
      "\n",
      "3.7373737373737375\n",
      "0    344207\n",
      "1      1609\n",
      "Name: anomaly, dtype: int64\n",
      "0.010675442468997069\n",
      "\n",
      "3.7878787878787876\n",
      "0    344207\n",
      "1      1609\n",
      "Name: anomaly, dtype: int64\n",
      "0.010675442468997069\n",
      "\n",
      "3.8383838383838382\n",
      "0    344207\n",
      "1      1609\n",
      "Name: anomaly, dtype: int64\n",
      "0.010675442468997069\n",
      "\n",
      "3.888888888888889\n",
      "0    344207\n",
      "1      1609\n",
      "Name: anomaly, dtype: int64\n",
      "0.010675442468997069\n",
      "\n",
      "3.9393939393939394\n",
      "0    344914\n",
      "1       902\n",
      "Name: anomaly, dtype: int64\n",
      "0.00594105984744111\n",
      "\n",
      "3.9898989898989896\n",
      "0    344914\n",
      "1       902\n",
      "Name: anomaly, dtype: int64\n",
      "0.00594105984744111\n",
      "\n",
      "4.040404040404041\n",
      "0    344914\n",
      "1       902\n",
      "Name: anomaly, dtype: int64\n",
      "0.00594105984744111\n",
      "\n",
      "4.090909090909091\n",
      "0    344914\n",
      "1       902\n",
      "Name: anomaly, dtype: int64\n",
      "0.00594105984744111\n",
      "\n",
      "4.141414141414141\n",
      "0    344914\n",
      "1       902\n",
      "Name: anomaly, dtype: int64\n",
      "0.00594105984744111\n",
      "\n",
      "4.191919191919192\n",
      "0    344914\n",
      "1       902\n",
      "Name: anomaly, dtype: int64\n",
      "0.00594105984744111\n",
      "\n",
      "4.242424242424242\n",
      "0    344914\n",
      "1       902\n",
      "Name: anomaly, dtype: int64\n",
      "0.00594105984744111\n",
      "\n",
      "4.292929292929293\n",
      "0    345335\n",
      "1       481\n",
      "Name: anomaly, dtype: int64\n",
      "0.0033698626443986143\n",
      "\n",
      "4.343434343434343\n",
      "0    345335\n",
      "1       481\n",
      "Name: anomaly, dtype: int64\n",
      "0.0033698626443986143\n",
      "\n",
      "4.393939393939394\n",
      "0    345335\n",
      "1       481\n",
      "Name: anomaly, dtype: int64\n",
      "0.0033698626443986143\n",
      "\n",
      "4.444444444444445\n",
      "0    345335\n",
      "1       481\n",
      "Name: anomaly, dtype: int64\n",
      "0.0033698626443986143\n",
      "\n",
      "4.494949494949495\n",
      "0    345335\n",
      "1       481\n",
      "Name: anomaly, dtype: int64\n",
      "0.0033698626443986143\n",
      "\n",
      "4.545454545454545\n",
      "0    345335\n",
      "1       481\n",
      "Name: anomaly, dtype: int64\n",
      "0.0033698626443986143\n",
      "\n",
      "4.595959595959596\n",
      "0    345335\n",
      "1       481\n",
      "Name: anomaly, dtype: int64\n",
      "0.0033698626443986143\n",
      "\n",
      "4.646464646464646\n",
      "0    345335\n",
      "1       481\n",
      "Name: anomaly, dtype: int64\n",
      "0.0033698626443986143\n",
      "\n",
      "4.696969696969697\n",
      "0    345566\n",
      "1       250\n",
      "Name: anomaly, dtype: int64\n",
      "0.0017280600500867403\n",
      "\n",
      "4.747474747474747\n",
      "0    345566\n",
      "1       250\n",
      "Name: anomaly, dtype: int64\n",
      "0.0017280600500867403\n",
      "\n",
      "4.797979797979798\n",
      "0    345566\n",
      "1       250\n",
      "Name: anomaly, dtype: int64\n",
      "0.0017280600500867403\n",
      "\n",
      "4.848484848484849\n",
      "0    345566\n",
      "1       250\n",
      "Name: anomaly, dtype: int64\n",
      "0.0017280600500867403\n",
      "\n",
      "4.898989898989899\n",
      "0    345566\n",
      "1       250\n",
      "Name: anomaly, dtype: int64\n",
      "0.0017280600500867403\n",
      "\n",
      "4.94949494949495\n",
      "0    345566\n",
      "1       250\n",
      "Name: anomaly, dtype: int64\n",
      "0.0017280600500867403\n",
      "\n",
      "5.0\n",
      "0    345566\n",
      "1       250\n",
      "Name: anomaly, dtype: int64\n",
      "0.0017280600500867403\n",
      "\n",
      "Best threshold: 0.0\n",
      "Best F1 Score: 0.5991100020457395\n"
     ]
    }
   ],
   "source": [
    "# Initialize the best threshold and the best score\n",
    "best_threshold = None\n",
    "best_score = -np.inf\n",
    "\n",
    "# Iterate over a range of possible threshold values\n",
    "for threshold in np.linspace(start=0, stop=5, num=100):\n",
    "    print( threshold)\n",
    "    # Classify anomalies for the training set based on the threshold\n",
    "    X_train['anomaly'] = np.where(X_train['z_score'] > threshold, 1, 0)\n",
    "    y_train_pred = X_train['anomaly']\n",
    "    \n",
    "    print(y_train_pred.value_counts())\n",
    "\n",
    "\n",
    "    # Calculate the score\n",
    "    score = f1_score(y_train, y_train_pred)\n",
    "    print(score)\n",
    "    print()\n",
    "\n",
    "    # If this threshold gives a better score, update the best threshold and the best score\n",
    "    if score > best_score:\n",
    "        best_threshold = threshold\n",
    "        best_score = score\n",
    "\n",
    "print('Best threshold:', best_threshold)\n",
    "print('Best F1 Score:', best_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Why Z-score does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_scaled = pd.read_csv(\n",
    "    '/Users/scottlichtenstein/Desktop/IE/Term 3/Capstone/capstone.airbus/Notebooks/generated_data/merged_dataset_with_fuel_leak_diff_test.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check distribution\n",
    "df_not_scaled['VALUE_FOB_DIFF'].plot(kind='hist', edgecolor='black')\n",
    "plt.title('Histogram of VALUE_FOB_DIFF')\n",
    "plt.xlabel('VALUE_FOB_DIFF')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:02:43] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training RandomForestClassifier...\n",
      "Training AdaBoostClassifier...\n",
      "Training GradientBoostingClassifier...\n",
      "Training DecisionTreeClassifier...\n",
      "Training LogisticRegression...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.783665</td>\n",
       "      <td>0.706191</td>\n",
       "      <td>0.699758</td>\n",
       "      <td>0.699378</td>\n",
       "      <td>0.699560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.745238</td>\n",
       "      <td>0.769854</td>\n",
       "      <td>0.768920</td>\n",
       "      <td>0.774679</td>\n",
       "      <td>0.768392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.633850</td>\n",
       "      <td>0.603743</td>\n",
       "      <td>0.590562</td>\n",
       "      <td>0.566888</td>\n",
       "      <td>0.553355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.664207</td>\n",
       "      <td>0.611921</td>\n",
       "      <td>0.604221</td>\n",
       "      <td>0.572189</td>\n",
       "      <td>0.555442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.691855</td>\n",
       "      <td>0.676337</td>\n",
       "      <td>0.675576</td>\n",
       "      <td>0.679349</td>\n",
       "      <td>0.674351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.498048</td>\n",
       "      <td>0.572698</td>\n",
       "      <td>0.286352</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.364150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model       AUC  Accuracy  Precision    Recall  \\\n",
       "0               XGBClassifier  0.783665  0.706191   0.699758  0.699378   \n",
       "1      RandomForestClassifier  0.745238  0.769854   0.768920  0.774679   \n",
       "2          AdaBoostClassifier  0.633850  0.603743   0.590562  0.566888   \n",
       "3  GradientBoostingClassifier  0.664207  0.611921   0.604221  0.572189   \n",
       "4      DecisionTreeClassifier  0.691855  0.676337   0.675576  0.679349   \n",
       "5          LogisticRegression  0.498048  0.572698   0.286352  0.499990   \n",
       "\n",
       "   F1 Score  \n",
       "0  0.699560  \n",
       "1  0.768392  \n",
       "2  0.553355  \n",
       "3  0.555442  \n",
       "4  0.674351  \n",
       "5  0.364150  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models\n",
    "models = [\n",
    "    XGBClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    LogisticRegression(max_iter=1000)\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"XGBClassifier\", \"RandomForestClassifier\", \"AdaBoostClassifier\",\n",
    "    \"GradientBoostingClassifier\", \"DecisionTreeClassifier\",\n",
    "    \"LogisticRegression\"\n",
    "]\n",
    "\n",
    "# Select features and target\n",
    "features = df.select_dtypes(include=[np.number]).drop(\n",
    "    columns=['Label', 'leakage'])\n",
    "target = df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Prepare an empty DataFrame to store the results\n",
    "results = pd.DataFrame(\n",
    "    columns=['Model', 'AUC', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Iterate over models, train, make predictions and get classification metrics\n",
    "for model, name in zip(models, model_names):\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    accuracy = report[\"accuracy\"]\n",
    "    precision = report[\"macro avg\"][\"precision\"]\n",
    "    recall = report[\"macro avg\"][\"recall\"]\n",
    "    f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    results = results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"AUC\": auc,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1\n",
    "        },\n",
    "        ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_0 = test_data[test_data['Label'] == 0]\n",
    "test_data_1 = test_data[test_data['Label'] == 1]\n",
    "\n",
    "# Randomly sample from each dataframe\n",
    "test_data_0 = test_data_0.sample(n=40000, random_state=1)\n",
    "test_data_1 = test_data_1.sample(n=10000, random_state=1)\n",
    "\n",
    "# Concatenate the two samples to get your final sample\n",
    "df_sample_test = pd.concat([test_data_0, test_data_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    40000\n",
       "1    10000\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_test.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_sample_test = df_sample_test.drop('Label', axis=1)\n",
    "y_df_sample_test = df_sample_test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:11:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training RandomForestClassifier...\n",
      "Training AdaBoostClassifier...\n",
      "Training GradientBoostingClassifier...\n",
      "Training DecisionTreeClassifier...\n",
      "Training LogisticRegression...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.782814</td>\n",
       "      <td>0.72700</td>\n",
       "      <td>0.643440</td>\n",
       "      <td>0.699588</td>\n",
       "      <td>0.651504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.742043</td>\n",
       "      <td>0.75372</td>\n",
       "      <td>0.688096</td>\n",
       "      <td>0.773775</td>\n",
       "      <td>0.697585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.633324</td>\n",
       "      <td>0.71802</td>\n",
       "      <td>0.564833</td>\n",
       "      <td>0.566700</td>\n",
       "      <td>0.565701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.73614</td>\n",
       "      <td>0.577748</td>\n",
       "      <td>0.573075</td>\n",
       "      <td>0.575113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.66344</td>\n",
       "      <td>0.616108</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>0.605086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.499212</td>\n",
       "      <td>0.79998</td>\n",
       "      <td>0.399998</td>\n",
       "      <td>0.499987</td>\n",
       "      <td>0.444438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model       AUC  Accuracy  Precision    Recall  \\\n",
       "0               XGBClassifier  0.782814   0.72700   0.643440  0.699588   \n",
       "1      RandomForestClassifier  0.742043   0.75372   0.688096  0.773775   \n",
       "2          AdaBoostClassifier  0.633324   0.71802   0.564833  0.566700   \n",
       "3  GradientBoostingClassifier  0.663500   0.73614   0.577748  0.573075   \n",
       "4      DecisionTreeClassifier  0.687400   0.66344   0.616108  0.676250   \n",
       "5          LogisticRegression  0.499212   0.79998   0.399998  0.499987   \n",
       "\n",
       "   F1 Score  \n",
       "0  0.651504  \n",
       "1  0.697585  \n",
       "2  0.565701  \n",
       "3  0.575113  \n",
       "4  0.605086  \n",
       "5  0.444438  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############NEW TEST\n",
    "\n",
    "# Prepare an empty DataFrame to store the results\n",
    "results = pd.DataFrame(\n",
    "    columns=['Model','AUC', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Iterate over models, train, make predictions and get classification metrics\n",
    "for model, name in zip(models, model_names):\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_df_sample_test)\n",
    "    y_pred_proba = model.predict_proba(X_df_sample_test)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_df_sample_test, y_pred_proba)\n",
    "    report = classification_report(y_df_sample_test, y_pred, output_dict=True)\n",
    "    accuracy = report[\"accuracy\"]\n",
    "    precision = report[\"macro avg\"][\"precision\"]\n",
    "    recall = report[\"macro avg\"][\"recall\"]\n",
    "    f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    results = results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"AUC\": auc,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1\n",
    "        },\n",
    "        ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and performing cross-validation on XGBClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:07:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:07:32] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:07:53] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:08:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:08:38] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:08:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training and performing cross-validation on RandomForestClassifier...\n",
      "Training and performing cross-validation on AdaBoostClassifier...\n",
      "Training and performing cross-validation on GradientBoostingClassifier...\n",
      "Training and performing cross-validation on DecisionTreeClassifier...\n",
      "Training and performing cross-validation on LogisticRegression...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>CV Mean AUC</th>\n",
       "      <th>CV Std AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.706191</td>\n",
       "      <td>0.699758</td>\n",
       "      <td>0.699378</td>\n",
       "      <td>0.699560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.732649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770896</td>\n",
       "      <td>0.770086</td>\n",
       "      <td>0.775883</td>\n",
       "      <td>0.769474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.628934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.603743</td>\n",
       "      <td>0.590562</td>\n",
       "      <td>0.566888</td>\n",
       "      <td>0.553355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.659422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.611921</td>\n",
       "      <td>0.604221</td>\n",
       "      <td>0.572189</td>\n",
       "      <td>0.555442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.687568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.675990</td>\n",
       "      <td>0.675287</td>\n",
       "      <td>0.679056</td>\n",
       "      <td>0.674024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.497078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.572698</td>\n",
       "      <td>0.286352</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.364150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  CV Mean AUC  CV Std AUC  Accuracy  Precision  \\\n",
       "0               XGBClassifier     0.783968         NaN  0.706191   0.699758   \n",
       "1      RandomForestClassifier     0.732649         NaN  0.770896   0.770086   \n",
       "2          AdaBoostClassifier     0.628934         NaN  0.603743   0.590562   \n",
       "3  GradientBoostingClassifier     0.659422         NaN  0.611921   0.604221   \n",
       "4      DecisionTreeClassifier     0.687568         NaN  0.675990   0.675287   \n",
       "5          LogisticRegression     0.497078         NaN  0.572698   0.286352   \n",
       "\n",
       "     Recall  F1 Score  \n",
       "0  0.699378  0.699560  \n",
       "1  0.775883  0.769474  \n",
       "2  0.566888  0.553355  \n",
       "3  0.572189  0.555442  \n",
       "4  0.679056  0.674024  \n",
       "5  0.499990  0.364150  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#crossvalidated Test\n",
    "\n",
    "# Prepare an empty DataFrame to store the results\n",
    "results = pd.DataFrame(columns=[\n",
    "    'Model', 'CV Mean AUC', 'CV Std AUC', 'Accuracy', 'Precision', 'Recall',\n",
    "    'F1 Score'\n",
    "])\n",
    "\n",
    "# Iterate over models, train, make predictions and get classification metrics\n",
    "for model, name in zip(models, model_names):\n",
    "    print(f\"Training and performing cross-validation on {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Perform cross-validation and get AUC scores\n",
    "    cv_auc = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    accuracy = report[\"accuracy\"]\n",
    "    precision = report[\"macro avg\"][\"precision\"]\n",
    "    recall = report[\"macro avg\"][\"recall\"]\n",
    "    f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    results = results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"CV Mean AUC\": cv_auc.mean(),\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1\n",
    "        },\n",
    "        ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and performing cross-validation on Random Forest...\n",
      "Training and performing cross-validation on XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:23] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:59] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:06] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:28] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:43] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:34:00] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:34:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlichtenstein/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:34:47] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grids for each model\n",
    "# For Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100],  # lower number of trees\n",
    "    'max_depth': [5],  # lower depth\n",
    "    'min_samples_split': [2, 5, 10]  # not affecting speed that much\n",
    "}\n",
    "\n",
    "# For XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100],  # lower number of gradient boosted trees\n",
    "    'max_depth': [3],  # lower depth\n",
    "    'learning_rate': [0.1,0.01,0.001]  # higher learning rate, but still reasonable to avoid underfitting\n",
    "}\n",
    "\n",
    "\n",
    "# Create the models\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search for each model\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='roc_auc')\n",
    "xgb_grid_search = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Train and perform cross-validation with grid search for Random Forest\n",
    "print(\"Training and performing cross-validation on Random Forest...\")\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "rf_cv_auc = rf_grid_search.best_score_\n",
    "\n",
    "# Train and perform cross-validation with grid search for XGBoost\n",
    "print(\"Training and performing cross-validation on XGBoost...\")\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "xgb_best_model = xgb_grid_search.best_estimator_\n",
    "xgb_cv_auc = xgb_grid_search.best_score_\n",
    "\n",
    "# Evaluate the best models on the test set\n",
    "rf_best_model.fit(X_train, y_train)\n",
    "rf_y_pred = rf_best_model.predict(X_test)\n",
    "rf_y_pred_proba = rf_best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "xgb_best_model.fit(X_train, y_train)\n",
    "xgb_y_pred = xgb_best_model.predict(X_test)\n",
    "xgb_y_pred_proba = xgb_best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate classification metrics for the best models\n",
    "rf_report = classification_report(y_test, rf_y_pred, output_dict=True)\n",
    "rf_accuracy = rf_report[\"accuracy\"]\n",
    "rf_precision = rf_report[\"macro avg\"][\"precision\"]\n",
    "rf_recall = rf_report[\"macro avg\"][\"recall\"]\n",
    "rf_f1 = rf_report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "xgb_report = classification_report(y_test, xgb_y_pred, output_dict=True)\n",
    "xgb_accuracy = xgb_report[\"accuracy\"]\n",
    "xgb_precision = xgb_report[\"macro avg\"][\"precision\"]\n",
    "xgb_recall = xgb_report[\"macro avg\"][\"recall\"]\n",
    "xgb_f1 = xgb_report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "# Append the results to the DataFrame\n",
    "results = results.append(\n",
    "    {\n",
    "        \"Model\": \"Random Forest (Grid Search)\",\n",
    "        \"AUC\": rf_cv_auc,\n",
    "        \"Accuracy\": rf_accuracy,\n",
    "        \"Precision\": rf_precision,\n",
    "        \"Recall\": rf_recall,\n",
    "        \"F1 Score\": rf_f1\n",
    "    },\n",
    "    ignore_index=True)\n",
    "\n",
    "results = results.append(\n",
    "    {\n",
    "        \"Model\": \"XGBoost (Grid Search)\",\n",
    "        \"AUC\": xgb_cv_auc,\n",
    "        \"Accuracy\": xgb_accuracy,\n",
    "        \"Precision\": xgb_precision,\n",
    "        \"Recall\": xgb_recall,\n",
    "        \"F1 Score\": xgb_f1\n",
    "    },\n",
    "    ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>CV Mean AUC</th>\n",
       "      <th>CV Std AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.706191</td>\n",
       "      <td>0.699758</td>\n",
       "      <td>0.699378</td>\n",
       "      <td>0.699560</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.732649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770896</td>\n",
       "      <td>0.770086</td>\n",
       "      <td>0.775883</td>\n",
       "      <td>0.769474</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.628934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.603743</td>\n",
       "      <td>0.590562</td>\n",
       "      <td>0.566888</td>\n",
       "      <td>0.553355</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.659422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.611921</td>\n",
       "      <td>0.604221</td>\n",
       "      <td>0.572189</td>\n",
       "      <td>0.555442</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.687568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.675990</td>\n",
       "      <td>0.675287</td>\n",
       "      <td>0.679056</td>\n",
       "      <td>0.674024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.497078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.572698</td>\n",
       "      <td>0.286352</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.364150</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (Grid Search)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.588001</td>\n",
       "      <td>0.599699</td>\n",
       "      <td>0.525654</td>\n",
       "      <td>0.446826</td>\n",
       "      <td>0.626534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost (Grid Search)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.615298</td>\n",
       "      <td>0.605452</td>\n",
       "      <td>0.580324</td>\n",
       "      <td>0.569896</td>\n",
       "      <td>0.661160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  CV Mean AUC  CV Std AUC  Accuracy  Precision  \\\n",
       "0                XGBClassifier     0.783968         NaN  0.706191   0.699758   \n",
       "1       RandomForestClassifier     0.732649         NaN  0.770896   0.770086   \n",
       "2           AdaBoostClassifier     0.628934         NaN  0.603743   0.590562   \n",
       "3   GradientBoostingClassifier     0.659422         NaN  0.611921   0.604221   \n",
       "4       DecisionTreeClassifier     0.687568         NaN  0.675990   0.675287   \n",
       "5           LogisticRegression     0.497078         NaN  0.572698   0.286352   \n",
       "6  Random Forest (Grid Search)          NaN         NaN  0.588001   0.599699   \n",
       "7        XGBoost (Grid Search)          NaN         NaN  0.615298   0.605452   \n",
       "\n",
       "     Recall  F1 Score       AUC  \n",
       "0  0.699378  0.699560       NaN  \n",
       "1  0.775883  0.769474       NaN  \n",
       "2  0.566888  0.553355       NaN  \n",
       "3  0.572189  0.555442       NaN  \n",
       "4  0.679056  0.674024       NaN  \n",
       "5  0.499990  0.364150       NaN  \n",
       "6  0.525654  0.446826  0.626534  \n",
       "7  0.580324  0.569896  0.661160  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

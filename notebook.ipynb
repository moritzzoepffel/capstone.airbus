{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"data/msn_02_fuel_leak_signals_preprocessed.csv\", \"data/msn_10_fuel_leak_signals_preprocessed.csv\", \"data/msn_11_fuel_leak_signals_preprocessed.csv\", \"data/msn_12_fuel_leak_signals_preprocessed.csv\", \"data/msn_14_fuel_leak_signals_preprocessed.csv\", \"data/msn_29_fuel_leak_signals_preprocessed.csv\", \"data/msn_37_fuel_leak_signals_preprocessed.csv\", \"data/msn_53_fuel_leak_signals_preprocessed.csv\"]\n",
    "\n",
    "datasets = []\n",
    "for filename in filenames:\n",
    "    datasets.append(pd.read_csv(filename, sep=\";\"))\n",
    "    \n",
    "print(\"Datasets loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset.info(verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NaN values\n",
    "for dataset in datasets:\n",
    "    # print percentage of NaN values if more than 0.8\n",
    "    for column in dataset.columns:\n",
    "        if dataset[column].isna().sum() > 0.7 * len(dataset[column]):\n",
    "            print(\"NaN percentage in column \" + column + \": \" + str(dataset[column].isna().sum() / len(dataset[column])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for all datasets numercial values\n",
    "#for dataset in datasets:\n",
    "    #print(dataset.corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw plots for all datasets for column LEAK_DETECTION_LEAK_FLOW\n",
    "for i in range(len(datasets)):\n",
    "    print(filenames[i], datasets[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw plots for all datasets for columns VALUE_FUEL_QTY_FT1;VALUE_FUEL_QTY_FT2;VALUE_FUEL_QTY_FT3;VALUE_FUEL_QTY_FT4;VALUE_FUEL_QTY_LXT;VALUE_FUEL_QTY_RXT\n",
    "#for dataset in datasets:\n",
    " #   dataset.plot.scatter(x='UTC_TIME', y='VALUE_FUEL_QTY_FT1')\n",
    "    #dataset.plot.scatter(x='UTC_TIME', y='VALUE_FUEL_QTY_FT2')\n",
    "    #dataset.plot.scatter(x='UTC_TIME', y='VALUE_FUEL_QTY_FT3')\n",
    "    #dataset.plot.scatter(x='UTC_TIME', y='VALUE_FUEL_QTY_FT4')\n",
    "    #dataset.plot.scatter(x='UTC_TIME', y='VALUE_FUEL_QTY_LXT')\n",
    "    #dataset.plot.scatter(x='UTC_TIME', y='VALUE_FUEL_QTY_RXT')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets[0].plot.scatter(x='UTC_TIME', y='VALUE_FUEL_QTY_FT1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert UTC_TIME to datetime\n",
    "for dataset in datasets:\n",
    "    dataset['UTC_TIME'] = pd.to_datetime(dataset['UTC_TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    #introduce column date\n",
    "    dataset['DATE'] = dataset['UTC_TIME'].dt.date\n",
    "    dataset[\"MONTH\"] = dataset['UTC_TIME'].dt.month\n",
    "    dataset[\"DAY\"] = dataset['UTC_TIME'].dt.day\n",
    "    dataset[\"HOUR\"] = dataset['UTC_TIME'].dt.hour\n",
    "    dataset[\"MINUTE\"] = dataset['UTC_TIME'].dt.minute\n",
    "    dataset[\"SECOND\"] = dataset['UTC_TIME'].dt.second\n",
    "    \n",
    "# number of seconds since epoch \n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for dataset 0 for column LEAK_DETECTION_LEAK_FLOW\n",
    "datasets[0].plot.scatter(x='UTC_TIME', y='LEAK_DETECTION_LEAK_FLOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_datasets(dataset):\n",
    "    # List of columns to plot for VALUE_FUEL_QTY_CT;VALUE_FUEL_QTY_FT1;VALUE_FUEL_QTY_FT2;VALUE_FUEL_QTY_FT3;VALUE_FUEL_QTY_FT4;VALUE_FUEL_QTY_LXT;VALUE_FUEL_QTY_RXT\n",
    "    fuel_qty_cols = ['VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3', 'VALUE_FUEL_QTY_FT4']\n",
    "\n",
    "    # Loop over every unique date\n",
    "    for date in dataset['DATE'].unique():\n",
    "        for flight in dataset['MSN'].unique():\n",
    "            # Create a subplot\n",
    "            fig = sp.make_subplots(rows=3, cols=3)\n",
    "            \n",
    "            # size of fig \n",
    "            fig.update_layout(height=1400, width=1400)\n",
    "\n",
    "            # Loop over each column\n",
    "            for i, col in enumerate(fuel_qty_cols):\n",
    "                # save the dataset for the current date and flight\n",
    "                dataset_tmp = dataset[(dataset['DATE'] == date) & (dataset['MSN'] == flight)]\n",
    "                # Add scatter plot to subplot\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=dataset_tmp['UTC_TIME'], \n",
    "                            y=dataset_tmp[col], \n",
    "                            mode='markers',\n",
    "                            name=col),\n",
    "                    row=i//3 + 1, \n",
    "                    col=i%3 + 1\n",
    "                )\n",
    "\n",
    "                # Update xaxis and yaxis titles\n",
    "                fig.update_xaxes(title_text='UTC_TIME', row=i//3 + 1, col=i%3 + 1)\n",
    "                fig.update_yaxes(title_text=col + \" \" + str(date), row=i//3 + 1, col=i%3 + 1)\n",
    "\n",
    "            # Show the plot\n",
    "            #fig.show()\n",
    "            \n",
    "            # save the plot as png\n",
    "            fig.write_image(\"plots2/\" + str(date) +\"-\" + str(flight) + \".png\")\n",
    "     \n",
    "            \n",
    "#plot_datasets(datasets[1])\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    if i > 1:\n",
    "        #plot_datasets(datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce new columns for each dataset [CT_DIFF, LXT_DIFF, RXT_DIFF, FT1_DIFF, FT2_DIFF, FT3_DIFF, FT4_DIFF]\n",
    "# value represents the difference between the current and the previous value\n",
    "for dataset in datasets:\n",
    "    dataset['CT_DIFF'] = dataset['VALUE_FUEL_QTY_CT'].diff()\n",
    "    dataset['LXT_DIFF'] = dataset['VALUE_FUEL_QTY_LXT'].diff()\n",
    "    dataset['RXT_DIFF'] = dataset['VALUE_FUEL_QTY_RXT'].diff()\n",
    "    dataset['FT1_DIFF'] = dataset['VALUE_FUEL_QTY_FT1'].diff()\n",
    "    dataset['FT2_DIFF'] = dataset['VALUE_FUEL_QTY_FT2'].diff()\n",
    "    dataset['FT3_DIFF'] = dataset['VALUE_FUEL_QTY_FT3'].diff()\n",
    "    dataset['FT4_DIFF'] = dataset['VALUE_FUEL_QTY_FT4'].diff()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff(): \n",
    "    #plot diff columns for dataset 0 for each unique date\n",
    "    for date in datasets[0]['DATE'].unique():\n",
    "        for flight in datasets[0]['MSN'].unique():\n",
    "            # create subplot 3 columns and len(datasets[0].unique) rows\n",
    "            fig = sp.make_subplots(rows=3, cols=3)\n",
    "            \n",
    "            # size of fig\n",
    "            fig.update_layout(height=1000, width=1000)\n",
    "            \n",
    "            diff_cols = ['CT_DIFF', 'LXT_DIFF', 'RXT_DIFF', 'FT1_DIFF', 'FT2_DIFF', 'FT3_DIFF', 'FT4_DIFF']\n",
    "            \n",
    "            # loop over the diff columns\n",
    "            for i, col in enumerate(diff_cols):\n",
    "                # save the dataset for the current date and flight\n",
    "                dataset_tmp = datasets[0][(datasets[0]['DATE'] == date) & (datasets[0]['MSN'] == flight)]\n",
    "                # add scatter plot to subplot\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=dataset_tmp['UTC_TIME'], \n",
    "                            y=dataset_tmp[col], \n",
    "                            mode='markers',\n",
    "                            name=col),\n",
    "                    row=i//3 + 1, \n",
    "                    col=i%3 + 1\n",
    "                )\n",
    "\n",
    "                # update xaxis and yaxis titles\n",
    "                fig.update_xaxes(title_text='UTC_TIME', row=i//3 + 1, col=i%3 + 1)\n",
    "                fig.update_yaxes(title_text=col + \" \" + str(date), row=i//3 + 1, col=i%3 + 1)\n",
    "                \n",
    "            # save fig as png \n",
    "            fig.write_image(\"plots2/diff_plots/\" + str(date) + \"-\" + str(flight) + \"_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas\n",
    "# 1. use lower bound and upper bound to detect outliers\n",
    "# 2. use the bounds to plot the data and see if there are any outliers\n",
    "# 3. deep learning auto detection anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use deep learning to detect anomalies\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/introduction-time-series-classification/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "prof = ProfileReport(datasets[0]) \n",
    "prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate synthetic data for each dataset including a simulated fuel leak\n",
    "# using 0,5l, 1L and 5L as the leak size per minute\n",
    "\n",
    "def generate_synthetic_data(dataset, leak_size):\n",
    "    # create a copy of the dataset\n",
    "    synthetic_dataset = dataset.copy()\n",
    "    \n",
    "    synthetic_dataset['VALUE_FUEL_QTY_CT'] = synthetic_dataset['VALUE_FUEL_QTY_CT'] - (leak_size / 60)\n",
    "    \n",
    "    return synthetic_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the fuel on board and the Fuel Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_used_cols = ['FUEL_USED_1', 'FUEL_USED_2', 'FUEL_USED_3', 'FUEL_USED_4']\n",
    "\n",
    "# save the sum of the fuel used columns in a new column\n",
    "datasets[0]['FUEL_USED_SUM'] = datasets[0][fuel_used_cols].sum(axis=1)\n",
    "\n",
    "fuel_cols = ['VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3', 'VALUE_FUEL_QTY_FT4']\n",
    " \n",
    "datasets[0]['FUEL_COLS_SUM'] = datasets[0][fuel_cols].sum(axis=1)\n",
    "\n",
    "#datasets[0]['FUEL_DIFF_SUM'] = datasets[0]['FUEL_DIFF_SUM'].cumsum()\n",
    "\n",
    "# substract from each value in FUEL_COLS_SUM the first value bigger than 1 and get the absoulte value. Do this for each day\n",
    "datasets[0]['FUEL_COLS_SUM'] = datasets[0].groupby(['DATE'])['FUEL_COLS_SUM'].transform(lambda x: x - x[x > 1].iloc[0]).abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "datasets[0][datasets[0]['DATE'] == datetime.date(2011, 3, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# plot the fuel_used columns for dataset 0 for 2010-10-26\n",
    "fig = go.Figure()\n",
    " \n",
    "dates_to_find = datasets[0]['DATE'].unique()[0:1]\n",
    "\n",
    "for date in dates_to_find:\n",
    "    # plot FUEL_USED_SUM and FUEL_DIFF_SUM\n",
    "    fig.add_trace(go.Scatter(x=datasets[0][datasets[0]['DATE'] == date_to_find]['UTC_TIME'],\n",
    "                                y=datasets[0][datasets[0]['DATE'] == date_to_find]['FUEL_USED_SUM'],\n",
    "                                mode='markers',\n",
    "                                name='FUEL_USED_SUM'))\n",
    "    fig.add_trace(go.Scatter(x=datasets[0][datasets[0]['DATE'] == date_to_find]['UTC_TIME'],\n",
    "                                y=datasets[0][datasets[0]['DATE'] == date_to_find]['FUEL_COLS_SUM'],\n",
    "                                mode='markers',\n",
    "                                name='FUEL_COLS_SUM'))\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbus Impact Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Imports](#1-imports)\n",
    "    - [Import Datasets](#10-import-datasets)\n",
    "2. [EDA](#define-date-and-time-columns)\n",
    "3. [New Features](#new-features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "from numpy import quantile, where, random\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from matplotlib import pyplot as plt, dates\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/msn_02_fuel_leak_signals_preprocessed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4n/swp60yyd37qg_dmlq1v1_6jw0000gn/T/ipykernel_63827/4240350578.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Copy datasets for leak simulation and add to datasets list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/msn_02_fuel_leak_signals_preprocessed.csv'"
     ]
    }
   ],
   "source": [
    "filenames = [\"data/msn_02_fuel_leak_signals_preprocessed.csv\", \n",
    "             \"data/msn_10_fuel_leak_signals_preprocessed.csv\", \n",
    "             \"data/msn_11_fuel_leak_signals_preprocessed.csv\", \n",
    "             \"data/msn_12_fuel_leak_signals_preprocessed.csv\", \n",
    "             \"data/msn_14_fuel_leak_signals_preprocessed.csv\", \n",
    "             \"data/msn_29_fuel_leak_signals_preprocessed.csv\", \n",
    "             \"data/msn_37_fuel_leak_signals_preprocessed.csv\", \n",
    "             \"data/msn_53_fuel_leak_signals_preprocessed.csv\"]\n",
    "\n",
    "datasets = []\n",
    "datasets_labels = [\"MSN_02\", \"MSN_10\", \"MSN_11\", \"MSN_12\", \"MSN_14\", \"MSN_29\", \"MSN_37\", \"MSN_53\", \"MSN_37_LEAK0.5\", \"MSN_37_LEAK1.0\", \"MSN_37_LEAK5.0\"]\n",
    "                   \n",
    "for filename in filenames:\n",
    "    datasets.append(pd.read_csv(filename, sep=\";\"))\n",
    "    \n",
    "# Copy datasets for leak simulation and add to datasets list\n",
    "for i in range(3):\n",
    "    datasets.append(datasets[6].copy())\n",
    "    \n",
    "print(\"Datasets loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define Date and Time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    #introduce column date\n",
    "    dataset['UTC_TIME'] = pd.to_datetime(dataset['UTC_TIME'])\n",
    "    dataset['DATE'] = dataset['UTC_TIME'].dt.date\n",
    "    dataset[\"TIME\"] = dataset['UTC_TIME'].dt.time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2. Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Check for Records and Features for every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "data['Records'] = [dataset.shape[0] for dataset in datasets]\n",
    "data['Features'] = [dataset.shape[1] for dataset in datasets]\n",
    "\n",
    "pd.DataFrame(data, index = datasets_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Display columns in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset.info(verbose=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Total Null values and relative Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'NULL_VALUES': datasets[0].isnull().sum(), 'NULL_VALUES_%': datasets[0].isnull().sum() / datasets[0].shape[0] * 100, 'UNIQUE VALUES': datasets[0].nunique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for all datasets\n",
    "df_feat = pd.DataFrame()\n",
    "\n",
    "for i in range(1, len(datasets)):\n",
    "    df_tmp = pd.DataFrame({f'{datasets_labels[i]}_miss': datasets[i].isnull().sum(), \n",
    "                           f'{datasets_labels[i]}_miss_%': datasets[i].isnull().sum() / datasets[i].shape[0] * 100, \n",
    "                           f'{datasets_labels[i]}_unique': datasets[i].nunique()})\n",
    "    df_feat = pd.concat([df_feat, df_tmp], axis=1)\n",
    "    \n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print relative amount of flight phase 8\n",
    "print(\"Relative amount of flight phases 3-11\")\n",
    "for dataset in datasets:\n",
    "    print(dataset[dataset['FLIGHT_PHASE_COUNT'].isin(list(range(3,12)))].shape[0] / dataset.shape[0] * 100)\n",
    "    \n",
    "print(\"\\nRelative amount of rows with NaN values in flight phases 3-11\")\n",
    "# count rows with NaN values in flight phases 3-11\n",
    "for dataset in datasets:\n",
    "    print(dataset[dataset['FLIGHT_PHASE_COUNT'].isin(list(range(3,12)))].isnull().any(axis=1).sum() / dataset.shape[0] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_total = sum([dataset.shape[0] for dataset in datasets])\n",
    "records_dropna = sum([dataset.dropna().shape[0] for dataset in datasets])\n",
    "\n",
    "records_dropna_flightphase = {\n",
    "    i: sum([dataset[dataset['FLIGHT_PHASE_COUNT'] == i].dropna().shape[0] for dataset in datasets])\n",
    "    for i in range(1, 13)\n",
    "}\n",
    "\n",
    "print(\"Total records:\", records_total)\n",
    "print(\"Records after dropna:\", records_dropna)\n",
    "print(\"Records after dropna per flight phase:\", records_dropna_flightphase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with duplicats in UTC_TIME\n",
    "for dataset in datasets:\n",
    "    dataset.drop_duplicates(subset=['UTC_TIME'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame(records_dropna_flightphase, index=['Records']).transpose()\n",
    " \n",
    "fig = px.bar(df_tmp, x=df_tmp.index, y='Records', title='Records after dropna per flight phase')\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate std of diff in tanks by flight-phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dropna = [df.dropna() for df in datasets]\n",
    "all_datasets = pd.concat(datasets_dropna, ignore_index=True)\n",
    "\n",
    "columns_to_describe = [\n",
    "    'FUEL_USED_1', \n",
    "    'FUEL_USED_2', \n",
    "    'FUEL_USED_3', \n",
    "    'FUEL_USED_4', \n",
    "    'VALUE_FUEL_QTY_CT', \n",
    "    'VALUE_FUEL_QTY_RXT', \n",
    "    'VALUE_FUEL_QTY_LXT', \n",
    "    'VALUE_FUEL_QTY_FT1', \n",
    "    'VALUE_FUEL_QTY_FT2', \n",
    "    'VALUE_FUEL_QTY_FT3', \n",
    "    'VALUE_FUEL_QTY_FT4'\n",
    "]\n",
    "\n",
    "all_datasets_diff = all_datasets.groupby('FLIGHT_PHASE_COUNT')[columns_to_describe].diff().dropna()\n",
    "std_df = all_datasets_diff[columns_to_describe].std().reset_index()\n",
    "std_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the examples above, flight phases two and eight are the ones that contain the most data. The fact that flight phase two describes the aircraft on the ground and that the parameters there could be falsified, for example, by refuelling or artificially resetting the fuel consumption, and the requirements of Airbus have prompted us to adopt only flight phase eight in our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 3. Data Cleaning and Feature Engineering\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Assign to each concetenated flight a unique flight number and only keep flight_phases 8\n",
    "\n",
    "In the next cell we will divide the data into individual flight segments, which will be assigned unique numbers. This has the advantage that we can analyse individual sequences and describe them further in the EDA process. Before and after this, zero values are dropped.\n",
    "\n",
    "We're keeping flight phases 2 and 8 as they have a lot of values that could be used for anomaly detection. \n",
    "\n",
    "We're cleaning the second flight_phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_where_to_drop_nan = datasets[2].columns\n",
    "\n",
    "def introduce_flight_column(dataset):\n",
    "    # copy dataset\n",
    "    dataset_tmp = dataset.copy()\n",
    "    \n",
    "    # Drop NaN values\n",
    "    dataset_tmp = dataset_tmp.dropna(subset=columns_where_to_drop_nan)\n",
    "    \n",
    "    # Variable to store flight number\n",
    "    flight_number = 0\n",
    "    \n",
    "    # assign flight number to rows that have indexes following each other\n",
    "    # if the difference between two indexes is greater than 1, a new flight is assumed\n",
    "    # print(dataset_tmp.shape)\n",
    "    \n",
    "    for i in range(1, dataset_tmp.shape[0]):\n",
    "        if dataset_tmp.index[i] - dataset_tmp.index[i-1] > 1:\n",
    "            flight_number += 1\n",
    "        dataset_tmp.at[dataset_tmp.index[i], 'FLIGHT'] = flight_number\n",
    "        \n",
    "    # Fill nan values in flight column with 0\n",
    "    dataset_tmp['FLIGHT'] = dataset_tmp['FLIGHT'].fillna(0)\n",
    "    \n",
    "    # only keep rows with flight_phase_count = 2 and 8\n",
    "    dataset_tmp =  dataset_tmp[dataset_tmp['FLIGHT_PHASE_COUNT'].isin([2, 8])]\n",
    "            \n",
    "    # return dataset with flight column\n",
    "    return dataset_tmp\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_unique_flights = [introduce_flight_column(dataset) for dataset in datasets_dropna]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_unique_flights:\n",
    "    print(dataset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Add synthetic Fuel Leak to different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leak(dataset, leak_flow):\n",
    "    # create copy of dataset    \n",
    "    dataset_tmp = dataset.copy()\n",
    "    \n",
    "    # define FUEL_TANK_COLS \n",
    "    FUEL_TANK_COLS = ['VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3', 'VALUE_FUEL_QTY_FT4']\n",
    "    \n",
    "    # leak flow in L/sec\n",
    "    leak_flow = leak_flow / 60\n",
    "    \n",
    "    # substract leak flow from VALUE_FOB \n",
    "    dataset_tmp['VALUE_FOB'] = dataset_tmp['VALUE_FOB'] - leak_flow\n",
    "\n",
    "    print('Before Tank chosen')\n",
    "\n",
    "    # choose random fuel tank where the fuel leak is happening\n",
    "    while True:\n",
    "        fuel_tank = random.choice(FUEL_TANK_COLS)\n",
    "     \n",
    "        # check if fuel tank is empty at any point in time\n",
    "        if dataset_tmp[fuel_tank].max() >= leak_flow:\n",
    "            break\n",
    "            \n",
    "    print('Fuel Tank chosen')\n",
    "            \n",
    "    # substract leak flow from fuel tank\n",
    "    dataset_tmp[fuel_tank] = dataset_tmp[fuel_tank] - leak_flow\n",
    "    \n",
    "    # set negative values in value_fob and fuel_tank to 0\n",
    "    dataset_tmp['VALUE_FOB'] = dataset_tmp['VALUE_FOB'].clip(lower=0)\n",
    "\n",
    "    \n",
    "    return dataset_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this point the following changes will be applied to two list of datasets\n",
    "datasets_unique_flights_with_fuel_leak = datasets_unique_flights.copy()\n",
    "\n",
    "# add fuel leak to the last 3 datasets. First one 0.5L per minute, second 1L per minute, third 5L per minute\n",
    "datasets_unique_flights_with_fuel_leak[-1] = create_leak(datasets_unique_flights[-1], 5)\n",
    "datasets_unique_flights_with_fuel_leak[-2] = create_leak(datasets_unique_flights[-2], 1)\n",
    "datasets_unique_flights_with_fuel_leak[-3] = create_leak(datasets_unique_flights[-3], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_unique_flights:\n",
    "    print(dataset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don't have a single value in Dataset 2 we gonna drop it at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop second dataset from datasets_unique_flights\n",
    "#datasets_unique_flights.pop(1)\n",
    "#datasets_unique_flights_with_fuel_leak.pop(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to subtract first value of a series from all values\n",
    "def subtract_first(x):\n",
    "    return x.iloc[0] - x\n",
    "\n",
    "def define_new_features(dataset):\n",
    "    # copy dataset\n",
    "    dataset_tmp = dataset.copy()\n",
    "    \n",
    "    # initialize TOTAL_FUEK_USED column\n",
    "    dataset_tmp['TOTAL_FUEL_USED'] = 0\n",
    "     \n",
    "    # Total fuel column is the sum of all fuel used columns\n",
    "    for i in range(1, 5):\n",
    "        dataset_tmp['TOTAL_FUEL_USED'] += dataset_tmp['FUEL_USED_' + str(i)]\n",
    "        \n",
    "    # Get difference for VALUE_FOB. For each flight the first value must be 0\n",
    "    dataset_tmp['VALUE_FOB_DIFF'] = dataset_tmp.groupby('FLIGHT')['VALUE_FOB'].diff()\n",
    "    dataset_tmp['VALUE_FOB_DIFF'] = dataset_tmp['VALUE_FOB_DIFF'].fillna(0)\n",
    "    \n",
    "    dataset_tmp['TOTAL_FOB_BY_QTY'] = dataset_tmp['VALUE_FUEL_QTY_CT'] + dataset_tmp['VALUE_FUEL_QTY_FT1'] + dataset_tmp['VALUE_FUEL_QTY_FT2'] + dataset_tmp['VALUE_FUEL_QTY_FT3'] + dataset_tmp['VALUE_FUEL_QTY_FT4'] + dataset_tmp['VALUE_FUEL_QTY_LXT'] + dataset_tmp['VALUE_FUEL_QTY_RXT']\n",
    "    \n",
    "    dataset_tmp['DELTA_VFOB_VS_VFOBQTY'] = dataset_tmp['VALUE_FOB'] - dataset_tmp['TOTAL_FOB_BY_QTY']\n",
    "    \n",
    "    dataset_tmp['ALTITUDE_DIFF'] = dataset_tmp['FW_GEO_ALTITUDE'].diff().abs() \n",
    "    \n",
    "    dataset_tmp['VALUE_FOB_MISSING'] = dataset.groupby('FLIGHT')['VALUE_FOB'].transform(subtract_first)  \n",
    "    \n",
    "    # code to remove first part of flight where the plane is on the ground and the fuel used is resetted\n",
    "    for flight_num in dataset_tmp['FLIGHT'].unique():\n",
    "        # get first index of flight \n",
    "        first_index = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num].index[0]\n",
    "        \n",
    "        # get last index of flight\n",
    "        last_index = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num].index[-1]\n",
    "        \n",
    "        # get location of min value of VALUE_FOB per flight \n",
    "        min_value_sum_fuel_used = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num]['TOTAL_FUEL_USED'].idxmin()\n",
    "        \n",
    "        # delete in dataset_tmp all rows between first_location and min_value_fob\n",
    "        dataset_tmp.drop(dataset_tmp.loc[first_index:min_value_sum_fuel_used].index, inplace=True) \n",
    "    \n",
    "    # for each FLIGHT substract the first value of TOTAL_FUEL_USED from all values\n",
    "    #for flight in dataset_tmp['FLIGHT'].unique():\n",
    "    #    dataset_tmp.loc[dataset_tmp['FLIGHT'] == flight, 'TOTAL_FUEL_USED'] -= dataset_tmp[dataset_tmp['FLIGHT'] == flight]['TOTAL_FUEL_USED'].iloc[0]\n",
    "    \n",
    "    return dataset_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_new_features = [define_new_features(dataset) for dataset in datasets_unique_flights]\n",
    "datasets_with_new_features_with_fuel_leak = [define_new_features(dataset) for dataset in datasets_unique_flights_with_fuel_leak]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of datasets to see total number of features\n",
    "for dataset in datasets_with_new_features:\n",
    "    print(dataset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Deleting Values before all Fuel_Used Columns are resetted \n",
    "Not necassary if only flight phase 8 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_flights(dataset):\n",
    "    # copy dataset\n",
    "    dataset_tmp = dataset.copy()\n",
    "     \n",
    "    for flight_num in dataset_tmp['FLIGHT'].unique():\n",
    "        # get first index of flight \n",
    "        first_index = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num].index[0]\n",
    "        \n",
    "        # get last index of flight\n",
    "        last_index = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num].index[-1]\n",
    "        \n",
    "        # get location of min value of VALUE_FOB per flight \n",
    "        min_value_sum_fuel_used = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num]['TOTAL_FUEL_USED'].idxmin()\n",
    "        \n",
    "        # delete in dataset_tmp all rows between first_location and min_value_fob\n",
    "        dataset_tmp.drop(dataset_tmp.loc[first_index:min_value_sum_fuel_used].index, inplace=True)\n",
    "        \n",
    "        # dataset_tmp[\"VALUE_FOB_MISSING\"] = dataset_tmp.groupby('FLIGHT')['VALUE_FOB'].transform(subtract_first)\n",
    "        \n",
    "    return dataset_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necassary if only flight phase 8 is used\n",
    "# datasets_without_reset_engines = [initalize_flights(dataset) for dataset in datasets_with_new_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(datasets_with_new_features):\n",
    "    print(f'Dataset {i} shape: {dataset.shape}')\n",
    "    print(f'Dataset {i} unique flights: {dataset[\"FLIGHT\"].nunique()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Altitude vs Value Fob for dataset 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Altitude vs Value Fob for the rest of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_phase_dict = {\n",
    "    1: 'Pre-flight',\n",
    "    2: 'Engine Run',\n",
    "    3: 'Take-Off 1',\n",
    "    4: 'Take-Off 2',\n",
    "    5: 'Take-Off 3',\n",
    "    6: 'Climbing 1',\n",
    "    7: 'Climbing 2',\n",
    "    8: 'Cruise',\n",
    "    9: 'Descent',\n",
    "    10: 'Approach',\n",
    "    11: 'Landing',\n",
    "    12: 'Post-flight'\n",
    "}\n",
    "\n",
    "def plot_flight(dataset, min_flight_length=100, max_graphs=10):\n",
    "    for flight_num in dataset[\"FLIGHT\"].unique(): \n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Define data for flight\n",
    "        flight_data = dataset[dataset['FLIGHT'] == flight_num]\n",
    "        \n",
    "        if flight_data.shape[0] < min_flight_length:\n",
    "            continue\n",
    "        \n",
    "        # Define traces\n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                y=flight_data['VALUE_FOB'],\n",
    "                                mode='lines',\n",
    "                                name='VALUE_FOB')) \n",
    "\n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                y=flight_data['FW_GEO_ALTITUDE'],\n",
    "                                mode='lines',\n",
    "                                name='FW_GEO_ALTITUDE'))\n",
    "        \n",
    "        # add trace for total_fuel_used\n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                    y=flight_data['TOTAL_FUEL_USED'],\n",
    "                                    mode='lines',\n",
    "                                    name='TOTAL_FUEL_USED'))\n",
    "        \n",
    "        # add trace for VALUE_FOB_DIFF\n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                    y=flight_data['VALUE_FOB_DIFF'],\n",
    "                                    mode='lines',\n",
    "                                    name='VALUE_FOB_DIFF'))\n",
    "        \n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                    y=flight_data['VALUE_FOB_MISSING'],\n",
    "                                    mode='lines',\n",
    "                                    name='VALUE_FOB_MISSING'))\n",
    "        \n",
    "        \n",
    "        for flight_phase in flight_data['FLIGHT_PHASE_COUNT'].unique():\n",
    "            fig.add_trace(go.Scatter(x=flight_data[flight_data['FLIGHT_PHASE_COUNT'] == flight_phase]['UTC_TIME'],\n",
    "                                    y=flight_data[flight_data['FLIGHT_PHASE_COUNT'] == flight_phase]['VALUE_FOB'],\n",
    "                                    mode='lines',\n",
    "                                        name=f'{flight_phase}, {flight_phase_dict[flight_phase]}'))\n",
    "\n",
    "        fig.update_layout(title=f'VALUE_FOB and FW_GEO_ALTITUDE for flight {flight_num}',\n",
    "                        xaxis_title='UTC_TIME',\n",
    "                        yaxis_title='VALUE_FOB and FW_GEO_ALTITUDE')\n",
    "        \n",
    "        # Define rectangles for different flight phases\n",
    "        shapes = []\n",
    "        colors = [\n",
    "        'LightSkyBlue', 'LightSalmon', 'LightGreen', 'LightYellow', 'LightPink', \n",
    "        'MediumPurple', 'DarkOrange', 'MediumSeaGreen', 'DeepSkyBlue']\n",
    "        # Define more colors if needed\n",
    "        for i, flight_phase in enumerate(flight_data['FLIGHT_PHASE_COUNT'].unique()):\n",
    "            phase_data = flight_data[flight_data['FLIGHT_PHASE_COUNT'] == flight_phase]\n",
    "            shapes.append(\n",
    "                go.layout.Shape(\n",
    "                    type=\"rect\",\n",
    "                    xref=\"x\",\n",
    "                    yref=\"paper\",  # use 'paper' to refer to the entire y range\n",
    "                    x0=phase_data['UTC_TIME'].min(),\n",
    "                    x1=phase_data['UTC_TIME'].max(),\n",
    "                    y0=0,\n",
    "                    y1=1,\n",
    "                    fillcolor=colors[i % len(colors)],\n",
    "                    opacity=0.5,\n",
    "                    layer=\"below\",\n",
    "                    line_width=0,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(shapes=shapes)\n",
    "        fig.show()\n",
    "        \n",
    "        max_graphs -= 1\n",
    "        \n",
    "        if(max_graphs == 0):    \n",
    "            break\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flight(datasets_with_new_features[-1], min_flight_length=500, max_graphs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flight(datasets_with_new_features_with_fuel_leak[-1], min_flight_length=500, max_graphs=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring all datasets to the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to drop so that all datasets have the same columns\n",
    "columns_to_drop = [column for column in datasets_with_new_features[0].columns if column not in datasets_with_new_features[2].columns]\n",
    "\n",
    "datasets_with_new_features[0].drop(columns_to_drop, axis=1, inplace=True)\n",
    "datasets_with_new_features_with_fuel_leak[0].drop(columns_to_drop, axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of datasets\n",
    "for i, dataset in enumerate(datasets_with_new_features):\n",
    "    print(f'Dataset {i} shape: {dataset.shape}')\n",
    "    print(f'Dataset {i} unique flights: {dataset[\"FLIGHT\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_new_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the columns \n",
    "for i, dataset in enumerate(datasets_with_new_features):\n",
    "    print(f'Dataset {i} shape: {dataset.shape}')\n",
    "    print(f'Dataset {i} unique flights: {dataset[\"FLIGHT\"].nunique()}')\n",
    "    dataset.hist(figsize=(20,20))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Outliers\n",
    "\n",
    "Since out data is not normally distributed most of the time we're using IQR for the outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of outliers outside of 95% confidence interval\n",
    "def drop_outliers(dataset):\n",
    "    # create copy of dataset\n",
    "    dataset_tmp = dataset.copy()\n",
    "    \n",
    "    for column in dataset_tmp.columns:\n",
    "        # skip columns that are not numeric\n",
    "        if dataset_tmp[column].dtype != 'float64':\n",
    "            continue\n",
    "        \n",
    "        # calculate 95% confidence interval\n",
    "        lower_bound, upper_bound = np.percentile(dataset_tmp[column], [2.5, 97.5])\n",
    "        \n",
    "        # count number of outliers\n",
    "        outliers = dataset_tmp[column][(dataset_tmp[column] < lower_bound) | (dataset_tmp[column] > upper_bound)]\n",
    "        print(f'Column {column} has {outliers.shape[0]} outliers')\n",
    "        \n",
    "        # delete outliers\n",
    "        dataset_tmp.drop(outliers.index, inplace=True)\n",
    "        \n",
    "    return dataset_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_without_outliers = [drop_outliers(dataset) for dataset in datasets_with_new_features]\n",
    "datasets_without_outliers_with_fuel_leak = [drop_outliers(dataset) for dataset in datasets_with_new_features_with_fuel_leak]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_without_outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all datasets into one\n",
    "merged_dataset = pd.concat(datasets_without_outliers)\n",
    "merged_dataset_with_fuel_leak = pd.concat(datasets_without_outliers_with_fuel_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column FLIGHT_PHASE_COUNT since it is unnecessary\n",
    "merged_dataset.drop(['FLIGHT_PHASE_COUNT'], axis=1, inplace=True)\n",
    "merged_dataset_with_fuel_leak.drop('FLIGHT_PHASE_COUNT', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# heatmap for numeric columns\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(merged_dataset.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not exists, create generated_data folder\n",
    "if not os.path.exists('generated_data'):\n",
    "    os.makedirs('generated_data')\n",
    "\n",
    "# save datasets into generated_data folder\n",
    "merged_dataset.to_csv('generated_data/merged_dataset.csv', index=False)\n",
    "merged_dataset_with_fuel_leak.to_csv('generated_data/merged_dataset_with_fuel_leak.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Derivated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = merged_dataset.select_dtypes(include=['float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset with resampled data diff for numeric  columns\n",
    "merged_dataset_diff = pd.DataFrame()\n",
    "merged_dataset_with_fuel_leak_diff = pd.DataFrame()\n",
    " \n",
    "for column in numeric_columns:\n",
    "    if column == 'ALTITUDE_DIFF' or column == 'VALUE_FOB_DIFF' or column == 'FLIGHT':\n",
    "        merged_dataset_diff[column] = merged_dataset[column]\n",
    "        merged_dataset_with_fuel_leak_diff[column] = merged_dataset_with_fuel_leak[column]\n",
    "    merged_dataset_diff[column] = merged_dataset[column].diff()\n",
    "    merged_dataset_with_fuel_leak_diff[column] = merged_dataset_with_fuel_leak[column].diff()\n",
    "    \n",
    "# fillna with 0\n",
    "merged_dataset_diff.fillna(0, inplace=True)\n",
    "merged_dataset_with_fuel_leak_diff.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets into generated_data folder\n",
    "merged_dataset_diff.to_csv('generated_data/merged_dataset_diff.csv', index=False)\n",
    "merged_dataset_with_fuel_leak_diff.to_csv('generated_data/merged_dataset_with_fuel_leak_diff.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "scaler = StandardScaler()\n",
    " \n",
    "# fit_transform on merged dataset\n",
    "scaled_data = scaler.fit_transform(merged_dataset_diff[numeric_columns])\n",
    "scaled_data_with_fuel_leak = scaler.fit_transform(merged_dataset_with_fuel_leak_diff[numeric_columns])\n",
    "\n",
    "# convert scaled_dataset to dataframe\n",
    "scaled_dataset = pd.DataFrame(scaled_data, columns=numeric_columns)\n",
    "scaled_dataset_with_fuel_leak = pd.DataFrame(scaled_data_with_fuel_leak, columns=numeric_columns)\n",
    "\n",
    "scaled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scaled_data),np.std(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scaled dataset to csv\n",
    "scaled_dataset.to_csv('generated_data/scaled_dataset.csv', index=False)\n",
    "scaled_dataset_with_fuel_leak.to_csv('generated_data/scaled_dataset_with_fuel_leak.csv', index=False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA without Synthetic Leak Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = pd.read_csv('generated_data/scaled_dataset.csv')\n",
    "scaled_data_with_fuel_leak = pd.read_csv('generated_data/scaled_dataset_with_fuel_leak.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 15\n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA()  \n",
    "\n",
    "# Fit PCA on scaled_data\n",
    "pca.fit(scaled_data)\n",
    " \n",
    "# access values and vectors\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    " \n",
    "# transform data \n",
    "pca_data = pca.transform(scaled_data)\n",
    " \n",
    "# convert pca_data to dataframe\n",
    "pca_dataset = pd.DataFrame(pca_data, columns=[f'PC{i}' for i in range(1, len(pca.components_)+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dataset.to_csv('generated_data/pca_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "\n",
    "explained_var, cumulative_var\n",
    "\n",
    "n_components = len(explained_var)\n",
    "\n",
    "variance_trace = go.Bar(\n",
    "    x=list(range(1, n_components + 1)),\n",
    "    y=explained_var,\n",
    "    name='Explained Variance'\n",
    ")\n",
    "\n",
    "cumulative_trace = go.Scatter(\n",
    "    x=list(range(1, n_components + 1)),\n",
    "    y=cumulative_var,\n",
    "    mode='lines',\n",
    "    name='Cumulative Explained Variance'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='PCA - Explained and Cumulative Variance',\n",
    "    xaxis=dict(title='Principal Component'),\n",
    "    yaxis=dict(title='Variance Explained'),\n",
    "    legend=dict(x=0.7, y=1.0),\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[variance_trace, cumulative_trace], layout=layout)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = []\n",
    "\n",
    "for i, pc in enumerate(pca.components_, 1):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=scaled_data.columns, y=pc))\n",
    "    fig.update_layout(title=f\"Principal Component {i}\", xaxis_title=\"Features\", yaxis_title=\"PC Value\", template='plotly_dark')\n",
    "    figures.append(fig)\n",
    "for fig in figures[0:6]:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d plot of first 3 principal components in different colors\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(pca_dataset['PC1'], pca_dataset['PC2'], pca_dataset['PC3'], c=merged_dataset['FLIGHT_PHASE_COUNT'], cmap='coolwarm', alpha=0.5)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 10\n",
    "\n",
    "# Create a PCA instance\n",
    "pca_with_fuel_leak = PCA()  \n",
    "\n",
    "# Fit PCA on scaled_data\n",
    "pca.fit(scaled_data_with_fuel_leak)\n",
    " \n",
    "# access values and vectors\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    " \n",
    "# transform data \n",
    "pca_data = pca.transform(scaled_data)\n",
    " \n",
    "# convert pca_data to dataframe\n",
    "pca_dataset_with_fuel_leak = pd.DataFrame(pca_data, columns=[f'PC{i}' for i in range(1, len(pca.components_)+1)])\n",
    "pca_dataset_with_fuel_leak.to_csv('generated_data/pca_dataset_with_fuel_leak.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "\n",
    "explained_var, cumulative_var\n",
    "\n",
    "n_components = len(explained_var)\n",
    "\n",
    "variance_trace = go.Bar(\n",
    "    x=list(range(1, n_components + 1)),\n",
    "    y=explained_var,\n",
    "    name='Explained Variance'\n",
    ")\n",
    "\n",
    "cumulative_trace = go.Scatter(\n",
    "    x=list(range(1, n_components + 1)),\n",
    "    y=cumulative_var,\n",
    "    mode='lines',\n",
    "    name='Cumulative Explained Variance'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='PCA - Explained and Cumulative Variance',\n",
    "    xaxis=dict(title='Principal Component'),\n",
    "    yaxis=dict(title='Variance Explained'),\n",
    "    legend=dict(x=0.7, y=1.0),\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[variance_trace, cumulative_trace], layout=layout)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = []\n",
    "\n",
    "for i, pc in enumerate(pca.components_, 1):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=scaled_data.columns, y=pc))\n",
    "    fig.update_layout(title=f\"Principal Component {i}\", xaxis_title=\"Features\", yaxis_title=\"PC Value\", template='plotly_dark')\n",
    "    figures.append(fig)\n",
    "for fig in figures[0:6]:\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_test plot TOTAL_FUEL_USED for each flight\n",
    "for flight in df_test['FLIGHT'].unique():\n",
    "    # count number of rows for each flight\n",
    "    if df_test[df_test['FLIGHT'] == flight].shape[0] > 1:\n",
    "        # plot TOTAL_FUEL_USED for each flight x = UTC_TIME, y = TOTAL_FUEL_USED only plot if FLIGHT_PHASE_COUNT == 8\n",
    "        if df_test[df_test['FLIGHT'] == flight]['FLIGHT_PHASE_COUNT'].iloc[0] == 2:\n",
    "            plt.plot(df_test[df_test['FLIGHT'] == flight]['UTC_TIME'], df_test[df_test['FLIGHT'] == flight]['VALUE_FOB_DIFF'])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each flight and each flightphase calculate the std of VALUE_FOB_DIFF \n",
    "\n",
    "results = []\n",
    "for flight in df_test['FLIGHT'].unique():\n",
    "    result = {}\n",
    "    for flightphase in df_test[df_test['FLIGHT'] == flight]['FLIGHT_PHASE_COUNT'].unique():\n",
    "        result['FLIGHT'] = flight\n",
    "        result['FLIGHT_PHASE'] = flightphase\n",
    "        result['VALUE_FOB_DIFF_std'] = df_test[(df_test['FLIGHT'] == flight) & (df_test['FLIGHT_PHASE_COUNT'] == flightphase)]['VALUE_FOB_DIFF'].std()\n",
    "        results.append(result)\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "pd.pivot_table(results_df, values='VALUE_FOB_DIFF_std', index=['FLIGHT'], columns=['FLIGHT_PHASE'], aggfunc=np.sum,  fill_value=0).sort_values(by=[11], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for dataset 0 for column LEAK_DETECTION_LEAK_FLOW\n",
    "datasets[0].plot.scatter(x='UTC_TIME', y='LEAK_DETECTION_LEAK_FLOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Extract numeric part from 'Flight' column\n",
    "datasets[0]['Flight_number'] = datasets[0]['Flight'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Sort DataFrame by 'Flight_number'\n",
    "sorted_df = datasets[0].sort_values(by='Flight_number')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# boxplot for sorted DataFrame for column VALUE_FOB.\n",
    "fig.add_trace(go.Box(\n",
    "    x= sorted_df['Flight'], \n",
    "    y= sorted_df['VALUE_FOB'], \n",
    "    name= 'VALUE_FOB',\n",
    "    line=dict(color='green'),  # change boxplot color to green\n",
    "    marker=dict(color='green') # change marker color to green\n",
    "))\n",
    "\n",
    "# Calculate mean for each flight in the sorted DataFrame\n",
    "mean_fob = sorted_df.groupby('Flight')['VALUE_FOB'].mean()\n",
    "\n",
    "# Add a scatter plot for the mean values\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=mean_fob.index, \n",
    "    y=mean_fob.values, \n",
    "    mode='lines', \n",
    "    name='Mean',\n",
    "    line=dict(color='purple')  # set line color to purple\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Boxplot for sorted dataset for column VALUE_FOB\", \n",
    "    xaxis_title=\"Flight\", \n",
    "    yaxis_title=\"VALUE_FOB\",\n",
    "    plot_bgcolor='black',  # set plot background color to black\n",
    "    paper_bgcolor='black',  # set paper background color to black\n",
    "    font=dict(color='white'),  # set font color to white\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cat_features02 = [col for col in datasets[0] if datasets[0][col].dtypes.name == 'bool']\n",
    "features02 = [col for col in datasets[0] if col not in cat_features02 if 'STATUS' not in col if 'MODE' not in col if col not in [\"day\", \"month\", \"time\", \"year\", \"SELECTED_GADIR_ALTITUDE_VALUE\", \"VALUE_FUEL_VOL_LST\", \"VALUE_FUEL_VOL_RST\", \"FLIGHT_PHASE_COUNT\", \"APU_FUEL_FLOW_REQUEST_SIGNAL_1\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,25)) \n",
    "sns.heatmap(msn02_smooth[features02].corr(), cmap=\"RdBu\", annot=True, ax=ax)\n",
    "plt.xticks(rotation=45, ha='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_datasets(dataset):\n",
    "    # List of columns to plot for VALUE_FUEL_QTY_CT;VALUE_FUEL_QTY_FT1;VALUE_FUEL_QTY_FT2;VALUE_FUEL_QTY_FT3;VALUE_FUEL_QTY_FT4;VALUE_FUEL_QTY_LXT;VALUE_FUEL_QTY_RXT\n",
    "    fuel_qty_cols = ['VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3', 'VALUE_FUEL_QTY_FT4']\n",
    "\n",
    "    # Loop over every unique date\n",
    "    for date in dataset['DATE'].unique():\n",
    "        for flight in dataset['MSN'].unique():\n",
    "            # Create a subplot\n",
    "            fig = sp.make_subplots(rows=3, cols=3)\n",
    "            \n",
    "            # size of fig \n",
    "            fig.update_layout(height=1400, width=1400)\n",
    "\n",
    "            # Loop over each column\n",
    "            for i, col in enumerate(fuel_qty_cols):\n",
    "                # save the dataset for the current date and flight\n",
    "                dataset_tmp = dataset[(dataset['DATE'] == date) & (dataset['MSN'] == flight)]\n",
    "                # Add scatter plot to subplot\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=dataset_tmp['UTC_TIME'], \n",
    "                            y=dataset_tmp[col], \n",
    "                            mode='markers',\n",
    "                            name=col),\n",
    "                    row=i//3 + 1, \n",
    "                    col=i%3 + 1\n",
    "                )\n",
    "\n",
    "                # Update xaxis and yaxis titles\n",
    "                fig.update_xaxes(title_text='UTC_TIME', row=i//3 + 1, col=i%3 + 1)\n",
    "                fig.update_yaxes(title_text=col + \" \" + str(date), row=i//3 + 1, col=i%3 + 1)\n",
    "\n",
    "            # Show the plot\n",
    "            #fig.show()\n",
    "            \n",
    "            # save the plot as png\n",
    "            fig.write_image(\"plots2/\" + str(date) +\"-\" + str(flight) + \".png\")\n",
    "     \n",
    "            \n",
    "#plot_datasets(datasets[1])\n",
    "\n",
    "#for i in range(len(datasets)):\n",
    " #   if i > 1:\n",
    "  #      plot_datasets(datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_for_date(dataset, date, flight_phase):\n",
    "    # List of columns to plot for VALUE_FUEL_QTY_CT;VALUE_FUEL_QTY_FT1;VALUE_FUEL_QTY_FT2;VALUE_FUEL_QTY_FT3;VALUE_FUEL_QTY_FT4;VALUE_FUEL_QTY_LXT;VALUE_FUEL_QTY_RXT\n",
    "    fuel_qty_cols = ['VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3', 'VALUE_FUEL_QTY_FT4']\n",
    "\n",
    "    # Loop over every unique date\n",
    "    for flight in dataset['MSN'].unique():\n",
    "        # Create a subplot\n",
    "        fig = sp.make_subplots(rows=3, cols=3)\n",
    "\n",
    "        # size of fig \n",
    "        fig.update_layout(height=1000, width=1200)\n",
    "\n",
    "        # Loop over each column\n",
    "        for i, col in enumerate(fuel_qty_cols):\n",
    "            # save the dataset for the current date and flight\n",
    "            dataset_tmp = dataset[(dataset['DATE'] == date) & (dataset['MSN'] == flight) & (dataset['FLIGHT_PHASE_COUNT'] == flight_phase)]\n",
    "            # Add scatter plot to subplot\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=dataset_tmp['UTC_TIME'], \n",
    "                        y=dataset_tmp[col], \n",
    "                        mode='markers',\n",
    "                        name=col),\n",
    "                row=i//3 + 1, \n",
    "                col=i%3 + 1\n",
    "            )\n",
    "\n",
    "            # Update xaxis and yaxis titles\n",
    "            fig.update_xaxes(title_text='UTC_TIME', row=i//3 + 1, col=i%3 + 1)\n",
    "            fig.update_yaxes(title_text=col + \" \" + str(date), row=i//3 + 1, col=i%3 + 1)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset_for_date(datasets[1], datetime.date(2015, 3, 7), 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[6][['DATE', 'MSN', 'FLIGHT_PHASE_COUNT']][datasets[6]['FLIGHT_PHASE_COUNT'] == 8.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add leakage data to the dataset\n",
    "def add_leakage_data(dataset, leakage_qty):\n",
    "    # substract leakage_qty from VALUE_FOB col\n",
    "    dataset['VALUE_FOB'] = dataset['VALUE_FOB'] - (leakage_qty / 60)\n",
    "    \n",
    "    # add column FUEL_USED_BY_FOB to dataset\n",
    "    # first column is 0 because we don't know the fuel used at the start of the flight\n",
    "    # every next row is the row + difference between the current and previous row in VALUE_FOB\n",
    "    dataset['FUEL_USED_BY_FOB'] = 0\n",
    "    for i in range(1, dataset.shape[0]):\n",
    "        dataset['FUEL_USED_BY_FOB'].iloc[i] = dataset['FUEL_USED_BY_FOB'].iloc[i-1] + (dataset['VALUE_FOB'].iloc[i] - dataset['VALUE_FOB'].iloc[i-1])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_leakage_data(datasets[8], 0.5)\n",
    "add_leakage_data(datasets[9], 1.0)\n",
    "add_leakage_data(datasets[10], 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_fuel_used(dataset):\n",
    "    nulls = dataset[dataset['FLIGHT_PHASE_COUNT'] == 8.0][\"FUEL_USED_1\"].isnull().sum()\n",
    "    shape_0 = dataset[dataset['FLIGHT_PHASE_COUNT'] == 8.0].shape[0]\n",
    "    \n",
    "    print(f\"dataset_{1}\", nulls / shape_0)\n",
    "    \n",
    "for dataset in datasets:\n",
    "    relative_fuel_used(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for dataset 6 for 2016-07-27, flightphase 8\n",
    "# plot columns VALUE_FOB and FUEL_USED_BY_FOB\n",
    "dataste_tmp = datasets[0][(datasets[0]['FLIGHT_PHASE_COUNT'] == 8.0) & (datasets[0]['DATE'] == datetime.date(2011, 3, 9))]\n",
    "\n",
    "dataste_tmp['VALUE_FOB_LEAKAGE'] = dataste_tmp['VALUE_FOB'] - (5.0)\n",
    "\n",
    "dataste_tmp[\"TOTAL_CONSUMPTION\"] = dataste_tmp[\"FUEL_USED_1\"] + dataste_tmp[\"FUEL_USED_2\"] + dataste_tmp[\"FUEL_USED_3\"] + dataste_tmp[\"FUEL_USED_4\"]\n",
    "dataste_tmp[\"TOTAL_CONSUMPTION\"] = dataste_tmp[\"TOTAL_CONSUMPTION\"] - dataste_tmp[\"TOTAL_CONSUMPTION\"].iloc[0]\n",
    "\n",
    "# invert the values of VALUE_FOB_LEAKAGE so that the highest value is 0\n",
    "dataste_tmp['VALUE_FOB_LEAKAGE'] = dataste_tmp['VALUE_FOB_LEAKAGE'].max() - dataste_tmp['VALUE_FOB_LEAKAGE'] + 5.0\n",
    "\n",
    "dataste_tmp['VALUE_FOB'] = dataste_tmp['VALUE_FOB'].max() - dataste_tmp['VALUE_FOB']\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dataste_tmp['UTC_TIME'],\n",
    "            y=dataste_tmp['TOTAL_CONSUMPTION'],\n",
    "            name='TOTAL_CONSUMPTION')\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dataste_tmp['UTC_TIME'],\n",
    "                y=dataste_tmp['VALUE_FOB_LEAKAGE'],\n",
    "                name='VALUE_FOB_LEAKAGE')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dataste_tmp['UTC_TIME'],\n",
    "                y=dataste_tmp['VALUE_FOB'],\n",
    "                name='VALUE_FOB')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dataste_tmp['UTC_TIME'],\n",
    "                y=dataste_tmp['FW_GEO_ALTITUDE'],\n",
    "                name='FW_GEO_ALTITUDE')\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce new columns for each dataset [CT_DIFF, LXT_DIFF, RXT_DIFF, FT1_DIFF, FT2_DIFF, FT3_DIFF, FT4_DIFF]\n",
    "# value represents the difference between the current and the previous value\n",
    "for dataset in datasets:\n",
    "    dataset['CT_DIFF'] = dataset['VALUE_FUEL_QTY_CT'].diff()\n",
    "    dataset['LXT_DIFF'] = dataset['VALUE_FUEL_QTY_LXT'].diff()\n",
    "    dataset['RXT_DIFF'] = dataset['VALUE_FUEL_QTY_RXT'].diff()\n",
    "    dataset['FT1_DIFF'] = dataset['VALUE_FUEL_QTY_FT1'].diff()\n",
    "    dataset['FT2_DIFF'] = dataset['VALUE_FUEL_QTY_FT2'].diff()\n",
    "    dataset['FT3_DIFF'] = dataset['VALUE_FUEL_QTY_FT3'].diff()\n",
    "    dataset['FT4_DIFF'] = dataset['VALUE_FUEL_QTY_FT4'].diff()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff(): \n",
    "    #plot diff columns for dataset 0 for each unique date\n",
    "    for date in datasets[0]['DATE'].unique():\n",
    "        for flight in datasets[0]['MSN'].unique():\n",
    "            # create subplot 3 columns and len(datasets[0].unique) rows\n",
    "            fig = sp.make_subplots(rows=3, cols=3)\n",
    "            \n",
    "            # size of fig\n",
    "            fig.update_layout(height=1000, width=1000)\n",
    "            \n",
    "            diff_cols = ['CT_DIFF', 'LXT_DIFF', 'RXT_DIFF', 'FT1_DIFF', 'FT2_DIFF', 'FT3_DIFF', 'FT4_DIFF']\n",
    "            \n",
    "            # loop over the diff columns\n",
    "            for i, col in enumerate(diff_cols):\n",
    "                # save the dataset for the current date and flight\n",
    "                dataset_tmp = datasets[0][(datasets[0]['DATE'] == date) & (datasets[0]['MSN'] == flight)]\n",
    "                # add scatter plot to subplot\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=dataset_tmp['UTC_TIME'], \n",
    "                            y=dataset_tmp[col], \n",
    "                            mode='markers',\n",
    "                            name=col),\n",
    "                    row=i//3 + 1, \n",
    "                    col=i%3 + 1\n",
    "                )\n",
    "\n",
    "                # update xaxis and yaxis titles\n",
    "                fig.update_xaxes(title_text='UTC_TIME', row=i//3 + 1, col=i%3 + 1)\n",
    "                fig.update_yaxes(title_text=col + \" \" + str(date), row=i//3 + 1, col=i%3 + 1)\n",
    "                \n",
    "            # save fig as png \n",
    "            fig.write_image(\"plots2/diff_plots/\" + str(date) + \"-\" + str(flight) + \"_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas\n",
    "# 1. use lower bound and upper bound to detect outliers\n",
    "# 2. use the bounds to plot the data and see if there are any outliers\n",
    "# 3. deep learning auto detection anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate synthetic data for each dataset including a simulated fuel leak\n",
    "# using 0,5l, 1L and 5L as the leak size per minute\n",
    "\n",
    "def generate_synthetic_data(dataset, leak_size):\n",
    "    # create a copy of the dataset\n",
    "    synthetic_dataset = dataset.copy()\n",
    "    \n",
    "    synthetic_dataset['VALUE_FUEL_QTY_LXT'] = synthetic_dataset['VALUE_FUEL_QTY_LXT'] - (leak_size)\n",
    "    \n",
    "    return synthetic_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the fuel on board and the Fuel Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fuel_used(dataset):\n",
    "    fuel_used_cols = ['FUEL_USED_1', 'FUEL_USED_2', 'FUEL_USED_3', 'FUEL_USED_4']\n",
    "\n",
    "    # save the sum of the fuel used columns in a new column\n",
    "    dataset['FUEL_USED_SUM'] = dataset[fuel_used_cols].sum(axis=1)\n",
    "\n",
    "    fuel_cols = ['VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3', 'VALUE_FUEL_QTY_FT4']\n",
    "    \n",
    "    dataset['FUEL_COLS_SUM'] = dataset[fuel_cols].sum(axis=1)\n",
    "\n",
    "    #datasets[0]['FUEL_DIFF_SUM'] = datasets[0]['FUEL_DIFF_SUM'].cumsum()\n",
    "\n",
    "    # substract from each value in FUEL_COLS_SUM the first value bigger than 1 and get the absoulte value. Do this for each day\n",
    "    dataset['FUEL_COLS_SUM'] = dataset.groupby(['DATE'])['FUEL_COLS_SUM'].transform(lambda x: x - x[x > 1].iloc[0]).abs()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "datasets[0][datasets[0]['DATE'] == datetime.date(2011, 3, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_fuel_cons_vs_fuel_qty(dataset):\n",
    "    # plot the fuel_used columns for dataset 0 for 2010-10-26\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    dates_to_find = dataset['DATE'].unique()[:1]\n",
    "\n",
    "    for date in dates_to_find:\n",
    "        dataset = dataset[dataset['FLIGHT_PHASE_COUNT'] == 8]\n",
    "        # plot FUEL_USED_SUM and FUEL_DIFF_SUM\n",
    "        fig.add_trace(go.Scatter(x=dataset[dataset['DATE'] == date]['UTC_TIME'],\n",
    "                                    y=dataset[dataset['DATE'] == date]['FUEL_USED_SUM'],\n",
    "                                    name='FUEL_USED_SUM'))\n",
    "        fig.add_trace(go.Scatter(x=dataset[dataset['DATE'] == date]['UTC_TIME'],\n",
    "                                    y=dataset[dataset['DATE'] == date]['FUEL_COLS_SUM'],\n",
    "                                    name='FUEL_COLS_SUM'))\n",
    "        # add altitude to the plot\n",
    "        fig.add_trace(go.Scatter(x=dataset[dataset['DATE'] == date]['UTC_TIME'],\n",
    "                                    y=dataset[dataset['DATE'] == date]['FW_GEO_ALTITUDE'],\n",
    "                                    name='FW_GEO_ALTITUDE'))\n",
    "        \n",
    "        fuel_tanks = ['CT', 'LXT', 'RXT', 'FT1', 'FT2', 'FT3', 'FT4']\n",
    "        # plot the fuel tanks\n",
    "        for tank in fuel_tanks:\n",
    "            fig.add_trace(go.Scatter(x=dataset[dataset['DATE'] == date]['UTC_TIME'],\n",
    "                                        y=dataset[dataset['DATE'] == date]['VALUE_FUEL_QTY_' + tank],\n",
    "                                        name='VALUE_FUEL_QTY_' + tank))        \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_fuel_leak = generate_synthetic_data(datasets[0], 1800)\n",
    "dataset_with_fuel_leak = calculate_fuel_used(dataset_with_fuel_leak)\n",
    "datasets[0] = calculate_fuel_used(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0]['FUEL_COLS_SUM'][datasets[0]['FUEL_COLS_SUM'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_fuel_leak['FUEL_COLS_SUM'][dataset_with_fuel_leak['FUEL_COLS_SUM'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fuel_cons_vs_fuel_qty(dataset_with_fuel_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fuel_cons_vs_fuel_qty(datasets[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pca \n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pcaInstance = PCA(n_components=2)\n",
    "\n",
    "dataset_tmp = datasets[0][(datasets[0]['FLIGHT_PHASE_COUNT'] == 8)]\n",
    "\n",
    "feat_cols = (item for item in dataset_tmp.columns if item.startswith('VALUE_FUEL_QTY'))\n",
    "\n",
    "dataset_tmp = dataset_tmp[feat_cols].dropna()\n",
    "\n",
    "print(dataset_tmp.shape)\n",
    "\n",
    "# fit the pca instance to the dataset\n",
    "pcaInstance.fit(dataset_tmp)\n",
    "\n",
    "# transform the dataset\n",
    "pcaInstance.transform(dataset_tmp)\n",
    "\n",
    "# get the explained variance ratio\n",
    "pcaInstance.explained_variance_ratio_\n",
    "\n",
    "# plot the explained variance ratio \n",
    "plt.plot(pcaInstance.explained_variance_ratio_)\n",
    "plt.show()\n",
    "\n",
    "# plot the pca components\n",
    "plt.scatter(pcaInstance.components_[0], pcaInstance.components_[1])\n",
    "plt.show()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Select numerical columns only\n",
    "numerical_cols = datasets[0].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Drop missing values\n",
    "datasets_num = datasets[0][numerical_cols].dropna()\n",
    "\n",
    "# Standardize the feature matrix\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to your data\n",
    "pipeline.fit(datasets_num)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d plot heatmap of the pca components \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    " \n",
    "x = pca.components_[0]\n",
    "y = pca.components_[1]\n",
    "z = pca.components_[2]\n",
    " \n",
    "ax.scatter(x,y,z, marker=\"s\", c=\"g\", s=40, label='first')\n",
    " \n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, pca.components_.shape[0]):\n",
    "    for j in range(0, pca.components_.shape[0]):\n",
    "        plt.scatter(pca.components_[i], pca.components_[j])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "plt.plot(cumulative_explained_variance)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Variance Explained by Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feat_cols = (item for item in datasets[0].columns if item.startswith('VALUE_FUEL_QTY'))\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(datasets[0][feat_cols].dropna())\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_pca, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select numerical columns only\n",
    "numerical_cols = datasets[0].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Drop missing values\n",
    "datasets_num = datasets[0][numerical_cols].dropna()\n",
    "\n",
    "# Standardize the feature matrix\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to your data\n",
    "pipeline.fit(datasets_num)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "plt.plot(cumulative_explained_variance)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Variance Explained by Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# show number of components that explain 95% of the variance\n",
    "print(np.where(cumulative_explained_variance > 0.95)[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# let's assume we take first 2 components as they explain the most variance\n",
    "pca = PCA(n_components=16)\n",
    "principalComponents = pca.fit_transform(datasets_num)\n",
    "\n",
    "# convert to dataframe\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8', 'pca_9', 'pca_10', 'pca_11', 'pca_12', 'pca_13', 'pca_14', 'pca_15', 'pca_16'])\n",
    "\n",
    "# Kmeans clustering\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(principalDf)\n",
    "\n",
    "# visualizing the clusters in 3d\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create trace\n",
    "trace = go.Scatter3d(\n",
    "    x=pca_transformed[:, 0],\n",
    "    y=pca_transformed[:, 1],\n",
    "    z=pca_transformed[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=clusters, # set color to an array/list of desired values\n",
    "        colorscale='Viridis', # choose a colorscale\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "layout = go.Layout(\n",
    "    title = '3D Scatter Plot of PCA Components with KMeans Clusters',\n",
    "    scene = dict(\n",
    "            xaxis_title='Component 1',\n",
    "            yaxis_title='Component 2',\n",
    "            zaxis_title='Component 3'),\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the original data\n",
    "pca_transformed = pipeline.transform(datasets_num)\n",
    "\n",
    "# Now you can fit KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "clusters = kmeans.fit_predict(pca_transformed)\n",
    "\n",
    "# Append the clusters to the original scaled dataset\n",
    "datasets_num_clustered = datasets_num.copy()\n",
    "datasets_num_clustered['Cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the clusters\n",
    "cluster_analysis = datasets_num_clustered.groupby('Cluster').mean()\n",
    "print(cluster_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, random_state=0)\n",
    "    km.fit(pca_transformed)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse)\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5)\n",
    "clusters = dbscan.fit_predict(pca_transformed)\n",
    " \n",
    "plt.scatter(pca_transformed[:, 0], pca_transformed[:, 1], c=clusters, cmap=\"plasma\")\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(pca.components_, columns=datasets_num.columns)\n",
    "\n",
    "# Plot the feature importances\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.bar(range(feature_importance.shape[1]), feature_importance.iloc[0])\n",
    "    \n",
    "plt.xticks(range(feature_importance.shape[1]), feature_importance.columns, rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "print(feature_importance)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "score = silhouette_score(pca_transformed, clusters)\n",
    "print('Silhouette Score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs with mean of the column\n",
    "datasets[0].fillna(datasets[0].mean(), inplace=True)\n",
    "\n",
    "datasets[0].drop_duplicates(inplace=True)\n",
    "\n",
    "from scipy import stats\n",
    "z_scores = stats.zscore(datasets[0][feat_cols])\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "new_df = datasets[0][filtered_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = new_df.corr()\n",
    "sns.heatmap(corr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(new_df)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8kaAjauv_l00"
      },
      "source": [
        "<img\n",
        "    src=\"https://upload.wikimedia.org/wikipedia/de/f/fe/Airbus_Logo.svg\" width=\"600\" height=\"490\" style=\"display: block;\n",
        "  margin-left: auto;\n",
        "  margin-right: auto\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA9F0BGqOfQB",
        "outputId": "dd610943-307b-4af3-d1f7-a21cf9b48fb3"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n7aqOI9K_mWu"
      },
      "source": [
        "## 1) Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bR6PNoASYyGD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "from matplotlib import pyplot as plt, dates\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jcsGpRri_q37"
      },
      "source": [
        "## 2) Loading data\n",
        "\n",
        "The dataset contains simulated leaks of 5kg/s, 1kg/s and 0.5kg/s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "3YEtauaPY5S_",
        "outputId": "a6ce30b1-2676-442b-8507-3893447daf28"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FUEL_USED_1</th>\n",
              "      <th>FUEL_USED_2</th>\n",
              "      <th>FUEL_USED_3</th>\n",
              "      <th>FUEL_USED_4</th>\n",
              "      <th>FW_GEO_ALTITUDE</th>\n",
              "      <th>VALUE_FOB</th>\n",
              "      <th>VALUE_FUEL_QTY_CT</th>\n",
              "      <th>VALUE_FUEL_QTY_FT1</th>\n",
              "      <th>VALUE_FUEL_QTY_FT2</th>\n",
              "      <th>VALUE_FUEL_QTY_FT3</th>\n",
              "      <th>...</th>\n",
              "      <th>FLIGHT</th>\n",
              "      <th>TOTAL_FUEL_USED</th>\n",
              "      <th>VALUE_FOB_DIFF</th>\n",
              "      <th>TOTAL_FOB_BY_QTY</th>\n",
              "      <th>DELTA_VFOB_VS_VFOBQTY</th>\n",
              "      <th>ALTITUDE_DIFF</th>\n",
              "      <th>VALUE_FOB_MISSING</th>\n",
              "      <th>VALUE_FOB_MISSING_BY_QTY</th>\n",
              "      <th>VALUE_FOB_BY_FUEL_USED</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>155.744975</td>\n",
              "      <td>154.661250</td>\n",
              "      <td>152.905900</td>\n",
              "      <td>150.077375</td>\n",
              "      <td>3008.56850</td>\n",
              "      <td>22643.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1842.00</td>\n",
              "      <td>2556.50</td>\n",
              "      <td>2597.25</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>613.389500</td>\n",
              "      <td>-5.50</td>\n",
              "      <td>22646.0</td>\n",
              "      <td>-2.75</td>\n",
              "      <td>20.56675</td>\n",
              "      <td>69.75</td>\n",
              "      <td>61.0</td>\n",
              "      <td>22099.610500</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>158.475560</td>\n",
              "      <td>157.385020</td>\n",
              "      <td>155.620860</td>\n",
              "      <td>152.839420</td>\n",
              "      <td>3157.07820</td>\n",
              "      <td>22628.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1839.20</td>\n",
              "      <td>2570.00</td>\n",
              "      <td>2616.20</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>624.320860</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>22631.0</td>\n",
              "      <td>-2.80</td>\n",
              "      <td>27.94520</td>\n",
              "      <td>84.80</td>\n",
              "      <td>76.0</td>\n",
              "      <td>22088.679140</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>160.995400</td>\n",
              "      <td>159.870920</td>\n",
              "      <td>158.076460</td>\n",
              "      <td>155.339320</td>\n",
              "      <td>3283.94220</td>\n",
              "      <td>22628.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1838.20</td>\n",
              "      <td>2585.80</td>\n",
              "      <td>2631.40</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>634.282100</td>\n",
              "      <td>0.80</td>\n",
              "      <td>22631.4</td>\n",
              "      <td>-3.20</td>\n",
              "      <td>23.38660</td>\n",
              "      <td>84.80</td>\n",
              "      <td>75.6</td>\n",
              "      <td>22078.717900</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>163.354250</td>\n",
              "      <td>162.208350</td>\n",
              "      <td>160.412150</td>\n",
              "      <td>157.717475</td>\n",
              "      <td>3391.20725</td>\n",
              "      <td>22631.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1837.75</td>\n",
              "      <td>2602.25</td>\n",
              "      <td>2644.25</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>643.692225</td>\n",
              "      <td>1.75</td>\n",
              "      <td>22634.5</td>\n",
              "      <td>-2.75</td>\n",
              "      <td>22.83350</td>\n",
              "      <td>81.25</td>\n",
              "      <td>72.5</td>\n",
              "      <td>22069.307775</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>165.982500</td>\n",
              "      <td>164.822925</td>\n",
              "      <td>162.983350</td>\n",
              "      <td>160.341150</td>\n",
              "      <td>3497.44875</td>\n",
              "      <td>22633.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1837.00</td>\n",
              "      <td>2619.75</td>\n",
              "      <td>2657.75</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>654.129925</td>\n",
              "      <td>0.75</td>\n",
              "      <td>22637.0</td>\n",
              "      <td>-3.75</td>\n",
              "      <td>18.66800</td>\n",
              "      <td>79.75</td>\n",
              "      <td>70.0</td>\n",
              "      <td>22058.870075</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51915</th>\n",
              "      <td>3812.429932</td>\n",
              "      <td>3814.106836</td>\n",
              "      <td>3858.093262</td>\n",
              "      <td>3869.840283</td>\n",
              "      <td>31982.60000</td>\n",
              "      <td>24561.80</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1847.20</td>\n",
              "      <td>2597.00</td>\n",
              "      <td>2652.00</td>\n",
              "      <td>...</td>\n",
              "      <td>6939.0</td>\n",
              "      <td>15354.470312</td>\n",
              "      <td>-7.20</td>\n",
              "      <td>24565.0</td>\n",
              "      <td>-3.20</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>848.20</td>\n",
              "      <td>846.0</td>\n",
              "      <td>10055.529688</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51916</th>\n",
              "      <td>3813.984326</td>\n",
              "      <td>3815.687402</td>\n",
              "      <td>3859.675195</td>\n",
              "      <td>3871.436426</td>\n",
              "      <td>31979.80000</td>\n",
              "      <td>24530.00</td>\n",
              "      <td>48.0</td>\n",
              "      <td>1845.80</td>\n",
              "      <td>2594.80</td>\n",
              "      <td>2650.40</td>\n",
              "      <td>...</td>\n",
              "      <td>6939.0</td>\n",
              "      <td>15360.783350</td>\n",
              "      <td>-6.00</td>\n",
              "      <td>24532.0</td>\n",
              "      <td>-2.00</td>\n",
              "      <td>0.80000</td>\n",
              "      <td>880.00</td>\n",
              "      <td>879.0</td>\n",
              "      <td>10049.216650</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51917</th>\n",
              "      <td>3815.558350</td>\n",
              "      <td>3817.233594</td>\n",
              "      <td>3861.262549</td>\n",
              "      <td>3873.028906</td>\n",
              "      <td>31978.60000</td>\n",
              "      <td>24502.00</td>\n",
              "      <td>49.2</td>\n",
              "      <td>1843.80</td>\n",
              "      <td>2593.00</td>\n",
              "      <td>2650.60</td>\n",
              "      <td>...</td>\n",
              "      <td>6939.0</td>\n",
              "      <td>15367.083398</td>\n",
              "      <td>-5.00</td>\n",
              "      <td>24504.4</td>\n",
              "      <td>-2.40</td>\n",
              "      <td>1.80000</td>\n",
              "      <td>908.00</td>\n",
              "      <td>906.6</td>\n",
              "      <td>10042.916602</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51918</th>\n",
              "      <td>3817.105664</td>\n",
              "      <td>3818.788037</td>\n",
              "      <td>3862.828857</td>\n",
              "      <td>3874.617285</td>\n",
              "      <td>31978.40000</td>\n",
              "      <td>24472.60</td>\n",
              "      <td>46.2</td>\n",
              "      <td>1840.40</td>\n",
              "      <td>2589.40</td>\n",
              "      <td>2651.60</td>\n",
              "      <td>...</td>\n",
              "      <td>6939.0</td>\n",
              "      <td>15373.339844</td>\n",
              "      <td>-7.40</td>\n",
              "      <td>24474.6</td>\n",
              "      <td>-2.00</td>\n",
              "      <td>2.60000</td>\n",
              "      <td>937.40</td>\n",
              "      <td>936.4</td>\n",
              "      <td>10036.660156</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51919</th>\n",
              "      <td>3818.576807</td>\n",
              "      <td>3820.265430</td>\n",
              "      <td>3864.310303</td>\n",
              "      <td>3876.118066</td>\n",
              "      <td>31981.60000</td>\n",
              "      <td>24439.60</td>\n",
              "      <td>42.2</td>\n",
              "      <td>1839.00</td>\n",
              "      <td>2588.00</td>\n",
              "      <td>2649.80</td>\n",
              "      <td>...</td>\n",
              "      <td>6939.0</td>\n",
              "      <td>15379.270605</td>\n",
              "      <td>-6.00</td>\n",
              "      <td>24441.8</td>\n",
              "      <td>-2.20</td>\n",
              "      <td>1.60000</td>\n",
              "      <td>970.40</td>\n",
              "      <td>969.2</td>\n",
              "      <td>10030.729395</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51920 rows Ã— 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       FUEL_USED_1  FUEL_USED_2  FUEL_USED_3  FUEL_USED_4  FW_GEO_ALTITUDE  \\\n",
              "0       155.744975   154.661250   152.905900   150.077375       3008.56850   \n",
              "1       158.475560   157.385020   155.620860   152.839420       3157.07820   \n",
              "2       160.995400   159.870920   158.076460   155.339320       3283.94220   \n",
              "3       163.354250   162.208350   160.412150   157.717475       3391.20725   \n",
              "4       165.982500   164.822925   162.983350   160.341150       3497.44875   \n",
              "...            ...          ...          ...          ...              ...   \n",
              "51915  3812.429932  3814.106836  3858.093262  3869.840283      31982.60000   \n",
              "51916  3813.984326  3815.687402  3859.675195  3871.436426      31979.80000   \n",
              "51917  3815.558350  3817.233594  3861.262549  3873.028906      31978.60000   \n",
              "51918  3817.105664  3818.788037  3862.828857  3874.617285      31978.40000   \n",
              "51919  3818.576807  3820.265430  3864.310303  3876.118066      31981.60000   \n",
              "\n",
              "       VALUE_FOB  VALUE_FUEL_QTY_CT  VALUE_FUEL_QTY_FT1  VALUE_FUEL_QTY_FT2  \\\n",
              "0       22643.25                0.0             1842.00             2556.50   \n",
              "1       22628.20                0.0             1839.20             2570.00   \n",
              "2       22628.20                0.0             1838.20             2585.80   \n",
              "3       22631.75                0.0             1837.75             2602.25   \n",
              "4       22633.25                0.0             1837.00             2619.75   \n",
              "...          ...                ...                 ...                 ...   \n",
              "51915   24561.80               47.0             1847.20             2597.00   \n",
              "51916   24530.00               48.0             1845.80             2594.80   \n",
              "51917   24502.00               49.2             1843.80             2593.00   \n",
              "51918   24472.60               46.2             1840.40             2589.40   \n",
              "51919   24439.60               42.2             1839.00             2588.00   \n",
              "\n",
              "       VALUE_FUEL_QTY_FT3  ...  FLIGHT  TOTAL_FUEL_USED  VALUE_FOB_DIFF  \\\n",
              "0                 2597.25  ...     2.0       613.389500           -5.50   \n",
              "1                 2616.20  ...     2.0       624.320860           -0.60   \n",
              "2                 2631.40  ...     2.0       634.282100            0.80   \n",
              "3                 2644.25  ...     2.0       643.692225            1.75   \n",
              "4                 2657.75  ...     2.0       654.129925            0.75   \n",
              "...                   ...  ...     ...              ...             ...   \n",
              "51915             2652.00  ...  6939.0     15354.470312           -7.20   \n",
              "51916             2650.40  ...  6939.0     15360.783350           -6.00   \n",
              "51917             2650.60  ...  6939.0     15367.083398           -5.00   \n",
              "51918             2651.60  ...  6939.0     15373.339844           -7.40   \n",
              "51919             2649.80  ...  6939.0     15379.270605           -6.00   \n",
              "\n",
              "       TOTAL_FOB_BY_QTY  DELTA_VFOB_VS_VFOBQTY  ALTITUDE_DIFF  \\\n",
              "0               22646.0                  -2.75       20.56675   \n",
              "1               22631.0                  -2.80       27.94520   \n",
              "2               22631.4                  -3.20       23.38660   \n",
              "3               22634.5                  -2.75       22.83350   \n",
              "4               22637.0                  -3.75       18.66800   \n",
              "...                 ...                    ...            ...   \n",
              "51915           24565.0                  -3.20        2.00000   \n",
              "51916           24532.0                  -2.00        0.80000   \n",
              "51917           24504.4                  -2.40        1.80000   \n",
              "51918           24474.6                  -2.00        2.60000   \n",
              "51919           24441.8                  -2.20        1.60000   \n",
              "\n",
              "       VALUE_FOB_MISSING  VALUE_FOB_MISSING_BY_QTY  VALUE_FOB_BY_FUEL_USED  \\\n",
              "0                  69.75                      61.0            22099.610500   \n",
              "1                  84.80                      76.0            22088.679140   \n",
              "2                  84.80                      75.6            22078.717900   \n",
              "3                  81.25                      72.5            22069.307775   \n",
              "4                  79.75                      70.0            22058.870075   \n",
              "...                  ...                       ...                     ...   \n",
              "51915             848.20                     846.0            10055.529688   \n",
              "51916             880.00                     879.0            10049.216650   \n",
              "51917             908.00                     906.6            10042.916602   \n",
              "51918             937.40                     936.4            10036.660156   \n",
              "51919             970.40                     969.2            10030.729395   \n",
              "\n",
              "       label  \n",
              "0        0.0  \n",
              "1        0.0  \n",
              "2        0.0  \n",
              "3        0.0  \n",
              "4        0.0  \n",
              "...      ...  \n",
              "51915    1.0  \n",
              "51916    1.0  \n",
              "51917    1.0  \n",
              "51918    1.0  \n",
              "51919    1.0  \n",
              "\n",
              "[51920 rows x 23 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the dataset\n",
        "#final = pd.read_csv(\"/content/drive/MyDrive/Uni/IE/Capstone/dataset_autoencoder_with_fuel_leak_resampled.csv\", sep=',')\n",
        "final = pd.read_csv(\"generated_data/dataset_autoencoder_with_fuel_leak_resampled.csv\", sep=',')\n",
        "final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zZv_5silV975"
      },
      "outputs": [],
      "source": [
        "dataset = final.copy()\n",
        "dataset = dataset.dropna()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H3MTSme0_uPL"
      },
      "source": [
        "## 3) Feature selection\n",
        "\n",
        "We decided to combine all the fuel used and all the fuel tank quantities into 2 features to simplify our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QOLsdaeVZduZ"
      },
      "outputs": [],
      "source": [
        "def add_features(dataset):\n",
        "  dataset['TOTAL_FUEL_USED'] = dataset['FUEL_USED_1'] +  dataset['FUEL_USED_2'] +  dataset['FUEL_USED_3'] +  dataset['FUEL_USED_4']\n",
        "  dataset['TOTAL_FOB_BY_QTY'] = dataset['VALUE_FUEL_QTY_CT'] + dataset['VALUE_FUEL_QTY_FT1'] + dataset['VALUE_FUEL_QTY_FT2'] + dataset['VALUE_FUEL_QTY_FT3'] + dataset['VALUE_FUEL_QTY_FT4'] + dataset['VALUE_FUEL_QTY_LXT'] + dataset['VALUE_FUEL_QTY_RXT']\n",
        "  return dataset\n",
        "\n",
        "def drop_features(dataset):\n",
        "    dataset = dataset.drop(['FUEL_USED_1','FUEL_USED_2','FUEL_USED_3','FUEL_USED_4','VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3','VALUE_FUEL_QTY_FT4', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT'], 1)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWF6GQfzZdua",
        "outputId": "a6440e0a-5bd5-4937-e7f9-d842332bfdc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/jz/7k4lz9z91w92yqvc2ffzy2b80000gn/T/ipykernel_9720/3407511113.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  dataset = dataset.drop(['FUEL_USED_1','FUEL_USED_2','FUEL_USED_3','FUEL_USED_4','VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3','VALUE_FUEL_QTY_FT4', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT'], 1)\n"
          ]
        }
      ],
      "source": [
        "dataset = add_features(dataset)\n",
        "dataset = drop_features(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyGuvJMM99tK",
        "outputId": "8bdf9ce7-eebd-4d95-d3a7-a344c22c892d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 51920 entries, 0 to 51919\n",
            "Data columns (total 12 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   FW_GEO_ALTITUDE           51920 non-null  float64\n",
            " 1   VALUE_FOB                 51920 non-null  float64\n",
            " 2   FLIGHT                    51920 non-null  float64\n",
            " 3   TOTAL_FUEL_USED           51920 non-null  float64\n",
            " 4   VALUE_FOB_DIFF            51920 non-null  float64\n",
            " 5   TOTAL_FOB_BY_QTY          51920 non-null  float64\n",
            " 6   DELTA_VFOB_VS_VFOBQTY     51920 non-null  float64\n",
            " 7   ALTITUDE_DIFF             51920 non-null  float64\n",
            " 8   VALUE_FOB_MISSING         51920 non-null  float64\n",
            " 9   VALUE_FOB_MISSING_BY_QTY  51920 non-null  float64\n",
            " 10  VALUE_FOB_BY_FUEL_USED    51920 non-null  float64\n",
            " 11  label                     51920 non-null  float64\n",
            "dtypes: float64(12)\n",
            "memory usage: 4.8 MB\n"
          ]
        }
      ],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6b5p9EQ-Zdua"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.drop(['FW_GEO_ALTITUDE'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BFVKgGS-Zdua"
      },
      "outputs": [],
      "source": [
        "move_column1 = dataset.pop('label')\n",
        "\n",
        "dataset.insert(10, 'LABEL', move_column1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OsLhYp62fNU7"
      },
      "outputs": [],
      "source": [
        "dataset[\"LABEL\"].replace([1, 0], [0, 1], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#dataset.to_csv(\"airclean_data/before_autoencoding.csv\", decimal=)\n",
        "\n",
        "dataset.to_clipboard(decimal=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNSJ1kMTfnbx",
        "outputId": "2462b35f-caae-495f-94f6-62b62aad820c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0    29746\n",
              "0.0    22174\n",
              "Name: LABEL, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['LABEL'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# devide raw_data into train and test sets. by flight sequence 2/3 of the data for training and the 1/3 for testing\n",
        "train = pd.DataFrame()\n",
        "test = pd.DataFrame()\n",
        "\n",
        "for flight in dataset[\"FLIGHT\"].unique():\n",
        "    flight_data = dataset[dataset[\"FLIGHT\"] == flight]\n",
        "    train = pd.concat([train, flight_data.iloc[:int(len(flight_data)*1/3)]])\n",
        "    test = pd.concat([test, flight_data.iloc[int(len(flight_data)*2/3):]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T0yLo3HEZdua"
      },
      "outputs": [],
      "source": [
        "# raw data for train_test_split method\n",
        "raw_data = dataset.values\n",
        "\n",
        "raw_train = train.values\n",
        "raw_test = test.values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb3-WbuUAk2c"
      },
      "source": [
        "## 4) Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TgvFXJ1xZdua"
      },
      "outputs": [],
      "source": [
        "# The last element contains the labels\n",
        "labels = raw_data[:, -1]\n",
        "\n",
        "# The other data points are the data\n",
        "data = raw_data[:, 0:-1]\n",
        "\n",
        "# assign train set\n",
        "train_data = raw_train[:, 0:-1]\n",
        "train_labels = raw_train[:, -1]\n",
        " \n",
        "# assign test set\n",
        "test_data = raw_test[:, 0:-1] \n",
        "test_labels = raw_test[:, -1]\n",
        "\n",
        "# train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xXpj_x8Zdub",
        "outputId": "f47f6d02-b877-40a9-edf4-81399447e975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16799, 10)\n",
            "(16799,)\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOcB7dWSZdub",
        "outputId": "a2bab5cd-aad5-49b3-b42e-5867e7f46712"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5737246264658611"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(train_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TjpKhCL-AxsK"
      },
      "source": [
        "## 4) Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DigHMBCAqJtz"
      },
      "outputs": [],
      "source": [
        "train_data_not_normalized = train_data.copy()\n",
        "test_data_not_normalized = test_data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4htDUTucZdub"
      },
      "outputs": [],
      "source": [
        "min_val = tf.reduce_min(train_data)\n",
        "max_val = tf.reduce_max(train_data)\n",
        "\n",
        "train_data = (train_data - min_val) / (max_val - min_val)\n",
        "test_data = (test_data - min_val) / (max_val - min_val)\n",
        "\n",
        "train_data = tf.cast(train_data, tf.float64)\n",
        "test_data = tf.cast(test_data, tf.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-1XmqfFwZdub"
      },
      "outputs": [],
      "source": [
        "train_labels = train_labels.astype(bool)\n",
        "test_labels = test_labels.astype(bool)\n",
        "\n",
        "normal_train_data = train_data[train_labels]\n",
        "normal_test_data = test_data[test_labels]\n",
        "\n",
        "anomalous_train_data = train_data[~train_labels]\n",
        "anomalous_test_data = test_data[~test_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "eHhHOnUTZdub",
        "outputId": "d549614e-c9c7-4e9c-94c6-d156eab3b29e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(10), normal_train_data[0])\n",
        "plt.title(\"normal sequence\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "xMBhad8WZdub",
        "outputId": "990c4db0-3211-46ee-cbd2-8de267e4e4a5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(10), anomalous_train_data[0])\n",
        "plt.title(\"abnormal sequence\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Up70W86_BwEa"
      },
      "source": [
        "## 5) Autoencoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yk4QvM6IZdub"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 400\n",
        "BATCH_SIZE = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Define the costs\n",
        "cost_true_negative = 0\n",
        "cost_true_positive = 0\n",
        "cost_false_negative = 15  # Cost of a false negative\n",
        "cost_false_positive = 10  # Cost of a false positive\n",
        "\n",
        "\n",
        "def modified_cost(y_true, y_pred):\n",
        "    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
        "    fn = tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred) * tf.keras.backend.cast(tf.keras.backend.equal(y_true, 1), tf.keras.backend.floatx()))\n",
        "    fp = tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred) * tf.keras.backend.cast(tf.keras.backend.equal(y_true, 0), tf.keras.backend.floatx()))\n",
        "    tn = tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred) * tf.keras.backend.cast(tf.keras.backend.equal(y_true, 0), tf.keras.backend.floatx()))\n",
        "    tp = tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred) * tf.keras.backend.cast(tf.keras.backend.equal(y_true, 1), tf.keras.backend.floatx()))\n",
        "    return mse + cost_false_negative * fn + cost_false_positive * fp + cost_true_negative * tn + cost_true_positive * tp\n",
        "\n",
        "\n",
        "class AnomalyDetector(Model):\n",
        "  def __init__(self):\n",
        "    super(AnomalyDetector, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Dense(80, activation=\"relu\"),\n",
        "      layers.Dense(70, activation=\"relu\"),\n",
        "      layers.Dense(60, activation=\"relu\"),\n",
        "      layers.Dense(50, activation=\"relu\"),\n",
        "      layers.Dense(40, activation=\"relu\"),\n",
        "      layers.Dense(30, activation=\"relu\"),\n",
        "      layers.Dense(20, activation=\"relu\"),\n",
        "      layers.Dense(10, activation=\"relu\")\n",
        "      ])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(20, activation=\"relu\"),\n",
        "      layers.Dense(30, activation=\"relu\"),\n",
        "      layers.Dense(40, activation=\"relu\"),\n",
        "      layers.Dense(50, activation=\"relu\"),\n",
        "      layers.Dense(60, activation=\"relu\"),\n",
        "      layers.Dense(70, activation=\"relu\"),\n",
        "      layers.Dense(80, activation=\"relu\"),\n",
        "      layers.Dense(10, activation=\"sigmoid\")])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "autoencoder = AnomalyDetector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "76/76 [==============================] - 1s 3ms/step - loss: 0.1081 - val_loss: 0.0053\n",
            "Epoch 2/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0706 - val_loss: 0.0075\n",
            "Epoch 3/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0549 - val_loss: 0.0049\n",
            "Epoch 4/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0669 - val_loss: 0.0378\n",
            "Epoch 5/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0453 - val_loss: 0.0219\n",
            "Epoch 6/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0299\n",
            "Epoch 7/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0205\n",
            "Epoch 8/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.0351\n",
            "Epoch 9/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0203\n",
            "Epoch 10/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 0.0198\n",
            "Epoch 11/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0087\n",
            "Epoch 12/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0687 - val_loss: 0.0116\n",
            "Epoch 13/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0547 - val_loss: 0.0126\n",
            "Epoch 14/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 0.0190\n",
            "Epoch 15/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0189\n",
            "Epoch 16/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.0184\n",
            "Epoch 17/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 0.0207\n",
            "Epoch 18/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 0.0212\n",
            "Epoch 19/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0177\n",
            "Epoch 20/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0631 - val_loss: 0.0308\n",
            "Epoch 21/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0244\n",
            "Epoch 22/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0140\n",
            "Epoch 23/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0238\n",
            "Epoch 24/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.0207\n",
            "Epoch 25/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0206\n",
            "Epoch 26/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0204\n",
            "Epoch 27/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.0215\n",
            "Epoch 28/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0204\n",
            "Epoch 29/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0205\n",
            "Epoch 30/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0211\n",
            "Epoch 31/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0192\n",
            "Epoch 32/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0239\n",
            "Epoch 33/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0194\n",
            "Epoch 34/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0276\n",
            "Epoch 35/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0217\n",
            "Epoch 36/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0174\n",
            "Epoch 37/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0213\n",
            "Epoch 38/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0221\n",
            "Epoch 39/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0098\n",
            "Epoch 40/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0125\n",
            "Epoch 41/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0085\n",
            "Epoch 42/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0066\n",
            "Epoch 43/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0140\n",
            "Epoch 44/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0039\n",
            "Epoch 45/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0093\n",
            "Epoch 46/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0090\n",
            "Epoch 47/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0086\n",
            "Epoch 48/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0142\n",
            "Epoch 49/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0093\n",
            "Epoch 50/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0092\n",
            "Epoch 51/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0091\n",
            "Epoch 52/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0091\n",
            "Epoch 53/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0091\n",
            "Epoch 54/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0091\n",
            "Epoch 55/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0089 - val_loss: 0.0091\n",
            "Epoch 56/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 57/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 58/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 59/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 60/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 61/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 62/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 63/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 64/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 65/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 66/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 67/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 68/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 69/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 70/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 71/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 72/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 73/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 74/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 75/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 76/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 77/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 78/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 79/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 80/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 81/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 82/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 83/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 84/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 85/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 86/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 87/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 88/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 89/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 90/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 91/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 92/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 93/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 94/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 95/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 96/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 97/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 98/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0091\n",
            "Epoch 99/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0090\n",
            "Epoch 100/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0090\n",
            "Epoch 1/100\n",
            "76/76 [==============================] - 1s 3ms/step - loss: 0.1487 - val_loss: 0.0639\n",
            "Epoch 2/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1456 - val_loss: 0.0608\n",
            "Epoch 3/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.0453\n",
            "Epoch 4/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0936 - val_loss: 0.0158\n",
            "Epoch 5/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0534 - val_loss: 0.0121\n",
            "Epoch 6/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0406 - val_loss: 0.0139\n",
            "Epoch 7/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.0142\n",
            "Epoch 8/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0146\n",
            "Epoch 9/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 0.0159\n",
            "Epoch 10/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.0161\n",
            "Epoch 11/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0166\n",
            "Epoch 12/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0179\n",
            "Epoch 13/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0188\n",
            "Epoch 14/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0186\n",
            "Epoch 15/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0193\n",
            "Epoch 16/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0193\n",
            "Epoch 17/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0192\n",
            "Epoch 18/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.0193\n",
            "Epoch 19/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0182\n",
            "Epoch 20/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0186\n",
            "Epoch 21/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0188\n",
            "Epoch 22/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0188\n",
            "Epoch 23/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0189\n",
            "Epoch 24/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0184\n",
            "Epoch 25/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.0198\n",
            "Epoch 26/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0183\n",
            "Epoch 27/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0178\n",
            "Epoch 28/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0176\n",
            "Epoch 29/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0186\n",
            "Epoch 30/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0179\n",
            "Epoch 31/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0183\n",
            "Epoch 32/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0179\n",
            "Epoch 33/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0168\n",
            "Epoch 34/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0186\n",
            "Epoch 35/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0202\n",
            "Epoch 36/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0197\n",
            "Epoch 37/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0187\n",
            "Epoch 38/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0174\n",
            "Epoch 39/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0163\n",
            "Epoch 40/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0186\n",
            "Epoch 41/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0184\n",
            "Epoch 42/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0173\n",
            "Epoch 43/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0190\n",
            "Epoch 44/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0163\n",
            "Epoch 45/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0171\n",
            "Epoch 46/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0170\n",
            "Epoch 47/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0174\n",
            "Epoch 48/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0166\n",
            "Epoch 49/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0163\n",
            "Epoch 50/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0150\n",
            "Epoch 51/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0143\n",
            "Epoch 52/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0144\n",
            "Epoch 53/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0146\n",
            "Epoch 54/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0164\n",
            "Epoch 55/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0161\n",
            "Epoch 56/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0146\n",
            "Epoch 57/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.0148\n",
            "Epoch 58/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0154\n",
            "Epoch 59/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0166\n",
            "Epoch 60/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0143\n",
            "Epoch 61/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0146\n",
            "Epoch 62/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0154\n",
            "Epoch 63/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0128\n",
            "Epoch 64/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0176 - val_loss: 0.0121\n",
            "Epoch 65/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0121\n",
            "Epoch 66/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0135\n",
            "Epoch 67/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0142\n",
            "Epoch 68/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0139\n",
            "Epoch 69/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0114\n",
            "Epoch 70/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0136\n",
            "Epoch 71/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0130\n",
            "Epoch 72/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0109\n",
            "Epoch 73/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0101\n",
            "Epoch 74/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0145\n",
            "Epoch 75/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0120\n",
            "Epoch 76/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0144 - val_loss: 0.0129\n",
            "Epoch 77/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0122\n",
            "Epoch 78/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0114\n",
            "Epoch 79/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0108\n",
            "Epoch 80/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0103\n",
            "Epoch 81/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0093\n",
            "Epoch 82/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0088\n",
            "Epoch 83/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0113\n",
            "Epoch 84/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0090\n",
            "Epoch 85/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0082\n",
            "Epoch 86/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0079\n",
            "Epoch 87/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0090\n",
            "Epoch 88/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0108 - val_loss: 0.0078\n",
            "Epoch 89/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0072\n",
            "Epoch 90/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.0104\n",
            "Epoch 91/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0068\n",
            "Epoch 92/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0084\n",
            "Epoch 93/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0065\n",
            "Epoch 94/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0064\n",
            "Epoch 95/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0069\n",
            "Epoch 96/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0062\n",
            "Epoch 97/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0086\n",
            "Epoch 98/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0056\n",
            "Epoch 99/100\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0057\n",
            "Epoch 100/100\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0073\n",
            "Epoch 1/200\n",
            "76/76 [==============================] - 1s 3ms/step - loss: 0.1133 - val_loss: 0.0044\n",
            "Epoch 2/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0641 - val_loss: 0.0093\n",
            "Epoch 3/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.0128\n",
            "Epoch 4/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0532 - val_loss: 0.0110\n",
            "Epoch 5/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0411 - val_loss: 0.0092\n",
            "Epoch 6/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0459 - val_loss: 0.0094\n",
            "Epoch 7/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0475 - val_loss: 0.0174\n",
            "Epoch 8/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.0129\n",
            "Epoch 9/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.0159\n",
            "Epoch 10/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0384 - val_loss: 0.0208\n",
            "Epoch 11/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0070\n",
            "Epoch 12/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0610 - val_loss: 0.0043\n",
            "Epoch 13/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.0160\n",
            "Epoch 14/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0706 - val_loss: 0.0152\n",
            "Epoch 15/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0738 - val_loss: 0.0240\n",
            "Epoch 16/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.0290\n",
            "Epoch 17/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.0242\n",
            "Epoch 18/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0206\n",
            "Epoch 19/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0655 - val_loss: 0.0086\n",
            "Epoch 20/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0261\n",
            "Epoch 21/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0227\n",
            "Epoch 22/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0308 - val_loss: 0.0194\n",
            "Epoch 23/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0271\n",
            "Epoch 24/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0168\n",
            "Epoch 25/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.0246\n",
            "Epoch 26/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.0090\n",
            "Epoch 27/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0035\n",
            "Epoch 28/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0111\n",
            "Epoch 29/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0091\n",
            "Epoch 30/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0090\n",
            "Epoch 31/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0088\n",
            "Epoch 32/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.0059\n",
            "Epoch 33/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0135 - val_loss: 0.0168\n",
            "Epoch 34/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.0090\n",
            "Epoch 35/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0087\n",
            "Epoch 36/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0031\n",
            "Epoch 37/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0090\n",
            "Epoch 38/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.0077\n",
            "Epoch 39/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0237\n",
            "Epoch 40/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0019\n",
            "Epoch 41/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0256\n",
            "Epoch 42/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0021\n",
            "Epoch 43/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0082\n",
            "Epoch 44/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0050\n",
            "Epoch 45/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0017\n",
            "Epoch 46/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 47/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 48/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0012\n",
            "Epoch 49/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 9.6815e-04\n",
            "Epoch 50/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0036\n",
            "Epoch 51/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0010\n",
            "Epoch 52/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "Epoch 53/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 54/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 8.8697e-04\n",
            "Epoch 55/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 8.9135e-04\n",
            "Epoch 56/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 57/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 8.7457e-04\n",
            "Epoch 58/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 8.9217e-04\n",
            "Epoch 59/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 9.6035e-04\n",
            "Epoch 60/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.3403e-04 - val_loss: 8.5191e-04\n",
            "Epoch 61/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 62/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 63/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 8.9034e-04\n",
            "Epoch 64/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.0142e-04 - val_loss: 8.4321e-04\n",
            "Epoch 65/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 66/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.2932e-04 - val_loss: 8.4208e-04\n",
            "Epoch 67/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 8.6788e-04\n",
            "Epoch 68/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.2788e-04 - val_loss: 8.3721e-04\n",
            "Epoch 69/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 8.7530e-04\n",
            "Epoch 70/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.9124e-04 - val_loss: 8.3437e-04\n",
            "Epoch 71/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 8.4609e-04\n",
            "Epoch 72/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.5455e-04 - val_loss: 8.4725e-04\n",
            "Epoch 73/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 74/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.3792e-04 - val_loss: 8.5372e-04\n",
            "Epoch 75/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 9.0900e-04 - val_loss: 8.3060e-04\n",
            "Epoch 76/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.4304e-04 - val_loss: 8.3861e-04\n",
            "Epoch 77/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.7566e-04 - val_loss: 9.0664e-04\n",
            "Epoch 78/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.8080e-04 - val_loss: 8.3336e-04\n",
            "Epoch 79/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.9380e-04 - val_loss: 8.2131e-04\n",
            "Epoch 80/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.8512e-04 - val_loss: 0.0011\n",
            "Epoch 81/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.9571e-04 - val_loss: 8.1723e-04\n",
            "Epoch 82/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.3700e-04 - val_loss: 8.2043e-04\n",
            "Epoch 83/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.1470e-04 - val_loss: 8.3293e-04\n",
            "Epoch 84/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.3885e-04 - val_loss: 0.0011\n",
            "Epoch 85/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.5071e-04 - val_loss: 8.0740e-04\n",
            "Epoch 86/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 9.1819e-04\n",
            "Epoch 87/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.8237e-04 - val_loss: 8.9225e-04\n",
            "Epoch 88/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.6701e-04 - val_loss: 9.6819e-04\n",
            "Epoch 89/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.7889e-04 - val_loss: 8.0318e-04\n",
            "Epoch 90/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.5621e-04 - val_loss: 8.0612e-04\n",
            "Epoch 91/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.2297e-04 - val_loss: 8.1322e-04\n",
            "Epoch 92/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.4346e-04 - val_loss: 8.0431e-04\n",
            "Epoch 93/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.1743e-04 - val_loss: 8.6645e-04\n",
            "Epoch 94/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.0925e-04 - val_loss: 8.0052e-04\n",
            "Epoch 95/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.8342e-04 - val_loss: 8.0893e-04\n",
            "Epoch 96/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 8.3302e-04 - val_loss: 8.0464e-04\n",
            "Epoch 97/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.0995e-04 - val_loss: 7.9342e-04\n",
            "Epoch 98/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.8472e-04 - val_loss: 8.2413e-04\n",
            "Epoch 99/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.9778e-04 - val_loss: 7.8693e-04\n",
            "Epoch 100/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.0738e-04 - val_loss: 9.0552e-04\n",
            "Epoch 101/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.0773e-04 - val_loss: 7.8475e-04\n",
            "Epoch 102/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.7463e-04 - val_loss: 9.2967e-04\n",
            "Epoch 103/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.0321e-04 - val_loss: 7.8591e-04\n",
            "Epoch 104/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 8.4554e-04 - val_loss: 0.0015\n",
            "Epoch 105/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.8593e-04 - val_loss: 8.1153e-04\n",
            "Epoch 106/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.7868e-04 - val_loss: 7.7703e-04\n",
            "Epoch 107/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.9583e-04 - val_loss: 0.0015\n",
            "Epoch 108/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.6961e-04 - val_loss: 8.1143e-04\n",
            "Epoch 109/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.7430e-04 - val_loss: 7.7422e-04\n",
            "Epoch 110/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.2415e-04 - val_loss: 8.2303e-04\n",
            "Epoch 111/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.9099e-04 - val_loss: 8.2509e-04\n",
            "Epoch 112/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.6695e-04 - val_loss: 7.5887e-04\n",
            "Epoch 113/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.5112e-04 - val_loss: 7.4325e-04\n",
            "Epoch 114/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 7.4396e-04 - val_loss: 7.2443e-04\n",
            "Epoch 115/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.7555e-04 - val_loss: 5.1505e-04\n",
            "Epoch 116/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 4.1242e-04 - val_loss: 1.6572e-04\n",
            "Epoch 117/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 1.7738e-04 - val_loss: 3.3669e-04\n",
            "Epoch 118/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 1.3095e-04 - val_loss: 9.5284e-05\n",
            "Epoch 119/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.2185e-05 - val_loss: 2.9256e-04\n",
            "Epoch 120/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 1.2331e-04 - val_loss: 8.5294e-05\n",
            "Epoch 121/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.7415e-05 - val_loss: 1.0567e-04\n",
            "Epoch 122/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.0614e-05 - val_loss: 7.7866e-05\n",
            "Epoch 123/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 8.8820e-05 - val_loss: 8.1331e-05\n",
            "Epoch 124/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.1897e-05 - val_loss: 7.6623e-05\n",
            "Epoch 125/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.6834e-05 - val_loss: 9.2906e-05\n",
            "Epoch 126/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.4024e-05 - val_loss: 8.4281e-05\n",
            "Epoch 127/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.9034e-05 - val_loss: 7.3787e-05\n",
            "Epoch 128/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.7870e-05 - val_loss: 7.2797e-05\n",
            "Epoch 129/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.6219e-05 - val_loss: 7.2264e-05\n",
            "Epoch 130/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.4271e-05 - val_loss: 7.2973e-05\n",
            "Epoch 131/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.6048e-05 - val_loss: 8.2760e-05\n",
            "Epoch 132/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.0571e-05 - val_loss: 6.7721e-05\n",
            "Epoch 133/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 7.2969e-05 - val_loss: 8.5599e-05\n",
            "Epoch 134/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.6313e-05 - val_loss: 7.3987e-05\n",
            "Epoch 135/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.4282e-05 - val_loss: 9.0800e-05\n",
            "Epoch 136/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.3076e-05 - val_loss: 6.7544e-05\n",
            "Epoch 137/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.7559e-05 - val_loss: 7.2244e-05\n",
            "Epoch 138/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.7318e-05 - val_loss: 9.6623e-05\n",
            "Epoch 139/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.3622e-05 - val_loss: 7.0413e-05\n",
            "Epoch 140/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 6.5007e-05 - val_loss: 6.9813e-05\n",
            "Epoch 141/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.2368e-05 - val_loss: 8.2305e-05\n",
            "Epoch 142/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.4259e-05 - val_loss: 6.6759e-05\n",
            "Epoch 143/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.3145e-05 - val_loss: 7.6770e-05\n",
            "Epoch 144/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.2034e-05 - val_loss: 6.6851e-05\n",
            "Epoch 145/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.1947e-05 - val_loss: 7.0106e-05\n",
            "Epoch 146/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.1674e-05 - val_loss: 6.8668e-05\n",
            "Epoch 147/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.1615e-05 - val_loss: 7.1310e-05\n",
            "Epoch 148/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.2057e-05 - val_loss: 6.6180e-05\n",
            "Epoch 149/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 6.0856e-05 - val_loss: 7.1310e-05\n",
            "Epoch 150/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.9091e-05 - val_loss: 6.7937e-05\n",
            "Epoch 151/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.9935e-05 - val_loss: 7.0367e-05\n",
            "Epoch 152/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.3194e-05 - val_loss: 6.8218e-05\n",
            "Epoch 153/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.8513e-05 - val_loss: 6.9729e-05\n",
            "Epoch 154/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.8541e-05 - val_loss: 6.7121e-05\n",
            "Epoch 155/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.7122e-05 - val_loss: 1.2026e-04\n",
            "Epoch 156/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.4256e-05 - val_loss: 6.8331e-05\n",
            "Epoch 157/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.7477e-05 - val_loss: 6.3855e-05\n",
            "Epoch 158/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.7432e-05 - val_loss: 7.6780e-05\n",
            "Epoch 159/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.9265e-05 - val_loss: 6.5437e-05\n",
            "Epoch 160/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.7676e-05 - val_loss: 8.4798e-05\n",
            "Epoch 161/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.9667e-05 - val_loss: 6.3440e-05\n",
            "Epoch 162/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.6041e-05 - val_loss: 6.3759e-05\n",
            "Epoch 163/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.8300e-05 - val_loss: 7.0308e-05\n",
            "Epoch 164/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.7676e-05 - val_loss: 6.7223e-05\n",
            "Epoch 165/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.7524e-05 - val_loss: 6.6977e-05\n",
            "Epoch 166/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.7253e-05 - val_loss: 7.1352e-05\n",
            "Epoch 167/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.7514e-05 - val_loss: 6.4385e-05\n",
            "Epoch 168/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.7325e-05 - val_loss: 6.4027e-05\n",
            "Epoch 169/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.4958e-05 - val_loss: 6.1885e-05\n",
            "Epoch 170/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.6632e-05 - val_loss: 6.7798e-05\n",
            "Epoch 171/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.6176e-05 - val_loss: 6.0658e-05\n",
            "Epoch 172/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.4441e-05 - val_loss: 6.2733e-05\n",
            "Epoch 173/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.5913e-05 - val_loss: 6.2342e-05\n",
            "Epoch 174/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.4354e-05 - val_loss: 6.6897e-05\n",
            "Epoch 175/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.6433e-05 - val_loss: 6.0769e-05\n",
            "Epoch 176/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.2626e-05 - val_loss: 6.2223e-05\n",
            "Epoch 177/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.3743e-05 - val_loss: 6.5429e-05\n",
            "Epoch 178/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.3893e-05 - val_loss: 6.1732e-05\n",
            "Epoch 179/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.3900e-05 - val_loss: 5.7988e-05\n",
            "Epoch 180/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.5732e-05 - val_loss: 6.3505e-05\n",
            "Epoch 181/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.4965e-05 - val_loss: 6.0504e-05\n",
            "Epoch 182/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.2446e-05 - val_loss: 6.0556e-05\n",
            "Epoch 183/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.5165e-05 - val_loss: 6.3838e-05\n",
            "Epoch 184/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.2572e-05 - val_loss: 6.0000e-05\n",
            "Epoch 185/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.2677e-05 - val_loss: 6.5119e-05\n",
            "Epoch 186/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.3747e-05 - val_loss: 5.9751e-05\n",
            "Epoch 187/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.2234e-05 - val_loss: 8.4486e-05\n",
            "Epoch 188/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.5837e-05 - val_loss: 6.3752e-05\n",
            "Epoch 189/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.3573e-05 - val_loss: 7.3933e-05\n",
            "Epoch 190/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.4975e-05 - val_loss: 6.8969e-05\n",
            "Epoch 191/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.2290e-05 - val_loss: 7.1090e-05\n",
            "Epoch 192/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.2884e-05 - val_loss: 5.8132e-05\n",
            "Epoch 193/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.3202e-05 - val_loss: 6.2098e-05\n",
            "Epoch 194/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.3097e-05 - val_loss: 6.4801e-05\n",
            "Epoch 195/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.1774e-05 - val_loss: 7.0176e-05\n",
            "Epoch 196/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.2433e-05 - val_loss: 5.7192e-05\n",
            "Epoch 197/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.1906e-05 - val_loss: 5.8845e-05\n",
            "Epoch 198/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.1813e-05 - val_loss: 6.9791e-05\n",
            "Epoch 199/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.3196e-05 - val_loss: 6.2169e-05\n",
            "Epoch 200/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.0755e-05 - val_loss: 6.1327e-05\n",
            "Epoch 1/200\n",
            "76/76 [==============================] - 1s 3ms/step - loss: 0.1489 - val_loss: 0.0625\n",
            "Epoch 2/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1429 - val_loss: 0.0491\n",
            "Epoch 3/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.1045 - val_loss: 0.0092\n",
            "Epoch 4/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 0.0083\n",
            "Epoch 5/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0098\n",
            "Epoch 6/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0463 - val_loss: 0.0100\n",
            "Epoch 7/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0398 - val_loss: 0.0090\n",
            "Epoch 8/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.0137\n",
            "Epoch 9/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.0147\n",
            "Epoch 10/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0165\n",
            "Epoch 11/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0178\n",
            "Epoch 12/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0176\n",
            "Epoch 13/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0186\n",
            "Epoch 14/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0187\n",
            "Epoch 15/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0318 - val_loss: 0.0193\n",
            "Epoch 16/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0189\n",
            "Epoch 17/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0190\n",
            "Epoch 18/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0166\n",
            "Epoch 19/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0201\n",
            "Epoch 20/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0199\n",
            "Epoch 21/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0196\n",
            "Epoch 22/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0166\n",
            "Epoch 23/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0179\n",
            "Epoch 24/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0184\n",
            "Epoch 25/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0307 - val_loss: 0.0176\n",
            "Epoch 26/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0193\n",
            "Epoch 27/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0185\n",
            "Epoch 28/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0190\n",
            "Epoch 29/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0180\n",
            "Epoch 30/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0191\n",
            "Epoch 31/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0194\n",
            "Epoch 32/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0196\n",
            "Epoch 33/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0187\n",
            "Epoch 34/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0186\n",
            "Epoch 35/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0172\n",
            "Epoch 36/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0186\n",
            "Epoch 37/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.0194\n",
            "Epoch 38/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0187\n",
            "Epoch 39/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0177\n",
            "Epoch 40/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0178\n",
            "Epoch 41/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0181\n",
            "Epoch 42/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0182\n",
            "Epoch 43/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0177\n",
            "Epoch 44/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0174\n",
            "Epoch 45/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0172\n",
            "Epoch 46/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.0172\n",
            "Epoch 47/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0176\n",
            "Epoch 48/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0170\n",
            "Epoch 49/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0174\n",
            "Epoch 50/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0175\n",
            "Epoch 51/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0177\n",
            "Epoch 52/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0164\n",
            "Epoch 53/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0160\n",
            "Epoch 54/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0173\n",
            "Epoch 55/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0169\n",
            "Epoch 56/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0168\n",
            "Epoch 57/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0163\n",
            "Epoch 58/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0164\n",
            "Epoch 59/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0173\n",
            "Epoch 60/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0168\n",
            "Epoch 61/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0162\n",
            "Epoch 62/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0179\n",
            "Epoch 63/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0166\n",
            "Epoch 64/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0166\n",
            "Epoch 65/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0154\n",
            "Epoch 66/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0149\n",
            "Epoch 67/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0149\n",
            "Epoch 68/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0141\n",
            "Epoch 69/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0147\n",
            "Epoch 70/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0146\n",
            "Epoch 71/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0138\n",
            "Epoch 72/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0144\n",
            "Epoch 73/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0145\n",
            "Epoch 74/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0147\n",
            "Epoch 75/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0131\n",
            "Epoch 76/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0134\n",
            "Epoch 77/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0174 - val_loss: 0.0137\n",
            "Epoch 78/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0116\n",
            "Epoch 79/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0136\n",
            "Epoch 80/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.0121\n",
            "Epoch 81/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0112\n",
            "Epoch 82/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0119\n",
            "Epoch 83/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0110\n",
            "Epoch 84/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0118\n",
            "Epoch 85/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.0117\n",
            "Epoch 86/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0122\n",
            "Epoch 87/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 0.0112\n",
            "Epoch 88/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0109\n",
            "Epoch 89/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0130\n",
            "Epoch 90/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0115\n",
            "Epoch 91/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0104\n",
            "Epoch 92/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0105\n",
            "Epoch 93/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0100\n",
            "Epoch 94/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0132 - val_loss: 0.0098\n",
            "Epoch 95/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0092\n",
            "Epoch 96/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.0097\n",
            "Epoch 97/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.0100\n",
            "Epoch 98/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.0089\n",
            "Epoch 99/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0085\n",
            "Epoch 100/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0084\n",
            "Epoch 101/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0084\n",
            "Epoch 102/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0111 - val_loss: 0.0077\n",
            "Epoch 103/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0078\n",
            "Epoch 104/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0076\n",
            "Epoch 105/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0070\n",
            "Epoch 106/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 107/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.0069\n",
            "Epoch 108/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0072\n",
            "Epoch 109/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0068\n",
            "Epoch 110/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0093 - val_loss: 0.0071\n",
            "Epoch 111/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.0068\n",
            "Epoch 112/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0062\n",
            "Epoch 113/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.0065\n",
            "Epoch 114/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.0071\n",
            "Epoch 115/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0066\n",
            "Epoch 116/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0061\n",
            "Epoch 117/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.0059\n",
            "Epoch 118/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0071\n",
            "Epoch 119/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0072 - val_loss: 0.0058\n",
            "Epoch 120/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0079\n",
            "Epoch 121/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0055\n",
            "Epoch 122/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0071\n",
            "Epoch 123/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0061\n",
            "Epoch 124/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0051\n",
            "Epoch 125/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0063\n",
            "Epoch 126/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0056\n",
            "Epoch 127/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0052\n",
            "Epoch 128/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0050\n",
            "Epoch 129/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0061\n",
            "Epoch 130/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0045\n",
            "Epoch 131/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0064\n",
            "Epoch 132/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0047\n",
            "Epoch 133/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0044\n",
            "Epoch 134/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0041\n",
            "Epoch 135/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0043\n",
            "Epoch 136/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0036\n",
            "Epoch 137/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0036\n",
            "Epoch 138/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0034\n",
            "Epoch 139/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0031\n",
            "Epoch 140/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0028\n",
            "Epoch 141/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 142/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0025\n",
            "Epoch 143/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0022\n",
            "Epoch 144/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 145/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 146/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0018\n",
            "Epoch 147/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 148/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 149/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 150/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 151/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 152/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 153/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0012\n",
            "Epoch 154/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0012\n",
            "Epoch 155/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 156/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 157/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 158/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 159/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 160/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 9.3238e-04\n",
            "Epoch 161/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 9.0461e-04\n",
            "Epoch 162/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 8.8645e-04\n",
            "Epoch 163/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 8.5400e-04\n",
            "Epoch 164/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 8.2087e-04\n",
            "Epoch 165/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 9.7746e-04 - val_loss: 7.9877e-04\n",
            "Epoch 166/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 9.3063e-04 - val_loss: 7.7000e-04\n",
            "Epoch 167/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.5534e-04 - val_loss: 9.3047e-04\n",
            "Epoch 168/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 8.3077e-04 - val_loss: 6.6726e-04\n",
            "Epoch 169/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 7.7695e-04 - val_loss: 6.4381e-04\n",
            "Epoch 170/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.8955e-04 - val_loss: 5.4045e-04\n",
            "Epoch 171/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 6.4035e-04 - val_loss: 4.9524e-04\n",
            "Epoch 172/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 5.9370e-04 - val_loss: 4.5012e-04\n",
            "Epoch 173/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 5.0154e-04 - val_loss: 3.8075e-04\n",
            "Epoch 174/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 4.7575e-04 - val_loss: 3.9156e-04\n",
            "Epoch 175/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 4.1655e-04 - val_loss: 3.1932e-04\n",
            "Epoch 176/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 3.9645e-04 - val_loss: 2.9767e-04\n",
            "Epoch 177/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 3.6203e-04 - val_loss: 2.8046e-04\n",
            "Epoch 178/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 3.6326e-04 - val_loss: 3.1586e-04\n",
            "Epoch 179/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 3.2101e-04 - val_loss: 2.5821e-04\n",
            "Epoch 180/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 3.3573e-04 - val_loss: 2.6648e-04\n",
            "Epoch 181/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 3.2315e-04 - val_loss: 2.7910e-04\n",
            "Epoch 182/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 3.0586e-04 - val_loss: 2.5024e-04\n",
            "Epoch 183/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 3.0266e-04 - val_loss: 2.6376e-04\n",
            "Epoch 184/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 3.0555e-04 - val_loss: 2.7195e-04\n",
            "Epoch 185/200\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 2.6742e-04 - val_loss: 3.2412e-04\n",
            "Epoch 186/200\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 2.7682e-04 - val_loss: 2.3120e-04\n",
            "Epoch 187/200\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 3.0187e-04 - val_loss: 2.4702e-04\n",
            "Epoch 188/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 2.5193e-04 - val_loss: 2.9192e-04\n",
            "Epoch 189/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 2.9097e-04 - val_loss: 2.5908e-04\n",
            "Epoch 190/200\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 2.3863e-04 - val_loss: 2.1783e-04\n",
            "Epoch 191/200\n",
            "76/76 [==============================] - 0s 4ms/step - loss: 2.9663e-04 - val_loss: 2.6010e-04\n",
            "Epoch 192/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 2.7703e-04 - val_loss: 2.8338e-04\n",
            "Epoch 193/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 2.4546e-04 - val_loss: 2.2462e-04\n",
            "Epoch 194/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 2.2235e-04 - val_loss: 2.1231e-04\n",
            "Epoch 195/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 2.7191e-04 - val_loss: 2.7211e-04\n",
            "Epoch 196/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 2.5471e-04 - val_loss: 2.5179e-04\n",
            "Epoch 197/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 2.4828e-04 - val_loss: 2.7952e-04\n",
            "Epoch 198/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 2.3344e-04 - val_loss: 2.5412e-04\n",
            "Epoch 199/200\n",
            "76/76 [==============================] - 0s 2ms/step - loss: 2.1970e-04 - val_loss: 2.0639e-04\n",
            "Epoch 200/200\n",
            "76/76 [==============================] - 0s 3ms/step - loss: 2.4119e-04 - val_loss: 2.0751e-04\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 1s 5ms/step - loss: 0.2167 - val_loss: 0.0405\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1634 - val_loss: 0.0094\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0949 - val_loss: 0.0120\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.0288\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.0272\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 0.0197\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 0.0123\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.0276\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.0304\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0445 - val_loss: 0.0316\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0406 - val_loss: 0.0292\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.0291\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.0272\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.0268\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.0292\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0292\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.0198\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.0427\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0316 - val_loss: 0.0209\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0232\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0185\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0209\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0222 - val_loss: 0.0163\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0238\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0156\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0198\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 0.0134\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0155\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0108\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0106\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - val_loss: 0.0102\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0105 - val_loss: 0.0097\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0191 - val_loss: 0.0211\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0141 - val_loss: 0.0090\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.0105\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0099 - val_loss: 0.0092\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0097 - val_loss: 0.0086\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0293\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0142 - val_loss: 0.0098\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0086 - val_loss: 0.0078\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0070 - val_loss: 0.0050\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0405\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0089\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0043\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0054 - val_loss: 0.0052\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 0.0022\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0020 - val_loss: 0.0025\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0041\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0019 - val_loss: 0.0012\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0010\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 9.7451e-04\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 9.2538e-04\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 9.2698e-04\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.9988e-04 - val_loss: 9.1218e-04\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 9.1489e-04\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.9915e-04 - val_loss: 0.0016\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0010 - val_loss: 8.8867e-04\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 9.6663e-04 - val_loss: 9.0034e-04\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.4186e-04 - val_loss: 9.5728e-04\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.1374e-04 - val_loss: 9.3280e-04\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.9184e-04 - val_loss: 8.5332e-04\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 9.2480e-04 - val_loss: 8.6791e-04\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.0873e-04 - val_loss: 9.4055e-04\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.6078e-04 - val_loss: 8.4077e-04\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.8045e-04 - val_loss: 8.4385e-04\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.7538e-04 - val_loss: 9.3423e-04\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.7643e-04 - val_loss: 8.4757e-04\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.5841e-04 - val_loss: 8.2907e-04\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.3570e-04 - val_loss: 8.2301e-04\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.1969e-04 - val_loss: 8.1592e-04\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.3594e-04 - val_loss: 0.0010\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 8.6736e-04 - val_loss: 8.1513e-04\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.1989e-04 - val_loss: 8.1447e-04\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.2430e-04 - val_loss: 8.4644e-04\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.2554e-04 - val_loss: 8.0904e-04\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.0978e-04 - val_loss: 8.0214e-04\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.0727e-04 - val_loss: 8.0274e-04\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.0476e-04 - val_loss: 8.4818e-04\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.0904e-04 - val_loss: 7.9668e-04\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.8878e-04 - val_loss: 7.8987e-04\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.0860e-04 - val_loss: 7.9518e-04\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.8962e-04 - val_loss: 8.0259e-04\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.9260e-04 - val_loss: 7.9199e-04\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.9176e-04 - val_loss: 8.1463e-04\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 7.9188e-04 - val_loss: 7.8649e-04\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.8006e-04 - val_loss: 7.9431e-04\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.7780e-04 - val_loss: 7.8775e-04\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.7355e-04 - val_loss: 7.7836e-04\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.7175e-04 - val_loss: 7.7366e-04\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 7.8388e-04 - val_loss: 7.8909e-04\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.7096e-04 - val_loss: 7.8043e-04\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 1s 5ms/step - loss: 0.2316 - val_loss: 0.0648\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.2289 - val_loss: 0.0636\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.2248 - val_loss: 0.0617\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.2189 - val_loss: 0.0577\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.2045 - val_loss: 0.0513\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.1831 - val_loss: 0.0395\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.1438 - val_loss: 0.0266\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 0.0190\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.0190\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.0221\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.0247\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.0243\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.0242\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.0231\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.0257\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0260\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 0.0236\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0266\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0259\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0261\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0268\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0265\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0350 - val_loss: 0.0255\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0269\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0265\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0267\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.0263\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0249\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0260\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0337 - val_loss: 0.0259\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0263\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0332 - val_loss: 0.0258\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0332 - val_loss: 0.0254\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0256\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0252\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0240\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.0249\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0321 - val_loss: 0.0250\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0229\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0246\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0317 - val_loss: 0.0229\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0225\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0320 - val_loss: 0.0241\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0216\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0241\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0230\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0236\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0223\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0221\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.0220\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0226\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0226\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0281 - val_loss: 0.0218\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0269 - val_loss: 0.0208\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0213\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0209\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0201\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.0207\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0208\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0258 - val_loss: 0.0197\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0194\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0251 - val_loss: 0.0195\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0188\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0191\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.0178\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0189\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0184\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.0183\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0232 - val_loss: 0.0180\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.0164\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0222 - val_loss: 0.0183\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0165\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0167\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0219 - val_loss: 0.0163\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0172\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0162\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0170\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0163\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0160\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0200 - val_loss: 0.0149\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0195 - val_loss: 0.0155\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0191 - val_loss: 0.0150\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0155\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.0149\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0142\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0184 - val_loss: 0.0135\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0176 - val_loss: 0.0143\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.0136\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 0.0143\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0129\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.0135\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0124\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0166 - val_loss: 0.0124\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.0128\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0156 - val_loss: 0.0122\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0158 - val_loss: 0.0116\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0158 - val_loss: 0.0124\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0149 - val_loss: 0.0108\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0140 - val_loss: 0.0118\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0141 - val_loss: 0.0108\n",
            "Epoch 1/200\n",
            "38/38 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.0069\n",
            "Epoch 2/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1215 - val_loss: 0.0072\n",
            "Epoch 3/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0808 - val_loss: 0.0112\n",
            "Epoch 4/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0650 - val_loss: 0.0149\n",
            "Epoch 5/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0803 - val_loss: 0.0169\n",
            "Epoch 6/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0708 - val_loss: 0.0255\n",
            "Epoch 7/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.0256\n",
            "Epoch 8/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.0357\n",
            "Epoch 9/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0432 - val_loss: 0.0350\n",
            "Epoch 10/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.0318\n",
            "Epoch 11/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.0277\n",
            "Epoch 12/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.0349\n",
            "Epoch 13/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.0247\n",
            "Epoch 14/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0448 - val_loss: 0.0315\n",
            "Epoch 15/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0388 - val_loss: 0.0260\n",
            "Epoch 16/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.0317\n",
            "Epoch 17/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.0207\n",
            "Epoch 18/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.0290\n",
            "Epoch 19/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.0255\n",
            "Epoch 20/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0438\n",
            "Epoch 21/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0302 - val_loss: 0.0217\n",
            "Epoch 22/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.0311\n",
            "Epoch 23/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0232 - val_loss: 0.0161\n",
            "Epoch 24/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0222\n",
            "Epoch 25/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0235\n",
            "Epoch 26/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0180\n",
            "Epoch 27/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0146\n",
            "Epoch 28/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0331 - val_loss: 0.0221\n",
            "Epoch 29/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.0159\n",
            "Epoch 30/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.0365\n",
            "Epoch 31/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.2817 - val_loss: 0.0386\n",
            "Epoch 32/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.0295\n",
            "Epoch 33/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.0303\n",
            "Epoch 34/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.0348\n",
            "Epoch 35/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0302\n",
            "Epoch 36/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0294\n",
            "Epoch 37/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0320 - val_loss: 0.0250\n",
            "Epoch 38/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0295 - val_loss: 0.0238\n",
            "Epoch 39/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0198\n",
            "Epoch 40/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0225 - val_loss: 0.0195\n",
            "Epoch 41/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0148\n",
            "Epoch 42/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0181 - val_loss: 0.0170\n",
            "Epoch 43/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0143 - val_loss: 0.0116\n",
            "Epoch 44/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0094\n",
            "Epoch 45/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0116\n",
            "Epoch 46/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0106\n",
            "Epoch 47/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0100 - val_loss: 0.0088\n",
            "Epoch 48/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0086 - val_loss: 0.0076\n",
            "Epoch 49/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0062\n",
            "Epoch 50/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0091\n",
            "Epoch 51/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0057\n",
            "Epoch 52/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0100 - val_loss: 0.0117\n",
            "Epoch 53/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0071 - val_loss: 0.0043\n",
            "Epoch 54/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0097\n",
            "Epoch 55/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0034\n",
            "Epoch 56/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0136\n",
            "Epoch 57/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0193 - val_loss: 0.0115\n",
            "Epoch 58/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0036\n",
            "Epoch 59/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0089 - val_loss: 0.0072\n",
            "Epoch 60/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.0020\n",
            "Epoch 61/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0151\n",
            "Epoch 62/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 63/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0301 - val_loss: 0.0330\n",
            "Epoch 64/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0187 - val_loss: 0.0124\n",
            "Epoch 65/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0107\n",
            "Epoch 66/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.0094\n",
            "Epoch 67/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0084 - val_loss: 0.0074\n",
            "Epoch 68/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0063\n",
            "Epoch 69/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0049 - val_loss: 0.0036\n",
            "Epoch 70/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0042\n",
            "Epoch 71/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 72/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 73/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 74/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 75/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 76/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 77/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 78/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 79/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 80/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0031\n",
            "Epoch 81/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0018 - val_loss: 0.0012\n",
            "Epoch 82/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 83/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 84/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.0010\n",
            "Epoch 85/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 86/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 9.9189e-04\n",
            "Epoch 87/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 9.9177e-04 - val_loss: 0.0012\n",
            "Epoch 88/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 8.6739e-04\n",
            "Epoch 89/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.9347e-04 - val_loss: 8.2898e-04\n",
            "Epoch 90/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.1828e-04 - val_loss: 8.8450e-04\n",
            "Epoch 91/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.9047e-04 - val_loss: 6.9232e-04\n",
            "Epoch 92/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 6.5115e-04 - val_loss: 5.7472e-04\n",
            "Epoch 93/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 9.3638e-04\n",
            "Epoch 94/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 5.9120e-04 - val_loss: 4.4288e-04\n",
            "Epoch 95/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0022 - val_loss: 0.0115\n",
            "Epoch 96/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0046\n",
            "Epoch 97/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0094\n",
            "Epoch 98/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0026\n",
            "Epoch 99/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 100/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 9.6795e-04\n",
            "Epoch 101/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 102/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 7.7016e-04\n",
            "Epoch 103/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.0033\n",
            "Epoch 104/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.0011\n",
            "Epoch 105/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 6.9457e-04 - val_loss: 4.0194e-04\n",
            "Epoch 106/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 3.4558e-04 - val_loss: 2.7772e-04\n",
            "Epoch 107/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 2.5866e-04 - val_loss: 2.3093e-04\n",
            "Epoch 108/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 2.2273e-04 - val_loss: 2.1048e-04\n",
            "Epoch 109/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 2.0268e-04 - val_loss: 1.8484e-04\n",
            "Epoch 110/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.9019e-04 - val_loss: 1.8032e-04\n",
            "Epoch 111/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.7295e-04 - val_loss: 1.6165e-04\n",
            "Epoch 112/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.6485e-04 - val_loss: 1.5534e-04\n",
            "Epoch 113/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.5550e-04 - val_loss: 1.4825e-04\n",
            "Epoch 114/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.5011e-04 - val_loss: 1.4681e-04\n",
            "Epoch 115/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.4812e-04 - val_loss: 1.3944e-04\n",
            "Epoch 116/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.4849e-04 - val_loss: 1.4806e-04\n",
            "Epoch 117/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.3938e-04 - val_loss: 1.3615e-04\n",
            "Epoch 118/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.3479e-04 - val_loss: 1.3968e-04\n",
            "Epoch 119/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.4365e-04 - val_loss: 1.5702e-04\n",
            "Epoch 120/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.4400e-04 - val_loss: 1.2871e-04\n",
            "Epoch 121/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.2497e-04 - val_loss: 1.2439e-04\n",
            "Epoch 122/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 1.2875e-04 - val_loss: 1.2869e-04\n",
            "Epoch 123/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.2187e-04 - val_loss: 1.2042e-04\n",
            "Epoch 124/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.1866e-04 - val_loss: 1.2064e-04\n",
            "Epoch 125/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.5869e-04 - val_loss: 1.7316e-04\n",
            "Epoch 126/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.3370e-04 - val_loss: 1.1573e-04\n",
            "Epoch 127/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.1469e-04 - val_loss: 1.1866e-04\n",
            "Epoch 128/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.1315e-04 - val_loss: 1.1386e-04\n",
            "Epoch 129/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.0958e-04 - val_loss: 1.1459e-04\n",
            "Epoch 130/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.8410e-04 - val_loss: 3.8091e-04\n",
            "Epoch 131/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.9585e-04 - val_loss: 1.2933e-04\n",
            "Epoch 132/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.1066e-04 - val_loss: 1.1089e-04\n",
            "Epoch 133/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.0524e-04 - val_loss: 1.0927e-04\n",
            "Epoch 134/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.0197e-04 - val_loss: 1.0511e-04\n",
            "Epoch 135/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.2642e-04 - val_loss: 1.1532e-04\n",
            "Epoch 136/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.0476e-04 - val_loss: 1.0667e-04\n",
            "Epoch 137/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.0022e-04 - val_loss: 1.0249e-04\n",
            "Epoch 138/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 3.0017e-04 - val_loss: 0.0016\n",
            "Epoch 139/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.1001e-04 - val_loss: 1.7682e-04\n",
            "Epoch 140/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.4211e-04 - val_loss: 1.4385e-04\n",
            "Epoch 141/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.2294e-04 - val_loss: 1.1272e-04\n",
            "Epoch 142/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.0441e-04 - val_loss: 1.0921e-04\n",
            "Epoch 143/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.0088e-04 - val_loss: 1.0492e-04\n",
            "Epoch 144/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.8498e-05 - val_loss: 1.0404e-04\n",
            "Epoch 145/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.8095e-05 - val_loss: 1.0235e-04\n",
            "Epoch 146/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.8864e-05 - val_loss: 1.0540e-04\n",
            "Epoch 147/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 9.4207e-05 - val_loss: 9.7509e-05\n",
            "Epoch 148/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.2693e-04 - val_loss: 1.3384e-04\n",
            "Epoch 149/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.0285e-04 - val_loss: 9.8915e-05\n",
            "Epoch 150/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 9.2686e-05 - val_loss: 9.7191e-05\n",
            "Epoch 151/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.3823e-05 - val_loss: 9.7506e-05\n",
            "Epoch 152/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.9853e-05 - val_loss: 9.4017e-05\n",
            "Epoch 153/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 4.5004e-04 - val_loss: 0.0027\n",
            "Epoch 154/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 2.4110e-04\n",
            "Epoch 155/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.5708e-04 - val_loss: 1.2428e-04\n",
            "Epoch 156/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.1131e-04 - val_loss: 1.1796e-04\n",
            "Epoch 157/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 1.0079e-04 - val_loss: 1.0393e-04\n",
            "Epoch 158/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.4713e-05 - val_loss: 9.9371e-05\n",
            "Epoch 159/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.1964e-05 - val_loss: 9.6486e-05\n",
            "Epoch 160/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.0704e-04 - val_loss: 1.5090e-04\n",
            "Epoch 161/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.1070e-04 - val_loss: 9.9327e-05\n",
            "Epoch 162/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.9005e-05 - val_loss: 9.3755e-05\n",
            "Epoch 163/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.9849e-05 - val_loss: 9.4907e-05\n",
            "Epoch 164/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.6524e-05 - val_loss: 9.3165e-05\n",
            "Epoch 165/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.5091e-05 - val_loss: 9.0126e-05\n",
            "Epoch 166/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 3.6335e-04 - val_loss: 0.0024\n",
            "Epoch 167/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.9591e-04 - val_loss: 1.7467e-04\n",
            "Epoch 168/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.4268e-04 - val_loss: 1.5209e-04\n",
            "Epoch 169/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.1957e-04 - val_loss: 1.0334e-04\n",
            "Epoch 170/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.3191e-05 - val_loss: 9.7973e-05\n",
            "Epoch 171/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 8.9261e-05 - val_loss: 9.5408e-05\n",
            "Epoch 172/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.7848e-05 - val_loss: 9.8211e-05\n",
            "Epoch 173/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.6867e-05 - val_loss: 9.3814e-05\n",
            "Epoch 174/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.5116e-05 - val_loss: 9.1674e-05\n",
            "Epoch 175/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 9.3627e-05 - val_loss: 1.3982e-04\n",
            "Epoch 176/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 1.0667e-04 - val_loss: 9.5054e-05\n",
            "Epoch 177/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.2584e-05 - val_loss: 8.7897e-05\n",
            "Epoch 178/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 1.0991e-04 - val_loss: 1.3726e-04\n",
            "Epoch 179/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.0153e-04 - val_loss: 9.2018e-05\n",
            "Epoch 180/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.1702e-05 - val_loss: 8.7735e-05\n",
            "Epoch 181/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.0203e-05 - val_loss: 8.6541e-05\n",
            "Epoch 182/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.9623e-05 - val_loss: 9.8458e-05\n",
            "Epoch 183/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.4114e-05 - val_loss: 8.7814e-05\n",
            "Epoch 184/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.7869e-05 - val_loss: 8.4522e-05\n",
            "Epoch 185/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 8.0316e-05 - val_loss: 8.6606e-05\n",
            "Epoch 186/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.9624e-05 - val_loss: 8.7581e-05\n",
            "Epoch 187/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.7849e-05 - val_loss: 8.3179e-05\n",
            "Epoch 188/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 8.2944e-05 - val_loss: 9.2510e-05\n",
            "Epoch 189/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.7270e-05 - val_loss: 8.2530e-05\n",
            "Epoch 190/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.6953e-05 - val_loss: 9.0731e-05\n",
            "Epoch 191/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.4743e-05 - val_loss: 8.0106e-05\n",
            "Epoch 192/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.9224e-05 - val_loss: 8.7611e-05\n",
            "Epoch 193/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.3186e-05 - val_loss: 8.1457e-05\n",
            "Epoch 194/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.4866e-05 - val_loss: 8.0156e-05\n",
            "Epoch 195/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 7.5926e-05 - val_loss: 8.4403e-05\n",
            "Epoch 196/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.1586e-05 - val_loss: 8.0018e-05\n",
            "Epoch 197/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.0375e-05 - val_loss: 8.0400e-05\n",
            "Epoch 198/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.2070e-05 - val_loss: 8.5289e-05\n",
            "Epoch 199/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 7.1028e-05 - val_loss: 7.9007e-05\n",
            "Epoch 200/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 1.0241e-04 - val_loss: 8.2800e-05\n",
            "Epoch 1/200\n",
            "38/38 [==============================] - 1s 5ms/step - loss: 0.2322 - val_loss: 0.0644\n",
            "Epoch 2/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.2299 - val_loss: 0.0633\n",
            "Epoch 3/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.2272 - val_loss: 0.0618\n",
            "Epoch 4/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.2227 - val_loss: 0.0589\n",
            "Epoch 5/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.2147 - val_loss: 0.0535\n",
            "Epoch 6/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1978 - val_loss: 0.0434\n",
            "Epoch 7/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.1694 - val_loss: 0.0293\n",
            "Epoch 8/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1291 - val_loss: 0.0180\n",
            "Epoch 9/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 0.0152\n",
            "Epoch 10/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.0179\n",
            "Epoch 11/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0194\n",
            "Epoch 12/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0507 - val_loss: 0.0218\n",
            "Epoch 13/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.0213\n",
            "Epoch 14/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.0220\n",
            "Epoch 15/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.0226\n",
            "Epoch 16/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0415 - val_loss: 0.0232\n",
            "Epoch 17/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.0231\n",
            "Epoch 18/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0394 - val_loss: 0.0253\n",
            "Epoch 19/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0391 - val_loss: 0.0254\n",
            "Epoch 20/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.0258\n",
            "Epoch 21/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0380 - val_loss: 0.0260\n",
            "Epoch 22/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.0259\n",
            "Epoch 23/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.0249\n",
            "Epoch 24/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.0277\n",
            "Epoch 25/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.0257\n",
            "Epoch 26/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.0282\n",
            "Epoch 27/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.0278\n",
            "Epoch 28/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.0281\n",
            "Epoch 29/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.0278\n",
            "Epoch 30/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0367 - val_loss: 0.0277\n",
            "Epoch 31/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0274\n",
            "Epoch 32/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.0279\n",
            "Epoch 33/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0270\n",
            "Epoch 34/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.0277\n",
            "Epoch 35/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0365 - val_loss: 0.0260\n",
            "Epoch 36/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0274\n",
            "Epoch 37/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0275\n",
            "Epoch 38/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0267\n",
            "Epoch 39/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0270\n",
            "Epoch 40/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0273\n",
            "Epoch 41/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0256\n",
            "Epoch 42/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0261\n",
            "Epoch 43/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0360 - val_loss: 0.0277\n",
            "Epoch 44/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0268\n",
            "Epoch 45/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0268\n",
            "Epoch 46/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0268\n",
            "Epoch 47/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0262\n",
            "Epoch 48/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0263\n",
            "Epoch 49/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0343 - val_loss: 0.0261\n",
            "Epoch 50/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0245\n",
            "Epoch 51/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.0262\n",
            "Epoch 52/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0339 - val_loss: 0.0260\n",
            "Epoch 53/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0240\n",
            "Epoch 54/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0253\n",
            "Epoch 55/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0336 - val_loss: 0.0262\n",
            "Epoch 56/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 0.0255\n",
            "Epoch 57/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0336 - val_loss: 0.0251\n",
            "Epoch 58/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0331 - val_loss: 0.0253\n",
            "Epoch 59/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0250\n",
            "Epoch 60/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0328 - val_loss: 0.0255\n",
            "Epoch 61/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0236\n",
            "Epoch 62/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0325 - val_loss: 0.0256\n",
            "Epoch 63/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.0250\n",
            "Epoch 64/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0316 - val_loss: 0.0241\n",
            "Epoch 65/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0320 - val_loss: 0.0226\n",
            "Epoch 66/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0241\n",
            "Epoch 67/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.0245\n",
            "Epoch 68/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0309 - val_loss: 0.0236\n",
            "Epoch 69/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0312 - val_loss: 0.0234\n",
            "Epoch 70/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0307 - val_loss: 0.0232\n",
            "Epoch 71/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.0231\n",
            "Epoch 72/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0218\n",
            "Epoch 73/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0296 - val_loss: 0.0208\n",
            "Epoch 74/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0299 - val_loss: 0.0233\n",
            "Epoch 75/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0296 - val_loss: 0.0234\n",
            "Epoch 76/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0294 - val_loss: 0.0232\n",
            "Epoch 77/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0293 - val_loss: 0.0225\n",
            "Epoch 78/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0223\n",
            "Epoch 79/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0222\n",
            "Epoch 80/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0213\n",
            "Epoch 81/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0220\n",
            "Epoch 82/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0289 - val_loss: 0.0217\n",
            "Epoch 83/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0217\n",
            "Epoch 84/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0216\n",
            "Epoch 85/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0202\n",
            "Epoch 86/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.0211\n",
            "Epoch 87/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.0204\n",
            "Epoch 88/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 0.0199\n",
            "Epoch 89/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0203\n",
            "Epoch 90/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0259 - val_loss: 0.0209\n",
            "Epoch 91/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.0199\n",
            "Epoch 92/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0204\n",
            "Epoch 93/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0252 - val_loss: 0.0203\n",
            "Epoch 94/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.0193\n",
            "Epoch 95/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.0194\n",
            "Epoch 96/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0252 - val_loss: 0.0179\n",
            "Epoch 97/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0245 - val_loss: 0.0196\n",
            "Epoch 98/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0240 - val_loss: 0.0188\n",
            "Epoch 99/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0196\n",
            "Epoch 100/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0188\n",
            "Epoch 101/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0183\n",
            "Epoch 102/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 0.0191\n",
            "Epoch 103/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 0.0177\n",
            "Epoch 104/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0232 - val_loss: 0.0185\n",
            "Epoch 105/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0229 - val_loss: 0.0184\n",
            "Epoch 106/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0177\n",
            "Epoch 107/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0162\n",
            "Epoch 108/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0189\n",
            "Epoch 109/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0177\n",
            "Epoch 110/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0173\n",
            "Epoch 111/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0216 - val_loss: 0.0172\n",
            "Epoch 112/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0208 - val_loss: 0.0167\n",
            "Epoch 113/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.0162\n",
            "Epoch 114/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0210 - val_loss: 0.0165\n",
            "Epoch 115/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0160\n",
            "Epoch 116/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0163\n",
            "Epoch 117/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0155\n",
            "Epoch 118/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0200 - val_loss: 0.0161\n",
            "Epoch 119/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0158\n",
            "Epoch 120/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0152\n",
            "Epoch 121/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0156\n",
            "Epoch 122/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0191 - val_loss: 0.0152\n",
            "Epoch 123/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.0155\n",
            "Epoch 124/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0155\n",
            "Epoch 125/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0177 - val_loss: 0.0143\n",
            "Epoch 126/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0179 - val_loss: 0.0145\n",
            "Epoch 127/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0172 - val_loss: 0.0138\n",
            "Epoch 128/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.0137\n",
            "Epoch 129/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0171 - val_loss: 0.0135\n",
            "Epoch 130/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.0137\n",
            "Epoch 131/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.0132\n",
            "Epoch 132/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0162 - val_loss: 0.0129\n",
            "Epoch 133/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0133\n",
            "Epoch 134/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0156 - val_loss: 0.0122\n",
            "Epoch 135/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.0111\n",
            "Epoch 136/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0172 - val_loss: 0.0127\n",
            "Epoch 137/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 0.0119\n",
            "Epoch 138/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0147 - val_loss: 0.0124\n",
            "Epoch 139/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0120\n",
            "Epoch 140/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0138 - val_loss: 0.0108\n",
            "Epoch 141/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0142 - val_loss: 0.0114\n",
            "Epoch 142/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0137 - val_loss: 0.0108\n",
            "Epoch 143/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.0115\n",
            "Epoch 144/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0109\n",
            "Epoch 145/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0138 - val_loss: 0.0116\n",
            "Epoch 146/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0104\n",
            "Epoch 147/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0133 - val_loss: 0.0112\n",
            "Epoch 148/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0108\n",
            "Epoch 149/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0102\n",
            "Epoch 150/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0093\n",
            "Epoch 151/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0102\n",
            "Epoch 152/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0100\n",
            "Epoch 153/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0098\n",
            "Epoch 154/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.0091\n",
            "Epoch 155/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0110 - val_loss: 0.0088\n",
            "Epoch 156/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0089\n",
            "Epoch 157/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0085\n",
            "Epoch 158/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0101 - val_loss: 0.0076\n",
            "Epoch 159/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0106 - val_loss: 0.0085\n",
            "Epoch 160/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0100 - val_loss: 0.0078\n",
            "Epoch 161/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0098 - val_loss: 0.0074\n",
            "Epoch 162/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.0083\n",
            "Epoch 163/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0094 - val_loss: 0.0078\n",
            "Epoch 164/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0093 - val_loss: 0.0074\n",
            "Epoch 165/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0087 - val_loss: 0.0071\n",
            "Epoch 166/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0086 - val_loss: 0.0068\n",
            "Epoch 167/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0062\n",
            "Epoch 168/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0085 - val_loss: 0.0068\n",
            "Epoch 169/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0063\n",
            "Epoch 170/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0084 - val_loss: 0.0077\n",
            "Epoch 171/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0061\n",
            "Epoch 172/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0080 - val_loss: 0.0059\n",
            "Epoch 173/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0066\n",
            "Epoch 174/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0059\n",
            "Epoch 175/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0076 - val_loss: 0.0068\n",
            "Epoch 176/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0074 - val_loss: 0.0063\n",
            "Epoch 177/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0060\n",
            "Epoch 178/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0062\n",
            "Epoch 179/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0053\n",
            "Epoch 180/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0068 - val_loss: 0.0056\n",
            "Epoch 181/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0054\n",
            "Epoch 182/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0067 - val_loss: 0.0062\n",
            "Epoch 183/200\n",
            "23/38 [=================>............] - ETA: 0s - loss: 0.0068"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'batch_size': [128, 256],\n",
        "    'epochs': [100, 200],\n",
        "    'learning_rate': [1e-3, 1e-4]\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "best_loss = float('inf')\n",
        "best_params = {}\n",
        "\n",
        "for batch_size, epochs, learning_rate in itertools.product(param_grid['batch_size'], param_grid['epochs'], param_grid['learning_rate']):\n",
        "    # Create an instance of the AnomalyDetector model\n",
        "    autoencoder = AnomalyDetector()\n",
        "    autoencoder.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), loss=modified_cost)\n",
        "\n",
        "    # Train the model\n",
        "    history = autoencoder.fit(normal_train_data, normal_train_data, epochs=epochs, batch_size=batch_size, validation_data=(normal_test_data, normal_test_data), shuffle=True)\n",
        "\n",
        "    # Get the loss value\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    # Check if this combination of hyperparameters resulted in a better loss\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_params = {\n",
        "            'batch_size': batch_size,\n",
        "            'epochs': epochs,\n",
        "            'learning_rate': learning_rate\n",
        "        }\n",
        "\n",
        "# Print the best hyperparameters and loss\n",
        "print(\"Best Hyperparameters: \", best_params)\n",
        "print(\"Best Loss: \", best_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55nCaZasbu5z"
      },
      "outputs": [],
      "source": [
        "def compile_and_train_model(model, X_train, X_test):\n",
        "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE), loss=modified_cost)\n",
        "    history = model.fit(X_train, X_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, X_test),\n",
        "                        shuffle=True)\n",
        "    return history\n",
        "\n",
        "\n",
        "def plot_loss(history):\n",
        "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_reconstruction(data, model, title=\"\"):\n",
        "    encoded_data = model.encoder(data).numpy()\n",
        "    decoded_data = model.decoder(encoded_data).numpy()\n",
        "\n",
        "    plt.plot(data[0], 'g')\n",
        "    plt.plot(decoded_data[0], 'r')\n",
        "    plt.fill_between(np.arange(10), decoded_data[0], data[0], color='lightcoral')\n",
        "    plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    fig.suptitle(\"Confusion Matrix\", fontsize=20)\n",
        "\n",
        "    sns.heatmap(cm, cmap='RdYlGn', annot=True, fmt='.0f', xticklabels=[0, 1], yticklabels=[0, 1], ax=ax[0])\n",
        "    ax[0].set_xlabel(\"Predicted Class\")\n",
        "    ax[0].set_ylabel(\"Actual Class\")\n",
        "    ax[0].set_title(\"Counts\")\n",
        "\n",
        "    sns.heatmap(cm_normalized * 100, cmap='RdYlGn', annot=True, fmt='.0f', xticklabels=[0, 1], yticklabels=[0, 1], ax=ax[1])\n",
        "    ax[1].set_xlabel(\"Predicted Class\")\n",
        "    ax[1].set_ylabel(\"Actual Class\")\n",
        "    ax[1].set_title(\"%\")\n",
        "    plt.show()\n",
        "\n",
        "def predict(model, data, threshold):\n",
        "    reconstructions = model(data)\n",
        "    loss = tf.keras.losses.mae(reconstructions, data)\n",
        "    return tf.math.less(loss, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2Qj6Ou87b0me",
        "outputId": "2aa81a1a-508b-42a6-b61f-2a961ca0b58c"
      },
      "outputs": [],
      "source": [
        "# Build, compile, and train the autoencoder\n",
        "autoencoder = AnomalyDetector()\n",
        "history = compile_and_train_model(autoencoder, normal_train_data, test_data)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plot_loss(history)\n",
        "\n",
        "# Plot reconstruction examples\n",
        "plot_reconstruction(normal_train_data, autoencoder, title=\"Reconstruction for Normal Data\")\n",
        "plot_reconstruction(anomalous_test_data, autoencoder, title=\"Reconstruction for Anomalous Data\")\n",
        "\n",
        "# Calculate train loss\n",
        "reconstructions = autoencoder.predict(normal_train_data)\n",
        "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n",
        "\n",
        "# Get predictions\n",
        "threshold = np.mean(train_loss) + np.std(train_loss)\n",
        "preds = predict(autoencoder, test_data, threshold)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(test_labels, preds)\n",
        "\n",
        "# Display performance metrics\n",
        "print(f\"Accuracy on testing data: {round(accuracy_score(test_labels, preds) * 100, 2)}%\")\n",
        "print(f\"Precision on testing data: {round(precision_score(test_labels, preds) * 100, 2)}%\")\n",
        "print(f\"Recall on testing data: {round(recall_score(test_labels, preds) * 100, 2)}%\")\n",
        "print(f\"F1 on testing data: {round(f1_score(test_labels, preds) * 100, 2)}%\")\n",
        "print(f\"Gini on testing data: {round((2 * roc_auc_score(test_labels, preds) - 1) * 100, 2)}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3VNXWThMhQvx"
      },
      "source": [
        "### Saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2t2iPywLkQj",
        "outputId": "53c4c3da-bed4-47dc-bd4b-928e5f4f0c24"
      },
      "outputs": [],
      "source": [
        "# Google\n",
        "\n",
        "#!mkdir -p /content/drive/MyDrive/Uni/IE/Capstone/model\n",
        "#autoencoder.save('/content/drive/MyDrive/Uni/IE/Capstone/model/my_model')\n",
        "\n",
        "# Local\n",
        "!mkdir -p ./model\n",
        "autoencoder.save('./model/my_model')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hZxu5hlFhHRR"
      },
      "source": [
        "## 8) Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "HyuTY6RRg8vj",
        "outputId": "551edf22-5d62-4333-9977-83633ecd7619"
      },
      "outputs": [],
      "source": [
        "# my_model directory\n",
        "#!ls /content/drive/MyDrive/Uni/IE/Capstone/model\n",
        "\n",
        "# Contains an assets folder, saved_model.pb, and variables folder.\n",
        "#!ls /content/drive/MyDrive/Uni/IE/Capstone/model/my_model\n",
        "\n",
        "#new_model = tf.keras.models.load_model('/content/drive/MyDrive/Uni/IE/Capstone/model/my_model')\n",
        "\n",
        "new_model = tf.keras.models.load_model('./model/my_model')\n",
        "\n",
        "# Check its architecture\n",
        "new_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncz4mGetjO2G"
      },
      "outputs": [],
      "source": [
        "new_model.encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tJJQZf9iNYf"
      },
      "outputs": [],
      "source": [
        "new_model.decoder.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

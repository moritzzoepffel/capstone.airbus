{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbus Impact Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Imports](#1-imports)\n",
    "    - [Import Datasets](#10-import-datasets)\n",
    "2. [EDA](#define-date-and-time-columns)\n",
    "3. [New Features](#new-features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "from numpy import quantile, where, random\n",
    "import datetime\n",
    "#import plotly.express as px\n",
    "#import plotly.graph_objs as go\n",
    "from matplotlib import pyplot as plt, dates\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"Scott/Capstone/msn_02_fuel_leak_signals_preprocessed.csv\",\n",
    "    \"Scott/Capstone/msn_10_fuel_leak_signals_preprocessed.csv\",\n",
    "    \"Scott/Capstone/msn_11_fuel_leak_signals_preprocessed.csv\",\n",
    "    \"Scott/Capstone/msn_12_fuel_leak_signals_preprocessed.csv\",\n",
    "    \"Scott/Capstone/msn_14_fuel_leak_signals_preprocessed.csv\",\n",
    "    \"Scott/Capstone/msn_29_fuel_leak_signals_preprocessed.csv\",\n",
    "    \"Scott/Capstone/msn_37_fuel_leak_signals_preprocessed.csv\",\n",
    "    \"Scott/Capstone/msn_53_fuel_leak_signals_preprocessed.csv\"\n",
    "]\n",
    "\n",
    "datasets = []\n",
    "datasets_labels = [\"MSN_02\", \"MSN_10\", \"MSN_11\", \"MSN_12\", \"MSN_14\", \"MSN_29\", \"MSN_37\", \"MSN_53\", \"MSN_37_LEAK0.5\", \"MSN_37_LEAK1.0\", \"MSN_37_LEAK5.0\"]\n",
    "\n",
    "for filename in filenames:\n",
    "    datasets.append(pd.read_csv(filename, sep=\";\"))\n",
    "\n",
    "# Copy datasets for leak simulation and add to datasets list\n",
    "for i in range(3):\n",
    "    datasets.append(datasets[6].copy())\n",
    "\n",
    "for i in datasets:\n",
    "    i['leakage']=0\n",
    "\n",
    "datasets[-1]['leakage']= 1\n",
    "datasets[-2]['leakage']= 1\n",
    "datasets[-3]['leakage'] = 1\n",
    "\n",
    "print(\"Datasets loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define Date and Time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    #introduce column date\n",
    "    dataset['UTC_TIME'] = pd.to_datetime(dataset['UTC_TIME'])\n",
    "    dataset['DATE'] = dataset['UTC_TIME'].dt.date\n",
    "    dataset[\"TIME\"] = dataset['UTC_TIME'].dt.time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define MSN for every dataset with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every dataset in datasets assign the label to MSN\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i][\"MSN\"] = datasets_labels[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2. Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Check for Records and Features for every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "data['Records'] = [dataset.shape[0] for dataset in datasets]\n",
    "data['Features'] = [dataset.shape[1] for dataset in datasets]\n",
    "\n",
    "pd.DataFrame(data, index = datasets_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Display columns in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset.info(verbose=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Total Null values and relative Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'NULL_VALUES': datasets[0].isnull().sum(), 'NULL_VALUES_%': datasets[0].isnull().sum() / datasets[0].shape[0] * 100, 'UNIQUE VALUES': datasets[0].nunique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for all datasets\n",
    "df_feat = pd.DataFrame()\n",
    "\n",
    "for i in range(1, len(datasets)):\n",
    "    df_tmp = pd.DataFrame({f'{datasets_labels[i]}_miss': datasets[i].isnull().sum(), \n",
    "                           f'{datasets_labels[i]}_miss_%': datasets[i].isnull().sum() / datasets[i].shape[0] * 100, \n",
    "                           f'{datasets_labels[i]}_unique': datasets[i].nunique()})\n",
    "    df_feat = pd.concat([df_feat, df_tmp], axis=1)\n",
    "    \n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print relative amount of flight phase 8\n",
    "print(\"Relative amount of flight phases 3-11\")\n",
    "for dataset in datasets:\n",
    "    print(dataset[dataset['FLIGHT_PHASE_COUNT'].isin(list(range(3,12)))].shape[0] / dataset.shape[0] * 100)\n",
    "    \n",
    "print(\"\\nRelative amount of rows with NaN values in flight phases 3-11\")\n",
    "# count rows with NaN values in flight phases 3-11\n",
    "for dataset in datasets:\n",
    "    print(dataset[dataset['FLIGHT_PHASE_COUNT'].isin(list(range(3,12)))].isnull().any(axis=1).sum() / dataset.shape[0] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_total = sum([dataset.shape[0] for dataset in datasets])\n",
    "records_dropna = sum([dataset.dropna().shape[0] for dataset in datasets])\n",
    "\n",
    "records_dropna_flightphase = {\n",
    "    i: sum([dataset[dataset['FLIGHT_PHASE_COUNT'] == i].dropna().shape[0] for dataset in datasets])\n",
    "    for i in range(1, 13)\n",
    "}\n",
    "\n",
    "print(\"Total records:\", records_total)\n",
    "print(\"Records after dropna:\", records_dropna)\n",
    "print(\"Records after dropna per flight phase:\", records_dropna_flightphase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with duplicats in UTC_TIME\n",
    "for dataset in datasets:\n",
    "    dataset.drop_duplicates(subset=['UTC_TIME'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame(records_dropna_flightphase, index=['Records']).transpose()\n",
    " \n",
    "fig = px.bar(df_tmp, x=df_tmp.index, y='Records', title='Records after dropna per flight phase')\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate std of diff in tanks by flight-phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dropna = [df.dropna() for df in datasets]\n",
    "all_datasets = pd.concat(datasets_dropna, ignore_index=True)\n",
    "\n",
    "columns_to_describe = [\n",
    "    'FUEL_USED_1', \n",
    "    'FUEL_USED_2', \n",
    "    'FUEL_USED_3', \n",
    "    'FUEL_USED_4', \n",
    "    'VALUE_FUEL_QTY_CT', \n",
    "    'VALUE_FUEL_QTY_RXT', \n",
    "    'VALUE_FUEL_QTY_LXT', \n",
    "    'VALUE_FUEL_QTY_FT1', \n",
    "    'VALUE_FUEL_QTY_FT2', \n",
    "    'VALUE_FUEL_QTY_FT3', \n",
    "    'VALUE_FUEL_QTY_FT4'\n",
    "]\n",
    "\n",
    "all_datasets_diff = all_datasets.groupby('FLIGHT_PHASE_COUNT')[columns_to_describe].diff().dropna()\n",
    "std_df = all_datasets_diff[columns_to_describe].std().reset_index()\n",
    "std_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the examples above, flight phases two and eight are the ones that contain the most data. The fact that flight phase two describes the aircraft on the ground and that the parameters there could be falsified, for example, by refuelling or artificially resetting the fuel consumption, and the requirements of Airbus have prompted us to adopt only flight phase eight in our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 3. Data Cleaning and Feature Engineering\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Assign to each concetenated flight a unique flight number and only keep flight_phases 8\n",
    "\n",
    "In the next cell we will divide the data into individual flight segments, which will be assigned unique numbers. This has the advantage that we can analyse individual sequences and describe them further in the EDA process. Before and after this, zero values are dropped.\n",
    "\n",
    "We're keeping flight phases 2 and 8 as they have a lot of values that could be used for anomaly detection. \n",
    "\n",
    "We're cleaning the second flight_phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_where_to_drop_nan = datasets[2].columns\n",
    "\n",
    "def introduce_flight_column(dataset, flight_number):\n",
    "    # copy dataset\n",
    "    dataset_tmp = dataset.copy()\n",
    "    \n",
    "    # Drop NaN values\n",
    "    dataset_tmp = dataset_tmp.dropna(subset=columns_where_to_drop_nan)\n",
    "    \n",
    "    # assign flight number to rows that have indexes following each other\n",
    "    # if the difference between two indexes is greater than 1, a new flight is assumed\n",
    "    # print(dataset_tmp.shape)\n",
    "    \n",
    "    for i in range(1, dataset_tmp.shape[0]):\n",
    "        if dataset_tmp.index[i] - dataset_tmp.index[i-1] > 1:\n",
    "            flight_number += 1\n",
    "        dataset_tmp.at[dataset_tmp.index[i], 'FLIGHT'] = flight_number\n",
    "        \n",
    "    # Fill nan values in flight column with 0\n",
    "    dataset_tmp['FLIGHT'] = dataset_tmp['FLIGHT'].fillna(0)\n",
    "    \n",
    "    # only keep rows with flight_phase_count = 2 and 8\n",
    "    dataset_tmp =  dataset_tmp[dataset_tmp['FLIGHT_PHASE_COUNT'].isin([2, 8])]\n",
    "            \n",
    "    # return dataset with flight column\n",
    "    return dataset_tmp, flight_number\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_number = 0\n",
    "datasets_unique_flights = []\n",
    " \n",
    "for dataset in datasets_dropna:\n",
    "    dataset_tmp, flight_number = introduce_flight_column(dataset, flight_number)\n",
    "    datasets_unique_flights.append(dataset_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset in datasets_unique_flights count occurences of FLIGHT\n",
    "for dataset in datasets_unique_flights:\n",
    "    counts = dataset['FLIGHT'].value_counts()\n",
    "    try:\n",
    "        print(\n",
    "            pd.DataFrame(counts).sort_values(by='FLIGHT',\n",
    "                                             ascending=False).iloc[0].FLIGHT)\n",
    "    except:\n",
    "        print(\"No flights found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_unique_flights:\n",
    "    print(dataset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Add synthetic Fuel Leak to different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leak(dataset,\n",
    "                leak_flow,\n",
    "                start_index=None,\n",
    "                end_index=None,\n",
    "                leak=True):\n",
    "    # create copy of dataset\n",
    "    dataset_tmp = dataset.copy()\n",
    "\n",
    "    # define FUEL_TANK_COLS\n",
    "    FUEL_TANK_COLS = [\n",
    "        'VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_LXT',\n",
    "        'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3',\n",
    "        'VALUE_FUEL_QTY_FT4'\n",
    "    ]\n",
    "\n",
    "    # choose random fuel tank where the fuel leak is happening\n",
    "    while True:\n",
    "        fuel_tank = random.choice(FUEL_TANK_COLS)\n",
    "\n",
    "        # check if fuel tank is empty at any point in time\n",
    "        if dataset_tmp[fuel_tank].max() >= leak_flow:\n",
    "            break\n",
    "\n",
    "    if leak == False:\n",
    "        # substract the same amount of fuel from value_fob\n",
    "        dataset_tmp['VALUE_FOB'] = dataset_tmp['VALUE_FOB'] - leak_flow\n",
    "    else:\n",
    "        increment = 0\n",
    "        #for index in dataset_tmp.index:\n",
    "        #   dataset_tmp.at[index, 'VALUE_FOB'] -= leak_flow + increment\n",
    "        #  increment += leak_flow\n",
    "        # substract leak flow from fuel tank per sequence in FLIGHT\n",
    "        for flight in dataset_tmp['FLIGHT'].unique():\n",
    "            # from value_fob substract leak flow. Leak flow should be increased by leak flow per row\n",
    "            increment = 0\n",
    "            for index in dataset_tmp[dataset_tmp[\"FLIGHT\"] == flight].index:\n",
    "                dataset_tmp.at[index, 'VALUE_FOB'] -= leak_flow + increment\n",
    "                dataset_tmp.at[index, fuel_tank] -= leak_flow + increment\n",
    "                increment += leak_flow\n",
    "\n",
    "    # set negative values in value_fob to 0\n",
    "    dataset_tmp['VALUE_FOB'] = dataset_tmp['VALUE_FOB'].clip(lower=0)\n",
    "\n",
    "    # set negative values in fuel_tank to 0\n",
    "    dataset_tmp[fuel_tank] = dataset_tmp[fuel_tank].clip(lower=0)\n",
    "\n",
    "    return dataset_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this point the following changes will be applied to two list of datasets\n",
    "datasets_unique_flights_with_fuel_leak = datasets_unique_flights.copy()\n",
    "\n",
    "# add fuel leak to the last 3 datasets. First one 0.5L per minute, second 1L per minute, third 5L per minute\n",
    "datasets_unique_flights_with_fuel_leak[-1] = create_leak(datasets_unique_flights[-1], 5)\n",
    "datasets_unique_flights_with_fuel_leak[-2] = create_leak(datasets_unique_flights[-2], 1)\n",
    "datasets_unique_flights_with_fuel_leak[-3] = create_leak(datasets_unique_flights[-3], 0.5)\n",
    "\n",
    "\"\"\"datasets_unique_flights_with_fuel_leak[-1] = create_leak(datasets_unique_flights[-1], 20)\n",
    "datasets_unique_flights_with_fuel_leak[-2] = create_leak(datasets_unique_flights[-2], 10)\n",
    "datasets_unique_flights_with_fuel_leak[-3] = create_leak(datasets_unique_flights[-3], 5)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_unique_flights:\n",
    "    print(dataset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don't have a single value in Dataset 2 we gonna drop it at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop second dataset from datasets_unique_flights\n",
    "datasets_unique_flights.pop(1)\n",
    "datasets_unique_flights_with_fuel_leak.pop(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to subtract first value of a series from all values\n",
    "def subtract_first(x):\n",
    "    return x.iloc[0] - x\n",
    "\n",
    "def define_new_features(dataset):\n",
    "    # copy dataset\n",
    "    dataset_tmp = dataset.copy()\n",
    "    \n",
    "    # initialize TOTAL_FUEK_USED column\n",
    "    dataset_tmp['TOTAL_FUEL_USED'] = 0\n",
    "     \n",
    "    # Total fuel column is the sum of all fuel used columns\n",
    "    for i in range(1, 5):\n",
    "        dataset_tmp['TOTAL_FUEL_USED'] += dataset_tmp['FUEL_USED_' + str(i)]\n",
    "        \n",
    "    # Get difference for VALUE_FOB for each flight \n",
    "    dataset_tmp['VALUE_FOB_DIFF'] = dataset_tmp.groupby('FLIGHT')['VALUE_FOB'].diff()\n",
    "    dataset_tmp['VALUE_FOB_DIFF'] = dataset_tmp['VALUE_FOB_DIFF'].fillna(0)\n",
    "    \n",
    "    dataset_tmp['TOTAL_FOB_BY_QTY'] = dataset_tmp['VALUE_FUEL_QTY_CT'] + dataset_tmp['VALUE_FUEL_QTY_FT1'] + dataset_tmp['VALUE_FUEL_QTY_FT2'] + dataset_tmp['VALUE_FUEL_QTY_FT3'] + dataset_tmp['VALUE_FUEL_QTY_FT4'] + dataset_tmp['VALUE_FUEL_QTY_LXT'] + dataset_tmp['VALUE_FUEL_QTY_RXT']\n",
    "    \n",
    "    dataset_tmp['DELTA_VFOB_VS_VFOBQTY'] = dataset_tmp['VALUE_FOB'] - dataset_tmp['TOTAL_FOB_BY_QTY']\n",
    "    \n",
    "    dataset_tmp['ALTITUDE_DIFF'] = dataset_tmp['FW_GEO_ALTITUDE'].diff().abs() \n",
    "    \n",
    "    dataset_tmp['VALUE_FOB_MISSING'] = dataset_tmp.groupby('FLIGHT')['VALUE_FOB'].transform(subtract_first)  \n",
    "    \n",
    "    dataset_tmp['VALUE_FOB_MISSING_BY_QTY'] = dataset_tmp.groupby('FLIGHT')['TOTAL_FOB_BY_QTY'].transform(subtract_first)\n",
    "    \n",
    "    # new columnn VALUE_FOB_BY_FUEL_USED. \n",
    "    #For ex: FOB_calculated(t=3) = VALUE_FOB(t=0) - delta_total_fuel_used(t=1) - delta_total_fuel_used(t=2) - delta_total_fuel_used(t=3)\n",
    "    for flight in dataset_tmp['FLIGHT'].unique():\n",
    "        value_fob_0 = dataset_tmp[dataset_tmp['FLIGHT'] == flight]['VALUE_FOB'].iloc[0]\n",
    "        diff_total_fuel_used = dataset_tmp[dataset_tmp['FLIGHT'] == flight]['TOTAL_FUEL_USED'].diff()\n",
    "        dataset_tmp.loc[dataset_tmp['FLIGHT'] == flight, 'VALUE_FOB_BY_FUEL_USED'] = value_fob_0 - diff_total_fuel_used\n",
    "        \n",
    "    # code to remove first part of flight where the plane is on the ground and the fuel used is resetted\n",
    "    for flight_num in dataset_tmp['FLIGHT'].unique():\n",
    "        # get first index of flight \n",
    "        first_index = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num].index[0]\n",
    "        \n",
    "        # get last index of flight\n",
    "        last_index = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num].index[-1]\n",
    "        \n",
    "        # get location of min value of VALUE_FOB per flight \n",
    "        min_value_sum_fuel_used = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num]['TOTAL_FUEL_USED'].idxmin()\n",
    "        \n",
    "        # delete in dataset_tmp all rows between first_location and min_value_fob\n",
    "        dataset_tmp.drop(dataset_tmp.loc[first_index:min_value_sum_fuel_used].index, inplace=True) \n",
    "    \n",
    "    # for each FLIGHT substract the first value of TOTAL_FUEL_USED from all values\n",
    "    #for flight in dataset_tmp['FLIGHT'].unique():\n",
    "    #    dataset_tmp.loc[dataset_tmp['FLIGHT'] == flight, 'TOTAL_FUEL_USED'] -= dataset_tmp[dataset_tmp['FLIGHT'] == flight]['TOTAL_FUEL_USED'].iloc[0]\n",
    "    \n",
    "    return dataset_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_new_features = [define_new_features(dataset) for dataset in datasets_unique_flights]\n",
    "datasets_with_new_features_with_fuel_leak = [define_new_features(dataset) for dataset in datasets_unique_flights_with_fuel_leak]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of datasets to see total number of features\n",
    "for dataset in datasets_with_new_features:\n",
    "    print(dataset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Deleting Values before all Fuel_Used Columns are resetted \n",
    "Not necassary if only flight phase 8 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_flights(dataset):\n",
    "    # copy dataset\n",
    "    dataset_tmp = dataset.copy()\n",
    "     \n",
    "    for flight_num in dataset_tmp['FLIGHT'].unique():\n",
    "        # get first index of flight \n",
    "        first_index = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num].index[0]\n",
    "        \n",
    "        # get last index of flight\n",
    "        last_index = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num].index[-1]\n",
    "        \n",
    "        # get location of min value of VALUE_FOB per flight \n",
    "        min_value_sum_fuel_used = dataset_tmp[dataset_tmp['FLIGHT'] == flight_num]['TOTAL_FUEL_USED'].idxmin()\n",
    "        \n",
    "        # delete in dataset_tmp all rows between first_location and min_value_fob\n",
    "        dataset_tmp.drop(dataset_tmp.loc[first_index:min_value_sum_fuel_used].index, inplace=True)\n",
    "        \n",
    "        # dataset_tmp[\"VALUE_FOB_MISSING\"] = dataset_tmp.groupby('FLIGHT')['VALUE_FOB'].transform(subtract_first)\n",
    "        \n",
    "    return dataset_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necassary if only flight phase 8 is used\n",
    "# datasets_without_reset_engines = [initalize_flights(dataset) for dataset in datasets_with_new_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(datasets_with_new_features):\n",
    "    print(f'Dataset {i} shape: {dataset.shape}')\n",
    "    print(f'Dataset {i} unique flights: {dataset[\"FLIGHT\"].nunique()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Altitude vs Value Fob for dataset 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Altitude vs Value Fob for the rest of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_phase_dict = {\n",
    "    1: 'Pre-flight',\n",
    "    2: 'Engine Run',\n",
    "    3: 'Take-Off 1',\n",
    "    4: 'Take-Off 2',\n",
    "    5: 'Take-Off 3',\n",
    "    6: 'Climbing 1',\n",
    "    7: 'Climbing 2',\n",
    "    8: 'Cruise',\n",
    "    9: 'Descent',\n",
    "    10: 'Approach',\n",
    "    11: 'Landing',\n",
    "    12: 'Post-flight'\n",
    "}\n",
    "\n",
    "def plot_flight(dataset, min_flight_length=100, max_graphs=10):\n",
    "    for flight_num in dataset[\"FLIGHT\"].unique(): \n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Define data for flight\n",
    "        flight_data = dataset[dataset['FLIGHT'] == flight_num]\n",
    "        \n",
    "        if flight_data.shape[0] < min_flight_length:\n",
    "            continue\n",
    "        \n",
    "        # Define traces\n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                y=flight_data['VALUE_FOB'],\n",
    "                                mode='lines',\n",
    "                                name='VALUE_FOB')) \n",
    "\n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                y=flight_data['FW_GEO_ALTITUDE'],\n",
    "                                mode='lines',\n",
    "                                name='FW_GEO_ALTITUDE'))\n",
    "        \n",
    "        # add trace for total_fuel_used\n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                    y=flight_data['TOTAL_FUEL_USED'],\n",
    "                                    mode='lines',\n",
    "                                    name='TOTAL_FUEL_USED'))\n",
    "        \n",
    "        # add trace for VALUE_FOB_DIFF\n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                    y=flight_data['VALUE_FOB_DIFF'],\n",
    "                                    mode='lines',\n",
    "                                    name='VALUE_FOB_DIFF'))\n",
    "        \n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                    y=flight_data['VALUE_FOB_MISSING'],\n",
    "                                    mode='lines',\n",
    "                                    name='VALUE_FOB_MISSING'))\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=flight_data['UTC_TIME'],\n",
    "                                 y=flight_data['VALUE_FOB_BY_FUEL_USED'],\n",
    "                                 mode='lines',\n",
    "                                 name='VALUE_FOB_BY_FUEL_USED'))\n",
    "        \n",
    "        \n",
    "        for flight_phase in flight_data['FLIGHT_PHASE_COUNT'].unique():\n",
    "            fig.add_trace(go.Scatter(x=flight_data[flight_data['FLIGHT_PHASE_COUNT'] == flight_phase]['UTC_TIME'],\n",
    "                                    y=flight_data[flight_data['FLIGHT_PHASE_COUNT'] == flight_phase]['VALUE_FOB'],\n",
    "                                    mode='lines',\n",
    "                                        name=f'{flight_phase}, {flight_phase_dict[flight_phase]}'))\n",
    "\n",
    "        fig.update_layout(title=f'VALUE_FOB and FW_GEO_ALTITUDE for flight {flight_num}',\n",
    "                        xaxis_title='UTC_TIME',\n",
    "                        yaxis_title='VALUE_FOB and FW_GEO_ALTITUDE')\n",
    "        \n",
    "        # Define rectangles for different flight phases\n",
    "        shapes = []\n",
    "        colors = [\n",
    "        'LightSkyBlue', 'LightSalmon', 'LightGreen', 'LightYellow', 'LightPink', \n",
    "        'MediumPurple', 'DarkOrange', 'MediumSeaGreen', 'DeepSkyBlue']\n",
    "        # Define more colors if needed\n",
    "        for i, flight_phase in enumerate(flight_data['FLIGHT_PHASE_COUNT'].unique()):\n",
    "            phase_data = flight_data[flight_data['FLIGHT_PHASE_COUNT'] == flight_phase]\n",
    "            shapes.append(\n",
    "                go.layout.Shape(\n",
    "                    type=\"rect\",\n",
    "                    xref=\"x\",\n",
    "                    yref=\"paper\",  # use 'paper' to refer to the entire y range\n",
    "                    x0=phase_data['UTC_TIME'].min(),\n",
    "                    x1=phase_data['UTC_TIME'].max(),\n",
    "                    y0=0,\n",
    "                    y1=1,\n",
    "                    fillcolor=colors[i % len(colors)],\n",
    "                    opacity=0.5,\n",
    "                    layer=\"below\",\n",
    "                    line_width=0,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(shapes=shapes)\n",
    "        fig.show()\n",
    "        \n",
    "        max_graphs -= 1\n",
    "        \n",
    "        if(max_graphs == 0):    \n",
    "            break\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flight(datasets_with_new_features[-1], min_flight_length=500, max_graphs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flight(datasets_with_new_features_with_fuel_leak[-1], min_flight_length=500, max_graphs=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring all datasets to the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to drop so that all datasets have the same columns\n",
    "columns_to_drop = [column for column in datasets_with_new_features[0].columns if column not in datasets_with_new_features[2].columns]\n",
    "\n",
    "datasets_with_new_features[0].drop(columns_to_drop, axis=1, inplace=True)\n",
    "datasets_with_new_features_with_fuel_leak[0].drop(columns_to_drop, axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of datasets\n",
    "for i, dataset in enumerate(datasets_with_new_features):\n",
    "    print(f'Dataset {i} shape: {dataset.shape}')\n",
    "    print(f'Dataset {i} unique flights: {dataset[\"FLIGHT\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_new_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the columns \n",
    "for i, dataset in enumerate(datasets_with_new_features):\n",
    "    print(f'Dataset {i} shape: {dataset.shape}')\n",
    "    print(f'Dataset {i} unique flights: {dataset[\"FLIGHT\"].nunique()}')\n",
    "    dataset.hist(figsize=(20,20))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Outliers\n",
    "\n",
    "Since out data is not normally distributed most of the time we're using IQR for the outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of outliers outside of 95% confidence interval\n",
    "def drop_outliers(dataset):\n",
    "    # create copy of dataset\n",
    "    dataset_tmp = dataset.copy()\n",
    "    \n",
    "    for column in dataset_tmp.columns:\n",
    "        # skip columns that are not numeric\n",
    "        if dataset_tmp[column].dtype != 'float64':\n",
    "            continue\n",
    "        \n",
    "        # calculate 95% confidence interval\n",
    "        lower_bound, upper_bound = np.percentile(dataset_tmp[column], [2.5, 97.5])\n",
    "        \n",
    "        # count number of outliers\n",
    "        outliers = dataset_tmp[column][(dataset_tmp[column] < lower_bound) | (dataset_tmp[column] > upper_bound)]\n",
    "        print(f'Column {column} has {outliers.shape[0]} outliers')\n",
    "        \n",
    "        # delete outliers\n",
    "        dataset_tmp.drop(outliers.index, inplace=True)\n",
    "        \n",
    "    return dataset_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_without_outliers = [drop_outliers(dataset) for dataset in datasets_with_new_features]\n",
    "datasets_without_outliers_with_fuel_leak = [drop_outliers(dataset) for dataset in datasets_with_new_features_with_fuel_leak]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_without_outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all datasets into one\n",
    "merged_dataset = pd.concat(datasets_without_outliers)\n",
    "merged_dataset_with_fuel_leak = pd.concat(datasets_without_outliers_with_fuel_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop flights that are no longer than 180 seconds\n",
    "for flight in merged_dataset[\"FLIGHT\"].unique():\n",
    "    flight_data = merged_dataset[merged_dataset[\"FLIGHT\"] == flight]\n",
    "    if flight_data.shape[0] < 180:\n",
    "        merged_dataset.drop(flight_data.index, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column FLIGHT_PHASE_COUNT since it is unnecessary\n",
    "merged_dataset.drop(['FLIGHT_PHASE_COUNT'], axis=1, inplace=True)\n",
    "merged_dataset_with_fuel_leak.drop('FLIGHT_PHASE_COUNT', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# heatmap for numeric columns\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(merged_dataset.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not exists, create generated_data folder\n",
    "if not os.path.exists('generated_data'):\n",
    "    os.makedirs('generated_data')\n",
    "\n",
    "# save datasets into generated_data folder\n",
    "merged_dataset.to_csv('generated_data/merged_dataset_supervised.csv', index=False)\n",
    "merged_dataset_with_fuel_leak.to_csv('generated_data/merged_dataset_with_fuel_leak_supervised.csv', index=False)\n",
    "\"\"\"MORE LEAKAGE\n",
    "merged_dataset.to_csv('generated_data/merged_dataset_supervised_moreLeakage.csv', index=False)\n",
    "merged_dataset_with_fuel_leak.to_csv('generated_data/merged_dataset_with_fuel_leak_supervised_moreLeakage.csv', index=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the columns that are not diffed of the two datasets\n",
    "utc_with_leak = list(merged_dataset_with_fuel_leak['UTC_TIME'])\n",
    "flight_with_leak = list(merged_dataset_with_fuel_leak['Flight'])\n",
    "index_with_leak = list(merged_dataset_with_fuel_leak.index)\n",
    "msn_with_leak = list(merged_dataset_with_fuel_leak['MSN'])\n",
    "\n",
    "utc_normal = list(merged_dataset['UTC_TIME'])\n",
    "flight_normal = list(merged_dataset['Flight'])\n",
    "index_normal = list(merged_dataset.index)\n",
    "msn_normal = list(merged_dataset['MSN'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Derivated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = merged_dataset.select_dtypes(include=['float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset with resampled data diff for numeric  columns\n",
    "merged_dataset_diff = pd.DataFrame()\n",
    "merged_dataset_with_fuel_leak_diff = pd.DataFrame()\n",
    " \n",
    "for column in numeric_columns:\n",
    "    if column == 'ALTITUDE_DIFF' or column == 'VALUE_FOB_DIFF' or column == 'FLIGHT':\n",
    "        merged_dataset_diff[column] = merged_dataset[column]\n",
    "        merged_dataset_with_fuel_leak_diff[column] = merged_dataset_with_fuel_leak[column]\n",
    "    merged_dataset_diff[column] = merged_dataset[column].diff()\n",
    "    merged_dataset_with_fuel_leak_diff[column] = merged_dataset_with_fuel_leak[column].diff()\n",
    "    \n",
    "# fillna with 0\n",
    "merged_dataset_diff.fillna(0, inplace=True)\n",
    "merged_dataset_with_fuel_leak_diff.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset_diff['leakage']=merged_dataset['leakage']\n",
    "merged_dataset_with_fuel_leak_diff['leakage'] = merged_dataset_with_fuel_leak[\n",
    "    'leakage']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset_with_fuel_leak_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop FLIGHT column in both datasets\n",
    "merged_dataset_diff.drop('FLIGHT', axis=1, inplace=True)\n",
    "merged_dataset_with_fuel_leak_diff.drop('FLIGHT', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = merged_dataset_diff.select_dtypes(include=['float64']).columns\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets into generated_data folder\n",
    "merged_dataset_diff.to_csv('generated_data/merged_dataset_diff_supervised.csv', index=False)\n",
    "merged_dataset_with_fuel_leak_diff.to_csv('generated_data/merged_dataset_with_fuel_leak_diff_supervised.csv', index=False)\n",
    "\"\"\"MORE LEAKAGE\n",
    "merged_dataset_diff.to_csv('generated_data/merged_dataset_diff_supervised_moreLeakage.csv', index=False)\n",
    "merged_dataset_with_fuel_leak_diff.to_csv('generated_data/merged_dataset_with_fuel_leak_diff_supervised_moreLeakage.csv', index=False)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit_transform on merged dataset\n",
    "scaled_data = scaler.fit_transform(merged_dataset_diff[numeric_columns])\n",
    "scaled_data_with_fuel_leak = scaler.fit_transform(merged_dataset_with_fuel_leak_diff[numeric_columns])\n",
    "\n",
    "# convert scaled_dataset to dataframe\n",
    "scaled_dataset = pd.DataFrame(scaled_data, columns=numeric_columns)\n",
    "scaled_dataset_with_fuel_leak = pd.DataFrame(scaled_data_with_fuel_leak, columns=numeric_columns)\n",
    "\n",
    "scaled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset_diff = merged_dataset_diff.reset_index(drop=True)\n",
    "scaled_dataset['leakage'] = merged_dataset_diff['leakage']\n",
    "merged_dataset_with_fuel_leak_diff = merged_dataset_with_fuel_leak_diff.reset_index(drop=True)\n",
    "scaled_dataset_with_fuel_leak['leakage'] = merged_dataset_with_fuel_leak_diff['leakage']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scaled_data),np.std(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scaled dataset to csv\n",
    "scaled_dataset.to_csv('generated_data/scaled_dataset_supervised.csv', index=False)\n",
    "scaled_dataset_with_fuel_leak.to_csv('generated_data/scaled_dataset_with_fuel_leak_supervised.csv', index=False) \n",
    "\"\"\"MORE LEAKAGE\n",
    "scaled_dataset.to_csv('generated_data/scaled_dataset_supervised_moreLeakage.csv', index=False)\n",
    "scaled_dataset_with_fuel_leak.to_csv('generated_data/scaled_dataset_with_fuel_leak_supervised_moreLeakage.csv', index=False) \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA without Synthetic Leak Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = pd.read_csv('generated_data/scaled_dataset.csv')\n",
    "scaled_data_with_fuel_leak = pd.read_csv('generated_data/scaled_dataset_with_fuel_leak.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 15\n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA()  \n",
    "\n",
    "# Fit PCA on scaled_data\n",
    "pca.fit(scaled_data)\n",
    " \n",
    "# access values and vectors\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    " \n",
    "# transform data \n",
    "pca_data = pca.transform(scaled_data)\n",
    " \n",
    "# convert pca_data to dataframe\n",
    "pca_dataset = pd.DataFrame(pca_data, columns=[f'PC{i}' for i in range(1, len(pca.components_)+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dataset.to_csv('generated_data/pca_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "\n",
    "explained_var, cumulative_var\n",
    "\n",
    "n_components = len(explained_var)\n",
    "\n",
    "variance_trace = go.Bar(\n",
    "    x=list(range(1, n_components + 1)),\n",
    "    y=explained_var,\n",
    "    name='Explained Variance'\n",
    ")\n",
    "\n",
    "cumulative_trace = go.Scatter(\n",
    "    x=list(range(1, n_components + 1)),\n",
    "    y=cumulative_var,\n",
    "    mode='lines',\n",
    "    name='Cumulative Explained Variance'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='PCA - Explained and Cumulative Variance',\n",
    "    xaxis=dict(title='Principal Component'),\n",
    "    yaxis=dict(title='Variance Explained'),\n",
    "    legend=dict(x=0.7, y=1.0),\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[variance_trace, cumulative_trace], layout=layout)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = []\n",
    "\n",
    "for i, pc in enumerate(pca.components_, 1):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=scaled_data.columns, y=pc))\n",
    "    fig.update_layout(title=f\"Principal Component {i}\", xaxis_title=\"Features\", yaxis_title=\"PC Value\", template='plotly_dark')\n",
    "    figures.append(fig)\n",
    "for fig in figures[0:6]:\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 10\n",
    "\n",
    "# Create a PCA instance\n",
    "pca_with_fuel_leak = PCA()  \n",
    "\n",
    "# Fit PCA on scaled_data\n",
    "pca.fit(scaled_data_with_fuel_leak)\n",
    " \n",
    "# access values and vectors\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    " \n",
    "# transform data \n",
    "pca_data = pca.transform(scaled_data)\n",
    " \n",
    "# convert pca_data to dataframe\n",
    "pca_dataset_with_fuel_leak = pd.DataFrame(pca_data, columns=[f'PC{i}' for i in range(1, len(pca.components_)+1)])\n",
    "pca_dataset_with_fuel_leak.to_csv('generated_data/pca_dataset_with_fuel_leak.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "\n",
    "explained_var, cumulative_var\n",
    "\n",
    "n_components = len(explained_var)\n",
    "\n",
    "variance_trace = go.Bar(\n",
    "    x=list(range(1, n_components + 1)),\n",
    "    y=explained_var,\n",
    "    name='Explained Variance'\n",
    ")\n",
    "\n",
    "cumulative_trace = go.Scatter(\n",
    "    x=list(range(1, n_components + 1)),\n",
    "    y=cumulative_var,\n",
    "    mode='lines',\n",
    "    name='Cumulative Explained Variance'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='PCA - Explained and Cumulative Variance',\n",
    "    xaxis=dict(title='Principal Component'),\n",
    "    yaxis=dict(title='Variance Explained'),\n",
    "    legend=dict(x=0.7, y=1.0),\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[variance_trace, cumulative_trace], layout=layout)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = []\n",
    "\n",
    "for i, pc in enumerate(pca.components_, 1):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=scaled_data.columns, y=pc))\n",
    "    fig.update_layout(title=f\"Principal Component {i}\", xaxis_title=\"Features\", yaxis_title=\"PC Value\", template='plotly_dark')\n",
    "    figures.append(fig)\n",
    "for fig in figures[0:6]:\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check PCA with colorcodes by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"MSN_02\", \"MSN_11\", \"MSN_12\", \"MSN_14\", \"MSN_29\", \"MSN_37\", \"MSN_53\", \"MSN_37_LEAK0.5\", \"MSN_37_LEAK1.0\", \"MSN_37_LEAK5.0\"]\n",
    "colors = ['lightgrey', 'lightgrey', 'lightgrey', 'lightgrey', 'lightgrey', 'lightgrey', 'lightgrey', 'b', 'c', 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_msn = PCA(n_components=3)\n",
    "principalcomponents_msn = pca_msn.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca3 = pd.DataFrame(data = principalcomponents_msn, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "X_pca3.insert(0, \"MSN\", msn_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8), dpi=100)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.set_xlabel('PC3')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC1')\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = X_pca3['MSN'] == target\n",
    "    ax.scatter(X_pca3.loc[indicesToKeep, 'PC3']\n",
    "               , X_pca3.loc[indicesToKeep, 'PC2']\n",
    "               , X_pca3.loc[indicesToKeep, 'PC1']\n",
    "               , c = color\n",
    "               ,s = 1, \n",
    "               alpha = 0.5)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_msn = PCA(n_components=3)\n",
    "principalcomponents_msn = pca_msn.fit_transform(scaled_data_with_fuel_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca3 = pd.DataFrame(data = principalcomponents_msn, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "X_pca3.insert(0, \"MSN\", msn_with_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8), dpi=100)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.set_xlabel('PC3')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC1')\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = X_pca3['MSN'] == target\n",
    "    ax.scatter(X_pca3.loc[indicesToKeep, 'PC3']\n",
    "               , X_pca3.loc[indicesToKeep, 'PC2']\n",
    "               , X_pca3.loc[indicesToKeep, 'PC1']\n",
    "               , c = color\n",
    "               ,s = 1, \n",
    "               alpha = 0.5)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering using k-means\n",
    "wccs=[]\n",
    "for i in range(2,12):\n",
    "    kmeans_pca = KMeans(n_clusters=i,init=\"k-means++\", random_state=42)\n",
    "    kmeans_pca.fit(pca_dataset_with_fuel_leak)\n",
    "    wccs.append(kmeans_pca.inertia_)\n",
    "    \n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(2,12), wccs, marker=\"o\", linestyle=\"--\")\n",
    "plt.xlabel(\"Number_of_Clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.title(\"K-means with PCA clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering with kmeans++\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "kmeans.fit(pca_dataset_with_fuel_leak)\n",
    "\n",
    "# k-means clustering with random initialization\n",
    "kmeans_random = KMeans(n_clusters=2, init='random', random_state=42)\n",
    "kmeans_random.fit(pca_dataset_with_fuel_leak)\n",
    " \n",
    "# predict \n",
    "y_pred = kmeans.predict(pca_dataset_with_fuel_leak)\n",
    "y_pred_random = kmeans_random.predict(pca_dataset_with_fuel_leak)\n",
    " \n",
    "# add cluster labels to dataset\n",
    "pca_dataset_with_fuel_leak['cluster'] = y_pred\n",
    "pca_dataset_with_fuel_leak['cluster_random'] = y_pred_random\n",
    "\n",
    "# inertia\n",
    "inertia_k_means = kmeans.inertia_\n",
    "inertion_random = kmeans_random.inertia_\n",
    "print(\"Inertia of k-means clustering with kmeans++: \", inertia_k_means)\n",
    "print(\"Inertia of k-means clustering with random initialization: \", inertion_random)\n",
    "\n",
    " \n",
    "# silhouette plot\n",
    "fig = plt.figure(figsize=(12, 8), dpi=100)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('PC3')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC1') \n",
    "ax.scatter(pca_dataset_with_fuel_leak.iloc[:, 1], pca_dataset_with_fuel_leak.iloc[:, 2], pca_dataset_with_fuel_leak.iloc[:, 3], c=y_pred, s=1, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# silhouette plot\n",
    "fig = plt.figure(figsize=(12, 8), dpi=100)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('PC3')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC1') \n",
    "ax.scatter(pca_dataset_with_fuel_leak.iloc[:, 1], pca_dataset_with_fuel_leak.iloc[:, 2], pca_dataset_with_fuel_leak.iloc[:, 3], c=y_pred_random, s=1, cmap='viridis')\n",
    "plt.show()\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dataset_with_fuel_leak.groupby('cluster').count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Approach using Auto Enconders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_autoencoder_with_fuel_leak = pd.read_csv(\"generated_data/merged_dataset_with_fuel_leak.csv\")\n",
    "dataset_autoencoder_with_fuel_leak.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels to dataset where MSN = MSN_37_LEAK0.5 or MSN_37_LEAK1.0 or MSN_37_LEAK5.0\n",
    "dataset_autoencoder_with_fuel_leak['label'] = dataset_autoencoder_with_fuel_leak['MSN'].apply(lambda x: 1 if x == 'MSN_37_LEAK0.5' or x == 'MSN_37_LEAK1.0' or x == 'MSN_37_LEAK5.0' else 0)\n",
    "dataset_autoencoder_with_fuel_leak['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert UTC_TIME to datetime\n",
    "dataset_autoencoder_with_fuel_leak[\"UTC_TIME\"] = pd.to_datetime(dataset_autoencoder_with_fuel_leak['UTC_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate datasets by MSN and add to list \n",
    "dataset_list = []\n",
    "for msn in dataset_autoencoder_with_fuel_leak['MSN'].unique():\n",
    "    dataset_list.append(dataset_autoencoder_with_fuel_leak[dataset_autoencoder_with_fuel_leak['MSN'] == msn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample each dataset to 5 seconds\n",
    "for i in range(len(dataset_list)):\n",
    "    dataset_list[i] = dataset_list[i].set_index('UTC_TIME').resample('5S').mean().dropna()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all datasets into one\n",
    "dataset_autoencoder_with_fuel_leak = pd.concat(dataset_list)\n",
    " \n",
    "# save dataset to csv\n",
    "dataset_autoencoder_with_fuel_leak.to_csv(\"generated_data/dataset_autoencoder_with_fuel_leak_resampled.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_test plot TOTAL_FUEL_USED for each flight\n",
    "for flight in df_test['FLIGHT'].unique():\n",
    "    # count number of rows for each flight\n",
    "    if df_test[df_test['FLIGHT'] == flight].shape[0] > 1:\n",
    "        # plot TOTAL_FUEL_USED for each flight x = UTC_TIME, y = TOTAL_FUEL_USED only plot if FLIGHT_PHASE_COUNT == 8\n",
    "        if df_test[df_test['FLIGHT'] == flight]['FLIGHT_PHASE_COUNT'].iloc[0] == 2:\n",
    "            plt.plot(df_test[df_test['FLIGHT'] == flight]['UTC_TIME'], df_test[df_test['FLIGHT'] == flight]['VALUE_FOB_DIFF'])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each flight and each flightphase calculate the std of VALUE_FOB_DIFF \n",
    "\n",
    "results = []\n",
    "for flight in df_test['FLIGHT'].unique():\n",
    "    result = {}\n",
    "    for flightphase in df_test[df_test['FLIGHT'] == flight]['FLIGHT_PHASE_COUNT'].unique():\n",
    "        result['FLIGHT'] = flight\n",
    "        result['FLIGHT_PHASE'] = flightphase\n",
    "        result['VALUE_FOB_DIFF_std'] = df_test[(df_test['FLIGHT'] == flight) & (df_test['FLIGHT_PHASE_COUNT'] == flightphase)]['VALUE_FOB_DIFF'].std()\n",
    "        results.append(result)\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "pd.pivot_table(results_df, values='VALUE_FOB_DIFF_std', index=['FLIGHT'], columns=['FLIGHT_PHASE'], aggfunc=np.sum,  fill_value=0).sort_values(by=[11], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for dataset 0 for column LEAK_DETECTION_LEAK_FLOW\n",
    "datasets[0].plot.scatter(x='UTC_TIME', y='LEAK_DETECTION_LEAK_FLOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Extract numeric part from 'Flight' column\n",
    "datasets[0]['Flight_number'] = datasets[0]['Flight'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Sort DataFrame by 'Flight_number'\n",
    "sorted_df = datasets[0].sort_values(by='Flight_number')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# boxplot for sorted DataFrame for column VALUE_FOB.\n",
    "fig.add_trace(go.Box(\n",
    "    x= sorted_df['Flight'], \n",
    "    y= sorted_df['VALUE_FOB'], \n",
    "    name= 'VALUE_FOB',\n",
    "    line=dict(color='green'),  # change boxplot color to green\n",
    "    marker=dict(color='green') # change marker color to green\n",
    "))\n",
    "\n",
    "# Calculate mean for each flight in the sorted DataFrame\n",
    "mean_fob = sorted_df.groupby('Flight')['VALUE_FOB'].mean()\n",
    "\n",
    "# Add a scatter plot for the mean values\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=mean_fob.index, \n",
    "    y=mean_fob.values, \n",
    "    mode='lines', \n",
    "    name='Mean',\n",
    "    line=dict(color='purple')  # set line color to purple\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Boxplot for sorted dataset for column VALUE_FOB\", \n",
    "    xaxis_title=\"Flight\", \n",
    "    yaxis_title=\"VALUE_FOB\",\n",
    "    plot_bgcolor='black',  # set plot background color to black\n",
    "    paper_bgcolor='black',  # set paper background color to black\n",
    "    font=dict(color='white'),  # set font color to white\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cat_features02 = [col for col in datasets[0] if datasets[0][col].dtypes.name == 'bool']\n",
    "features02 = [col for col in datasets[0] if col not in cat_features02 if 'STATUS' not in col if 'MODE' not in col if col not in [\"day\", \"month\", \"time\", \"year\", \"SELECTED_GADIR_ALTITUDE_VALUE\", \"VALUE_FUEL_VOL_LST\", \"VALUE_FUEL_VOL_RST\", \"FLIGHT_PHASE_COUNT\", \"APU_FUEL_FLOW_REQUEST_SIGNAL_1\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,25)) \n",
    "sns.heatmap(msn02_smooth[features02].corr(), cmap=\"RdBu\", annot=True, ax=ax)\n",
    "plt.xticks(rotation=45, ha='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_datasets(dataset):\n",
    "    # List of columns to plot for VALUE_FUEL_QTY_CT;VALUE_FUEL_QTY_FT1;VALUE_FUEL_QTY_FT2;VALUE_FUEL_QTY_FT3;VALUE_FUEL_QTY_FT4;VALUE_FUEL_QTY_LXT;VALUE_FUEL_QTY_RXT\n",
    "    fuel_qty_cols = ['VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3', 'VALUE_FUEL_QTY_FT4']\n",
    "\n",
    "    # Loop over every unique date\n",
    "    for date in dataset['DATE'].unique():\n",
    "        for flight in dataset['MSN'].unique():\n",
    "            # Create a subplot\n",
    "            fig = sp.make_subplots(rows=3, cols=3)\n",
    "            \n",
    "            # size of fig \n",
    "            fig.update_layout(height=1400, width=1400)\n",
    "\n",
    "            # Loop over each column\n",
    "            for i, col in enumerate(fuel_qty_cols):\n",
    "                # save the dataset for the current date and flight\n",
    "                dataset_tmp = dataset[(dataset['DATE'] == date) & (dataset['MSN'] == flight)]\n",
    "                # Add scatter plot to subplot\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=dataset_tmp['UTC_TIME'], \n",
    "                            y=dataset_tmp[col], \n",
    "                            mode='markers',\n",
    "                            name=col),\n",
    "                    row=i//3 + 1, \n",
    "                    col=i%3 + 1\n",
    "                )\n",
    "\n",
    "                # Update xaxis and yaxis titles\n",
    "                fig.update_xaxes(title_text='UTC_TIME', row=i//3 + 1, col=i%3 + 1)\n",
    "                fig.update_yaxes(title_text=col + \" \" + str(date), row=i//3 + 1, col=i%3 + 1)\n",
    "\n",
    "            # Show the plot\n",
    "            #fig.show()\n",
    "            \n",
    "            # save the plot as png\n",
    "            fig.write_image(\"plots2/\" + str(date) +\"-\" + str(flight) + \".png\")\n",
    "     \n",
    "            \n",
    "#plot_datasets(datasets[1])\n",
    "\n",
    "#for i in range(len(datasets)):\n",
    " #   if i > 1:\n",
    "  #      plot_datasets(datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_for_date(dataset, date, flight_phase):\n",
    "    # List of columns to plot for VALUE_FUEL_QTY_CT;VALUE_FUEL_QTY_FT1;VALUE_FUEL_QTY_FT2;VALUE_FUEL_QTY_FT3;VALUE_FUEL_QTY_FT4;VALUE_FUEL_QTY_LXT;VALUE_FUEL_QTY_RXT\n",
    "    fuel_qty_cols = ['VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3', 'VALUE_FUEL_QTY_FT4']\n",
    "\n",
    "    # Loop over every unique date\n",
    "    for flight in dataset['MSN'].unique():\n",
    "        # Create a subplot\n",
    "        fig = sp.make_subplots(rows=3, cols=3)\n",
    "\n",
    "        # size of fig \n",
    "        fig.update_layout(height=1000, width=1200)\n",
    "\n",
    "        # Loop over each column\n",
    "        for i, col in enumerate(fuel_qty_cols):\n",
    "            # save the dataset for the current date and flight\n",
    "            dataset_tmp = dataset[(dataset['DATE'] == date) & (dataset['MSN'] == flight) & (dataset['FLIGHT_PHASE_COUNT'] == flight_phase)]\n",
    "            # Add scatter plot to subplot\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=dataset_tmp['UTC_TIME'], \n",
    "                        y=dataset_tmp[col], \n",
    "                        mode='markers',\n",
    "                        name=col),\n",
    "                row=i//3 + 1, \n",
    "                col=i%3 + 1\n",
    "            )\n",
    "\n",
    "            # Update xaxis and yaxis titles\n",
    "            fig.update_xaxes(title_text='UTC_TIME', row=i//3 + 1, col=i%3 + 1)\n",
    "            fig.update_yaxes(title_text=col + \" \" + str(date), row=i//3 + 1, col=i%3 + 1)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset_for_date(datasets[1], datetime.date(2015, 3, 7), 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[6][['DATE', 'MSN', 'FLIGHT_PHASE_COUNT']][datasets[6]['FLIGHT_PHASE_COUNT'] == 8.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add leakage data to the dataset\n",
    "def add_leakage_data(dataset, leakage_qty):\n",
    "    # substract leakage_qty from VALUE_FOB col\n",
    "    dataset['VALUE_FOB'] = dataset['VALUE_FOB'] - (leakage_qty / 60)\n",
    "    \n",
    "    # add column FUEL_USED_BY_FOB to dataset\n",
    "    # first column is 0 because we don't know the fuel used at the start of the flight\n",
    "    # every next row is the row + difference between the current and previous row in VALUE_FOB\n",
    "    dataset['FUEL_USED_BY_FOB'] = 0\n",
    "    for i in range(1, dataset.shape[0]):\n",
    "        dataset['FUEL_USED_BY_FOB'].iloc[i] = dataset['FUEL_USED_BY_FOB'].iloc[i-1] + (dataset['VALUE_FOB'].iloc[i] - dataset['VALUE_FOB'].iloc[i-1])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_leakage_data(datasets[8], 0.5)\n",
    "add_leakage_data(datasets[9], 1.0)\n",
    "add_leakage_data(datasets[10], 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_fuel_used(dataset):\n",
    "    nulls = dataset[dataset['FLIGHT_PHASE_COUNT'] == 8.0][\"FUEL_USED_1\"].isnull().sum()\n",
    "    shape_0 = dataset[dataset['FLIGHT_PHASE_COUNT'] == 8.0].shape[0]\n",
    "    \n",
    "    print(f\"dataset_{1}\", nulls / shape_0)\n",
    "    \n",
    "for dataset in datasets:\n",
    "    relative_fuel_used(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for dataset 6 for 2016-07-27, flightphase 8\n",
    "# plot columns VALUE_FOB and FUEL_USED_BY_FOB\n",
    "dataste_tmp = datasets[0][(datasets[0]['FLIGHT_PHASE_COUNT'] == 8.0) & (datasets[0]['DATE'] == datetime.date(2011, 3, 9))]\n",
    "\n",
    "dataste_tmp['VALUE_FOB_LEAKAGE'] = dataste_tmp['VALUE_FOB'] - (5.0)\n",
    "\n",
    "dataste_tmp[\"TOTAL_CONSUMPTION\"] = dataste_tmp[\"FUEL_USED_1\"] + dataste_tmp[\"FUEL_USED_2\"] + dataste_tmp[\"FUEL_USED_3\"] + dataste_tmp[\"FUEL_USED_4\"]\n",
    "dataste_tmp[\"TOTAL_CONSUMPTION\"] = dataste_tmp[\"TOTAL_CONSUMPTION\"] - dataste_tmp[\"TOTAL_CONSUMPTION\"].iloc[0]\n",
    "\n",
    "# invert the values of VALUE_FOB_LEAKAGE so that the highest value is 0\n",
    "dataste_tmp['VALUE_FOB_LEAKAGE'] = dataste_tmp['VALUE_FOB_LEAKAGE'].max() - dataste_tmp['VALUE_FOB_LEAKAGE'] + 5.0\n",
    "\n",
    "dataste_tmp['VALUE_FOB'] = dataste_tmp['VALUE_FOB'].max() - dataste_tmp['VALUE_FOB']\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dataste_tmp['UTC_TIME'],\n",
    "            y=dataste_tmp['TOTAL_CONSUMPTION'],\n",
    "            name='TOTAL_CONSUMPTION')\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dataste_tmp['UTC_TIME'],\n",
    "                y=dataste_tmp['VALUE_FOB_LEAKAGE'],\n",
    "                name='VALUE_FOB_LEAKAGE')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dataste_tmp['UTC_TIME'],\n",
    "                y=dataste_tmp['VALUE_FOB'],\n",
    "                name='VALUE_FOB')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=dataste_tmp['UTC_TIME'],\n",
    "                y=dataste_tmp['FW_GEO_ALTITUDE'],\n",
    "                name='FW_GEO_ALTITUDE')\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce new columns for each dataset [CT_DIFF, LXT_DIFF, RXT_DIFF, FT1_DIFF, FT2_DIFF, FT3_DIFF, FT4_DIFF]\n",
    "# value represents the difference between the current and the previous value\n",
    "for dataset in datasets:\n",
    "    dataset['CT_DIFF'] = dataset['VALUE_FUEL_QTY_CT'].diff()\n",
    "    dataset['LXT_DIFF'] = dataset['VALUE_FUEL_QTY_LXT'].diff()\n",
    "    dataset['RXT_DIFF'] = dataset['VALUE_FUEL_QTY_RXT'].diff()\n",
    "    dataset['FT1_DIFF'] = dataset['VALUE_FUEL_QTY_FT1'].diff()\n",
    "    dataset['FT2_DIFF'] = dataset['VALUE_FUEL_QTY_FT2'].diff()\n",
    "    dataset['FT3_DIFF'] = dataset['VALUE_FUEL_QTY_FT3'].diff()\n",
    "    dataset['FT4_DIFF'] = dataset['VALUE_FUEL_QTY_FT4'].diff()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff(): \n",
    "    #plot diff columns for dataset 0 for each unique date\n",
    "    for date in datasets[0]['DATE'].unique():\n",
    "        for flight in datasets[0]['MSN'].unique():\n",
    "            # create subplot 3 columns and len(datasets[0].unique) rows\n",
    "            fig = sp.make_subplots(rows=3, cols=3)\n",
    "            \n",
    "            # size of fig\n",
    "            fig.update_layout(height=1000, width=1000)\n",
    "            \n",
    "            diff_cols = ['CT_DIFF', 'LXT_DIFF', 'RXT_DIFF', 'FT1_DIFF', 'FT2_DIFF', 'FT3_DIFF', 'FT4_DIFF']\n",
    "            \n",
    "            # loop over the diff columns\n",
    "            for i, col in enumerate(diff_cols):\n",
    "                # save the dataset for the current date and flight\n",
    "                dataset_tmp = datasets[0][(datasets[0]['DATE'] == date) & (datasets[0]['MSN'] == flight)]\n",
    "                # add scatter plot to subplot\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=dataset_tmp['UTC_TIME'], \n",
    "                            y=dataset_tmp[col], \n",
    "                            mode='markers',\n",
    "                            name=col),\n",
    "                    row=i//3 + 1, \n",
    "                    col=i%3 + 1\n",
    "                )\n",
    "\n",
    "                # update xaxis and yaxis titles\n",
    "                fig.update_xaxes(title_text='UTC_TIME', row=i//3 + 1, col=i%3 + 1)\n",
    "                fig.update_yaxes(title_text=col + \" \" + str(date), row=i//3 + 1, col=i%3 + 1)\n",
    "                \n",
    "            # save fig as png \n",
    "            fig.write_image(\"plots2/diff_plots/\" + str(date) + \"-\" + str(flight) + \"_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas\n",
    "# 1. use lower bound and upper bound to detect outliers\n",
    "# 2. use the bounds to plot the data and see if there are any outliers\n",
    "# 3. deep learning auto detection anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate synthetic data for each dataset including a simulated fuel leak\n",
    "# using 0,5l, 1L and 5L as the leak size per minute\n",
    "\n",
    "def generate_synthetic_data(dataset, leak_size):\n",
    "    # create a copy of the dataset\n",
    "    synthetic_dataset = dataset.copy()\n",
    "    \n",
    "    synthetic_dataset['VALUE_FUEL_QTY_LXT'] = synthetic_dataset['VALUE_FUEL_QTY_LXT'] - (leak_size)\n",
    "    \n",
    "    return synthetic_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the fuel on board and the Fuel Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fuel_used(dataset):\n",
    "    fuel_used_cols = ['FUEL_USED_1', 'FUEL_USED_2', 'FUEL_USED_3', 'FUEL_USED_4']\n",
    "\n",
    "    # save the sum of the fuel used columns in a new column\n",
    "    dataset['FUEL_USED_SUM'] = dataset[fuel_used_cols].sum(axis=1)\n",
    "\n",
    "    fuel_cols = ['VALUE_FUEL_QTY_CT', 'VALUE_FUEL_QTY_LXT', 'VALUE_FUEL_QTY_RXT', 'VALUE_FUEL_QTY_FT1', 'VALUE_FUEL_QTY_FT2', 'VALUE_FUEL_QTY_FT3', 'VALUE_FUEL_QTY_FT4']\n",
    "    \n",
    "    dataset['FUEL_COLS_SUM'] = dataset[fuel_cols].sum(axis=1)\n",
    "\n",
    "    #datasets[0]['FUEL_DIFF_SUM'] = datasets[0]['FUEL_DIFF_SUM'].cumsum()\n",
    "\n",
    "    # substract from each value in FUEL_COLS_SUM the first value bigger than 1 and get the absoulte value. Do this for each day\n",
    "    dataset['FUEL_COLS_SUM'] = dataset.groupby(['DATE'])['FUEL_COLS_SUM'].transform(lambda x: x - x[x > 1].iloc[0]).abs()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "datasets[0][datasets[0]['DATE'] == datetime.date(2011, 3, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_fuel_cons_vs_fuel_qty(dataset):\n",
    "    # plot the fuel_used columns for dataset 0 for 2010-10-26\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    dates_to_find = dataset['DATE'].unique()[:1]\n",
    "\n",
    "    for date in dates_to_find:\n",
    "        dataset = dataset[dataset['FLIGHT_PHASE_COUNT'] == 8]\n",
    "        # plot FUEL_USED_SUM and FUEL_DIFF_SUM\n",
    "        fig.add_trace(go.Scatter(x=dataset[dataset['DATE'] == date]['UTC_TIME'],\n",
    "                                    y=dataset[dataset['DATE'] == date]['FUEL_USED_SUM'],\n",
    "                                    name='FUEL_USED_SUM'))\n",
    "        fig.add_trace(go.Scatter(x=dataset[dataset['DATE'] == date]['UTC_TIME'],\n",
    "                                    y=dataset[dataset['DATE'] == date]['FUEL_COLS_SUM'],\n",
    "                                    name='FUEL_COLS_SUM'))\n",
    "        # add altitude to the plot\n",
    "        fig.add_trace(go.Scatter(x=dataset[dataset['DATE'] == date]['UTC_TIME'],\n",
    "                                    y=dataset[dataset['DATE'] == date]['FW_GEO_ALTITUDE'],\n",
    "                                    name='FW_GEO_ALTITUDE'))\n",
    "        \n",
    "        fuel_tanks = ['CT', 'LXT', 'RXT', 'FT1', 'FT2', 'FT3', 'FT4']\n",
    "        # plot the fuel tanks\n",
    "        for tank in fuel_tanks:\n",
    "            fig.add_trace(go.Scatter(x=dataset[dataset['DATE'] == date]['UTC_TIME'],\n",
    "                                        y=dataset[dataset['DATE'] == date]['VALUE_FUEL_QTY_' + tank],\n",
    "                                        name='VALUE_FUEL_QTY_' + tank))        \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_fuel_leak = generate_synthetic_data(datasets[0], 1800)\n",
    "dataset_with_fuel_leak = calculate_fuel_used(dataset_with_fuel_leak)\n",
    "datasets[0] = calculate_fuel_used(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0]['FUEL_COLS_SUM'][datasets[0]['FUEL_COLS_SUM'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_fuel_leak['FUEL_COLS_SUM'][dataset_with_fuel_leak['FUEL_COLS_SUM'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fuel_cons_vs_fuel_qty(dataset_with_fuel_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fuel_cons_vs_fuel_qty(datasets[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pca \n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pcaInstance = PCA(n_components=2)\n",
    "\n",
    "dataset_tmp = datasets[0][(datasets[0]['FLIGHT_PHASE_COUNT'] == 8)]\n",
    "\n",
    "feat_cols = (item for item in dataset_tmp.columns if item.startswith('VALUE_FUEL_QTY'))\n",
    "\n",
    "dataset_tmp = dataset_tmp[feat_cols].dropna()\n",
    "\n",
    "print(dataset_tmp.shape)\n",
    "\n",
    "# fit the pca instance to the dataset\n",
    "pcaInstance.fit(dataset_tmp)\n",
    "\n",
    "# transform the dataset\n",
    "pcaInstance.transform(dataset_tmp)\n",
    "\n",
    "# get the explained variance ratio\n",
    "pcaInstance.explained_variance_ratio_\n",
    "\n",
    "# plot the explained variance ratio \n",
    "plt.plot(pcaInstance.explained_variance_ratio_)\n",
    "plt.show()\n",
    "\n",
    "# plot the pca components\n",
    "plt.scatter(pcaInstance.components_[0], pcaInstance.components_[1])\n",
    "plt.show()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Select numerical columns only\n",
    "numerical_cols = datasets[0].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Drop missing values\n",
    "datasets_num = datasets[0][numerical_cols].dropna()\n",
    "\n",
    "# Standardize the feature matrix\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to your data\n",
    "pipeline.fit(datasets_num)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d plot heatmap of the pca components \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    " \n",
    "x = pca.components_[0]\n",
    "y = pca.components_[1]\n",
    "z = pca.components_[2]\n",
    " \n",
    "ax.scatter(x,y,z, marker=\"s\", c=\"g\", s=40, label='first')\n",
    " \n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, pca.components_.shape[0]):\n",
    "    for j in range(0, pca.components_.shape[0]):\n",
    "        plt.scatter(pca.components_[i], pca.components_[j])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "plt.plot(cumulative_explained_variance)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Variance Explained by Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feat_cols = (item for item in datasets[0].columns if item.startswith('VALUE_FUEL_QTY'))\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(datasets[0][feat_cols].dropna())\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_pca, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select numerical columns only\n",
    "numerical_cols = datasets[0].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Drop missing values\n",
    "datasets_num = datasets[0][numerical_cols].dropna()\n",
    "\n",
    "# Standardize the feature matrix\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to your data\n",
    "pipeline.fit(datasets_num)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "plt.plot(cumulative_explained_variance)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Variance Explained by Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# show number of components that explain 95% of the variance\n",
    "print(np.where(cumulative_explained_variance > 0.95)[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# let's assume we take first 2 components as they explain the most variance\n",
    "pca = PCA(n_components=16)\n",
    "principalComponents = pca.fit_transform(datasets_num)\n",
    "\n",
    "# convert to dataframe\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8', 'pca_9', 'pca_10', 'pca_11', 'pca_12', 'pca_13', 'pca_14', 'pca_15', 'pca_16'])\n",
    "\n",
    "# Kmeans clustering\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(principalDf)\n",
    "\n",
    "# visualizing the clusters in 3d\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create trace\n",
    "trace = go.Scatter3d(\n",
    "    x=pca_transformed[:, 0],\n",
    "    y=pca_transformed[:, 1],\n",
    "    z=pca_transformed[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=clusters, # set color to an array/list of desired values\n",
    "        colorscale='Viridis', # choose a colorscale\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "layout = go.Layout(\n",
    "    title = '3D Scatter Plot of PCA Components with KMeans Clusters',\n",
    "    scene = dict(\n",
    "            xaxis_title='Component 1',\n",
    "            yaxis_title='Component 2',\n",
    "            zaxis_title='Component 3'),\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the original data\n",
    "pca_transformed = pipeline.transform(datasets_num)\n",
    "\n",
    "# Now you can fit KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "clusters = kmeans.fit_predict(pca_transformed)\n",
    "\n",
    "# Append the clusters to the original scaled dataset\n",
    "datasets_num_clustered = datasets_num.copy()\n",
    "datasets_num_clustered['Cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the clusters\n",
    "cluster_analysis = datasets_num_clustered.groupby('Cluster').mean()\n",
    "print(cluster_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, random_state=0)\n",
    "    km.fit(pca_transformed)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse)\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5)\n",
    "clusters = dbscan.fit_predict(pca_transformed)\n",
    " \n",
    "plt.scatter(pca_transformed[:, 0], pca_transformed[:, 1], c=clusters, cmap=\"plasma\")\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(pca.components_, columns=datasets_num.columns)\n",
    "\n",
    "# Plot the feature importances\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.bar(range(feature_importance.shape[1]), feature_importance.iloc[0])\n",
    "    \n",
    "plt.xticks(range(feature_importance.shape[1]), feature_importance.columns, rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "print(feature_importance)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "score = silhouette_score(pca_transformed, clusters)\n",
    "print('Silhouette Score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs with mean of the column\n",
    "datasets[0].fillna(datasets[0].mean(), inplace=True)\n",
    "\n",
    "datasets[0].drop_duplicates(inplace=True)\n",
    "\n",
    "from scipy import stats\n",
    "z_scores = stats.zscore(datasets[0][feat_cols])\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "new_df = datasets[0][filtered_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = new_df.corr()\n",
    "sns.heatmap(corr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(new_df)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
